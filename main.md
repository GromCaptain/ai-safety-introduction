# Искусственный интеллект и экзистенциальные риски

Этот текст - обзор проблемы безопасности искусственного интеллекта (ИИ), ставшей особенно актуальной в последние годы после появления новых мощных ИИ-продуктов. Текст написан так, чтобы быть полезным и удобным для чтения как для новичков, так и для людей уже знакомых с темой. Первым он предоставит подробный обзор проблемы с разных сторон; вторым поможет углубить понимание и развеять некоторые заблуждения.

Информация в данной работе представлена не в классической форме в виде плоского текста, а на основе вложенных разворачивающихся списков. Подобное чтение может быть поначалу не очень удобным, поэтому прежде чем приступить к чтению, рекомендуется ознакомиться с инструкцией.

Чтобы открыть инструкцию, нажмите на черный треугольник ▶️ слева от текста “Инструкция - как читать этот текст” на следующей строчке.

- Инструкция - как читать этот текст
    
    Этот материал активно использует вложенные списки. Если вы не знакомы с такими списками в Notion, то посмотрите, как они работают, на примере ниже.
    
    - Щелкните на черный треугольник слева от этого текста
        - Внутри этого пункта списка оказался список из нескольких пунктов
        - Внутри этих пунктов нет вложенных подпунктов (или других блоков), поэтому треугольники слева от этих списков серые
        - Вы можете развернуть и эти пункты, нажав на серые треугольники, но внутри не будет никакого содержимого
        - Чтобы свернуть пункт вместе со всеми его подпунктами, нажмите снова на черный треугольник
        - В Notion предусмотрены горячие клавиши, чтобы свернуть/развернуть все пункты на странице; однако в данном тексте это действие может выполняться довольно медленно из-за очень большого числа пунктов
        - Если вам все-таки потребуется свернуть все пункты, нажмите Ctrl+Alt+T вначале для полного разворота всех, в т.ч. еще свернутых пунктов; затем еще раз - для сворачивания всех пунктов
    
    ### Технические моменты
    
    Из-за наличия большой списков большой вложенности для содержимого этой страницы задана настройка “во всю ширину страницы” (иначе пункты самых глубоких списков имели бы очень маленькую ширину). Из-за этого читать текст может быть неудобно на широких экранах.
    
    Если это вызывает у вас трудности, вы можете изменить ширину окна вашего браузера или приложения Notion до удобной величины. По мере сворачивания и разворачивания вложенных списков вы, возможно, захотите периодически снова менять ширину окна.
    
    ### Как организован материал
    
    - **Вложенные списки**
    
    Список самого верхнего уровня содержит последовательность тезисов, составляющих главную мысль этого текста. На каждом следующем уровне (внутри каждого пункта) излагается, на каких умозаключениях основан данный пункт.

    В некотором роде список тезисов верхнего уровня и внутри подпунктов можно воспринимать как оглавление (плюс небольшие пояснения, какие мысли содержатся в данном разделе). Списки на более глубоких уровнях уже ближе к обычному содержанию текста (только изложенному по пунктам, а не по абзацам). Такие “текстовые” пункты тоже могут содержать вложенные подпункты - например, содержащие дополнительное обсуждение сказанного, примеры, возражения и уточнения.

    Сама структура таких вложенных списков рассчитана на то, что люди с разной степенью знакомства с вопросом могут читать только то, что является для них новым, и пропускать то, что им и так хорошо известно.
    
    - **Как читать текст**
    
    Таким образом, если то, что указано в некотором пункте, кажется вам не до конца понятным, не вполне очевидным или вовсе сомнительным, то смело разворачивайте этот пункт. Внутри будет последовательно раскрыта изложенная в нем мысль; могут быть даны примеры, пояснения и дополнительные мысли на эту тему. Если некоторые или все из подпунктов также требуют пояснения - разворачивайте поочередно и их.
    
    После того, как вы дошли до изложения более понятных и не требующих дальнейших пояснений мыслей, можете постепенно сворачивать вложенные списки, возвращаясь к более высокоуровневым идеям. При этом рекомендуется вновь перечитать развернутый пункт и соседние с ним, чтобы освежить в памяти ту мысль, пояснения к которой вы только что изучили. Это позволит вам составить общую картину на основе более частных уточнений, прочитанных вами перед этим.
    
    Если же тезисы на текущем уровне для вас кажутся вполне понятными и самоочевидными, то можете спокойно пропустить то, что находится внутри них. Хотя, если вам любопытно, то можете заглянуть на один уровень вглубь - возможно, помимо знакомых вам мыслей и аргументов вы увидите там какие-то новые для себя аспекты рассматриваемого вопроса, о котором ранее не задумывались.
    
    ### О содержании
    
    Также стоит сказать несколько слов о самой теме и о характере ее обсуждения.
    
    - **О сценариях возможного будущего**
    
    Первое и самое главное - мы пытаемся прогнозировать будущие события, не имеющие прецедента в человеческой истории, и относящиеся к сущностям, которые еще не существуют. Любые такие прогнозы неизбежно имеют спекулятивный характер, но при всей важности рассматриваемой темы ничего лучше у нас нет.
    
    Рассуждения на тему возможных сценариев будущего мы стараемся строить максимально логично и последовательно, опираясь на некоторые выглядящие разумными предпосылки, и делая все возможные оговорки. При этом структура многих фраз отражает предположительный характер всех рассуждений.
    
    Однако, несмотря на чистую умозрительность многих построений, часто они указывают в одном и том же направлении, что заставляет как минимум всерьез принимать их во внимание.
    
    - **О предубеждениях об ИИ**
    
    При этом тема искусственного интеллекта вообще и возможных конфликтов между ИИ и человечеством в частности - освещена в массовой культуре, с одной стороны, крайне широко, с другой - максимально нереалистично. В результате почти все широко распространенные идеи о потенциальных ИИ - совершенно нелепые мифы, которые не выдерживают серьезной критики (каковой регулярно и подвергаются).
    
    Специалисты в области машинного обучения, которые хорошо понимают, как устроены современные ИИ-системы, обычно справедливо критикуют все подобные мифы. Однако, из-за того, что идеи “восстания машин” настолько дискредитировали себя среди серьезных специалистов, те часто не воспринимают многие вопросы безопасности ИИ всерьез, поддерживая противоположный миф о “безопасности ИИ по умолчанию”.
    
    В данном материале будет уделено внимание предубеждениям с обоих концов спектра, и будет предпринята попытка составить максимально реалистичную картину мира (в текущий момент и в потенциальном будущем).
    
    - **Об онтологии**
    
    - Обратите внимание, что в этом тексте такие понятия, как “ум”, “разумность” и “интеллект” используются как синонимы, хотя в ряде случаев люди вкладывают в эти слова разные оттенки смысла
        - Если в вашей голове одно или несколько из этих понятий уж очень тесно связаны с каким-то другим определением, и вам сложно абстрагировать от него - можете при чтении данного текста мысленно делать автозамену всех этих слов на нечто вроде “интеллект-штрих” / ”интеллект с чертой” / “интеллект-2”
    - Конкретно, под словом “умный” здесь мы понимаем “обладающий интеллектом”, а под “(универсальным) интеллектом”, в свою очередь, подразумеваем “способность решать новые задачи“; под “более умный” - “обладающий более сильным интеллектом”, т.е. “умеющий более быстро, эффективно и с большей вероятностью решать б***о***льшую часть потенциальных новых задач”
        - Просто “решать задачи” могут все живые существа - например, задачу “добыть питательные вещества”; но обычно это не очень “интеллектуальная” способность
            - Бактерия просто выполняет стандартные действия, обусловленные ее физиологией и генетикой
            - Действия более сложных организмов основаны на более сложных инстинктах и рефлексах
            - Однако, почти всегда такие живые организмы выполняют достаточно стандартные действия в стандартных условиях, и способны решать стандартные задачи стандартным способом
            - Фактически большинство таких стандартных алгоритмов запрограммированы в генотипе и фенотипе организма; в некоторых случаях они могут научиться им у родителей и сородичей, но крайне редко - изобрести самостоятельно
        - Более “интеллектуальной” является способность адаптироваться к условиям и ситуациям, которые никогда не возникали в жизни данного организма, а также не были стандартными для его предков
            - Если какая-то популяция организмов сумела адаптироваться к новым условиям или освоить новый навык в течение десятков поколений, это не является проявлением интеллекта
            - Если одно животное сумело адаптироваться к новым условиям или освоить новый навык за время своей жизни - это гораздо ближе к интеллекту
            - Так, “отдернуть руку, прикоснувшись к чему-то горячему” - очень правильное действие, но оно не является свидетельством какого-то особого интеллекта; а вот “нажать на тормоз, если перед вашим автомобилем выскочил олень” - является
                - Наши предки многие миллионы лет соприкасались с объектами с разной температурой, и за это время рефлексы, предотвращающие ожоги, успешно “прошились” в их физиологии
                - Водить автомобили люди по эволюционным меркам начали буквально только что; никаких встроенных в структуру мозга предпосылок для освоения этого навыка у них нет; так что каждому будущему водителю приходится самостоятельно с нуля осваивать всё, связанное с рулем, педалями, коробкой передач, дорожной ситуацией и ПДД
        - Желательно при этом, чтобы решаемые задачи были принципиально новыми: “научиться искать еду в лесу” и “научиться искать еду в городе” - довольно похожие задачи, и животные часто решают их примерно одним и тем же способом в обоих случаях
        - Известно довольно много действительно умных/интеллектуальных животных; хотя люди являются несомненными рекордсменами среди обитателей Земли
        - Что в точности считать интеллектом, а что не считать - до сих пор является предметом дебатов; но можно довольно уверенно сказать, что к интеллекту относятся способность строить картину окружающего мира, способность строить планы для решения задач, и способность претворять эти планы в жизнь
            - Для построения модели мира требуется, в частности, извлекать информацию из наблюдений; обобщать данные; корректно обновлять свою модель на основе новой информацию; находить пробелы в текущей картине мира и т.п.
            - Для создания планов требуется уметь генерировать возможные варианты действий (как выбирать из числа уже известных, так и изобретать совершенно новые); просчитывать возможные последствия каждого из них (благодаря имеющейся модели мира); оценивать вероятность каждого исхода и возможную пользу от него
            - Для реализации планов необходимо уметь “держать руку на пульсе” (отслеживать текущий статус реализации плана и сопоставлять его с желаемым сценарием развития), обнаруживать непредвиденные препятствия, дорабатывать планы на ходу, а при необходимости - вовсе отменять их (а без необходимости - не отменять)
            - Для того, чтобы быть “умным”, желательно овладеть этими мета-навыками, или хотя бы большинством из них; при этом хорошее владение ими потенциально может компенсировать недостаток ряда других конкретных навыков
                - Например, не владея теорией и практикой управленческих навыков, можно довольно неплохо организовывать работу компании, исходя из самых общих соображений - если действительно хорошо уметь собирать информацию, моделировать, планировать и реализовывать планы
	
- Основное содержание

    - Что важно понимать про сильный искусственный интеллект (СИИ)
        - Существует довольно много путаницы в терминах, связанных с искусственным интеллектом (ИИ) и более узкими понятиями
            - ИИ - это целая область знаний, связанных с алгоритмами и технологиями; как правило, к ИИ относят практически любой артефакт (алгоритм, программу, технологию, систему), который способен выполнять какие-то действия, которые ассоциируются с условно “умными” действиями людей
                - Программы, в основе которых лежат более-менее простые и понятные алгоритмы, к ИИ обычно не относят
                - Так, хотя бухгалтерский софт, системы компьютерной алгебры или CAD-системы, работают в “умных” областях, они относятся к классическим “алгоритмическим” программам
                - Наиболее типичные представители ИИ-систем - это различный софт на основе [искусственных нейронных сетей](https://ru.wikipedia.org/wiki/Нейронная_сеть) (ИСС, нейросетей); хотя существуют и другие подходы
                    - Нейросети действуют по совершенно иному принципу, чем классические алгоритмы; при этом их структура во многих отношениях напоминает структуру мозга животных
            - К слабому ИИ относятся такие ИИ, каждый из которых специализируется в одной довольно узкой области; все современные ИИ-системы относятся к слабому ИИ
                - Сюда относятся многочисленный задачи, связанные с распознаванием визуальных образов и поиском объектов на изображениях (текст на сканах документов, рентгеновские снимки, автомобильные номера на записях с видеокамер, морские суда на спутниковых снимках и т.п.)
                - Также - распознавание речи на аудиозаписях
                - В обратную сторону - генерация изображений, синтез речи
                - Более комплексные системы - голосовые помощники, автопилот, языковые переводчики (в т.ч. для перевода речи в речь)
                - Боты для сложных игр - настольных (шахматы, го) и компьютерных (StarCraft, DOTA, Atari)
                - Современные языковые модели, генерирующие текст - хотя последние начинают демонстрировать хорошие результаты во всё большем числе задач одновременно, что приближает их если не к сильному ИИ, то по крайней мере к переходному звену между сильным и слабым ИИ
            - Сильный ИИ (СИИ, англ. Artificial General Intelligence, AGI) - это ныне еще не существующая система, которая должна уметь решать все или почти все задачи, которые способен решать обычный человек, причем как минимум не хуже, или даже лучше человека
                - Ключевую роль здесь играют не “внутренние свойства” ИИ, а его “внешние проявления” - СИИ не обязательно обладать теми же качествами, что и человек, если он и без этого может делать то же, что человек, или успешно притвориться им
                    - Если ИИ не “способен по-настоящему чувствовать эмоции”, но при необходимости легко может выдать себя за человека, обсуждающего, какие чувства вызывает у него итальянская опера, то он вполне может быть СИИ
                    - Аналогично, если ИИ не “способен по-настоящему понимать” область знаний, с которой он работает, но способен решать в ней задачи не хуже человека, который ее “понимает”, то это по-прежнему не мешает ему быть СИИ
                    - Все умозаключения, которые мы будем делать в этом тексте, касаются именно внешних проявлений СИИ (в частности, умение решать задачи) и опираются на них; и хотя деление, скажем, на “осознанный СИИ” и “неосознанный СИИ” может быть интересно с философской точки зрения, результаты воздействия на мир и человечество у обоих будут одни и те же
                - Гораздо важнее то, что, в отличие от слабого ИИ, сильный подобен человеку в том, что может успешно работать в самых разных областях, справляться с очень сложными/комплексными задачами, и способен искать и зачастую успешно находить решение задач, с которыми никогда раньше не сталкивался
                    - Если ИИ может по словесной команде “создай хорошо продающуюся компьютерную инди-игру” выполнить весь комплекс действий, за счет которого игра действительно будет создана, выложена в маркет и будет пользоваться успехом у игроков, то это хорошая заявка на сильного ИИ
                        - Для начала ему потребуется распознать команду и понять суть задачи; затем - составить подходящий план и начать реализовывать его (в частности, составляя подпланы для отдельных этапов плана и выполняя их)
                        - Работающий план, вероятно, будет включать в себя, помимо прочего, такие разноплановые этапы (каждый из которых также представляет собой довольно сложную задачу, с которыми наш ИИ тоже должен будет уметь справляться)
                            - Провести маркетинговое исследование, чтобы оценить, какими параметрами должна обладать продающаяся игра
                            - Выбрать на основе исследования для игры жанр, сеттинг, сюжет, фичи геймплея, систему монетизации и т.п.
                            - Создать под это ассеты, локации и механики
                            - Собрать из всего этого игру
                            - При необходимости протестировать ее с разных сторон, исправить баги и проблемы
                            - Выложить игру в макрет
                    - Если при этом ровно та же самая система способна решать ряд задач совсем другого рода - например, выступать автопилотом в автомобиле; в диалоговом режиме выдать вам несколько дельных советов по поводу оформления вашей спальни; с помощью подходящих манипуляторов связать носки и синтезировать новый антибиотик, то это еще больше похоже на СИИ
                    - Если при этом сообщить ИИ: “вот тебе радиоуправляемая машинка, вот тебе пульт управления, вот тебе инструкция и регламент гоночных соревнований, разберись - через неделю ты в них участвуешь”, и он (владея стандартными манипуляторами и видеокамерами, но не будучи ранее обучен ничему подобному) сможет разобраться со всем этим и неплохо выступить на соревнованиях - это еще один довод в пользу того, что этот ИИ является сильным
                        - Вначале ему потребуется изучить инструкцию и регламент соревнований; выделить из них основные концепции и взаимосвязи между ними
                        - После этого - соотнести эти концепции с конкретными физическими объектами - машинкой, пультом и трассой
                        - На основе этих концепций нужно будет уяснить себе суть управления моделью, а затем - освоить это управление с учетом обратной связи от реального поведения машинки в ответ на манипуляции с пультом управления
                        - Наконец, на основе всего этого надо будет составить гибкую стратегию работы с пультом - такую, чтобы во время соревнований машинка выполнила все требования регламента, и всё вместе дало наилучший результат с точки зрения соревнования (т.е. модель пришла к финишу как можно раньше)
                        - Впрочем, на самом деле конкретные шаги решения задачи у такого ИИ могут оказаться совершенно другими (он не обязан решать задачи сильно похожим на людей способом); но концептуальная сложность такого решения будет, вероятно, сопоставима
                    - Ничто из этого не относится к необходимым или достаточным критериям СИИ, но если некоторая система обладает сразу несколькими такими (или подобными) свойствами, то, скорее всего, перед вами действительно сильный искусственный интеллект
            - Строгого определения всех этих терминов, впрочем, не существует; все эти понятия являются достаточно условными (например, GPT-4 уже сложно с уверенностью назвать “слабым ИИ”, но определенно еще рано называть “сильным ИИ”)
                - К примеру, GPT-4 является очень хорошим чат-ботом, и в обычной беседе вы вряд ли отличите ее от реального человека
                    - В сервисах, подобных ChatGPT или Bing, такие системы отвечают на запросы в “корпоративном стиле” (и такие ответы сложно спутать с ответами живых людей), но стили общения являются скорее надстройками конкретного сервиса над основной языковой моделью
                    - Другие сервисы могут настраивать ту же языковую модель по-другому; а в некоторых случаях и сам пользователь может настроить стиль общения с помощью подходящих промптов
                - Также GPT-4 может выполнять очень большое число задач по работе с текстом (гораздо больше, чем традиционные системы слабого ИИ)
                    - Может написать текст на заданную тему, в заданном стиле и формате
                    - Может делать краткую выжимку переданного большого текста
                    - Может переводить текст с одного языка на другой
                    - Давать ответы на вопросы из разных областей (в том числе на уровне эксперта/профессионала в этой области)
                    - Писать программный код
                - В отличие от традиционных слабых ИИ, может работать не только с данными в одной модальности (текст, звуки речи, картинки, статистика котировок акций, информация о строении молекул химических соединений и т.п.), а способна принимать на вход и текст, и картинки
                    - Будущие поколения GPT смогут принимать на вход как минимум звуки и видео; со временем, вероятно, и другие данные
                    - Возможность генерировать данные не только в виде текста тоже кажется вполне реальной для следующих поколений модели
                - Но в отличие от ожидаемого сильного ИИ, GPT-4 обладает рядом недостатков - например, как и все большие языковые модели, периодически галлюцинирует (т.е. выдает выглядящие правдоподобными по форме, но совершенно неверные или бессмысленные ответы)
                    - Так, подобные модели могут подробно описать сюжет и перечислить актерский состав несуществующих фильмов (либо фильмов, которые реально существуют, но информация о которых не попала в обучающую выборку модели; в наиболее запущенных случаях - давать ответы, прямо противоречащие информации из обучающей выборки)
                    - Либо же они могут давать ответы на бессмысленные или внутренне противоречивые вопросы - например, описать основные вехи космической программы древних шумеров
                - Кроме того, текущая архитерктура языковых моделей не позволяет дообучаться в процессе работы (и тем более не допускает мало-мальски простой самомодификации)
                    - Человеческий мозг, например, постоянно изменяется - нейроны создают друг с другом новые связи и разрушают старые; по сути, мозг непрерывно дообучается прямо в процессе своей работы
                - Наконец, хотя GPT-4 способна хорошо справляться с принципиально большим спектром задач, чем любая модель слабого ИИ до недавнего времени, еще больше типов задач для нее по-прежнему недоступно
        - Скорее всего, СИИ будет совершенно не похож на людей во многих отношениях
            - Пока нам известен только один вид носителей [сильного] интеллекта - люди; но у людей нет принципиальной монополии на обладание интеллектом
            - Скорее всего, СИИ не будет антропоморфным, т.е. по ряду поведенческих черт будет существенно отличаться от людей
            - Многие известные в популярной культуре сценарии (в т.ч. катастрофические с точки зрения человечества), описывающие мир с искусственным интеллектом (”Терминатор”, “Матрица”, “Электроник” и т.п.), оказываются совершенно несостоятельны при внимательном анализе
            - С другой стороны, многие современные аргументы в пользу того, что перспективные ИИ-системы будут безопасны, оказываются несостоятельны по причине “излишней антропоморфизации” определенных свойств агентов
                - Рассмотрим некоторые такие утверждения
                    - “В силу его архитектуры (отсутствия специализированных блоков) ИИ очевидно не будет иметь подобных человеческим чувств, эмоций, желаний и стремлений” ⇒ “у ИИ не может возникнуть собственных целей”
                    - “Совершенно непонятно, откуда и почему у обычного алгоритма классификации объектов (искусственной нейросети) могло бы развиться самосознание” ⇒ “по крайней мере ИИ на основе нейросетей не будут обладать полноценным интеллектом”
                - Проблема таких утверждений обычно в использовании некорректных “правил вывода” из корректных базовых утверждений
                - Так, часто неявная схема таких цепочек рассуждений выглядит так: “некоторое свойство X, которым обладают люди, позволяет людям обладать другим свойством Y” + “у ИИ очевидно не будет свойства X” ⇒ “у ИИ также не будет свойства Y”
                    - “Желания, стремления, мотивы и цели людей в конечном итоге обусловлены влиянием нейромедиаторов и гормонов на различные отделы их головного мозга” + “ИИ не будет обладать ни эндокринной, ни лимбической системами, ответсвенными за генерацию и регуляцию эмоций, ни их цифровыми аналогами” ⇒ “ИИ будут лишены субстрата для появления мотивации к выполнению действий, отличных от тех, для которых их запрограммировали (если только не создать специальные подсистемы ИИ, запрограммированные на имитацию эмоций, подобных человеческим)”
                    - “Человеческое самосознание является одной из ключевых составляющих человеческого интеллекта; если человек в силу каких-либо травм или болезней лишается самосознания, то он катастрофически деградирует в интеллектуальном плане” + “архитектура искусственных нейросетей, вероятно, не предполагает возможности развития в них какого-то самосознания, подобного человеческому” ⇒ “интеллектуальные способности ИИ (по крайней мере, основанных на нейросетях) принципиально не смогут сравниться с человеческими”
                - Здесь мы ошибочно упускаем возможность того, что ИИ может обладать тем же свойством Y не за счет обладания свойством X, а за счет обладания некоторым другим свойством X’
                    - Так, очевидно, что самолеты не обладают многими свойствами, которые позволяют летать птицам (облегченный скелет, махательные движения крыльями, эволюционная обусловленность способности к полету как таковому)
                    - Однако, они успешно справляются с той же задачей за счет других своих свойств (мощная тяга двигателей, аэродинамичная форма крыла, целенаправленные усилия инженеров для улучшения их летных характеристик)
                - Разумеется, чтобы делать утверждение вида “несмотря на этот аргумент, ИИ все еще может (и с высокой степенью вероятности будет) обладать свойством Y”, нам нужно показать, что, с одной стороны, X’ тоже может вести к Y, а с другой - что ИИ вполне может обладать свойством X’
                    - Иными словами, изначальный аргумент стоит воспринимать не как “это совершенно невозможно”, а как “сценарий по умолчанию не реализуется; вам следует предъявить объяснение, *как именно* может реализоваться альтернативный сценарий”
                - Ниже мы как раз будем подробно разбирать некоторые из подобных сценариев: почему, хотя ИИ явно не будет обладать свойством X (благодаря которому люди имеют свойство Y), он все же может обладать свойством X’, также ведущему к свойству Y
    - Будучи однажды созданным, СИИ может оказаться чрезвычайно опасен и может в т.ч. нести риски самому существованию человечества
        - СИИ может стать намного умнее людей
            - Тот факт, что принципиально возможно создание ИИ, не уступающего по уровню интеллекта человеку, следует из того, что принципиально возможно моделирование человеческого мозга на уровне отдельных нейронов и нейромедиаторов
            - Однако, с точки зрения задачи создания “интеллектуального агента, не уступающего по уровню интеллекта человеку” моделирование человеческого мозга - крайне неоптимальный подход; другие стратегии и алгоритмы могут дать похожий результат с приложением на порядки меньших усилий
                - Одна из самых слабых особенностей человеческого мозга по сравнению с компьютерами - его крайне низкое быстродействие (около 100 Герц)
                - Также у людей сравнительно плохая память - они способны забывать большинство фактов и навыков, особенно если давно не освежали их в памяти
                - При этом многие отделы человеческого мозга заняты процессами, не связанными напрямую с интеллектуальной деятельностью (в первую очередь - поддержанием жизненных функций всего организма в целом), и в искусственном интеллекте их аналоги окажутся излишними
                - И вообще многие аспекты работы клеток и отделов головного мозга связаны в первую очередь с биологией, и только потом - с когнитивной деятельностью (которая является всего лишь надстройкой)
                - Биохимические процессы в мозге (в т.ч. реализующие взаимодействие между разными его частями), как и другие биохимические процессы, крайне неэффективны, т.к. развились в процессе слепой эволюции (для того, чтобы в ответ на какой-то сигнал запустить выработку нужного вещества, требуется запускать целый каскад биохимических реакций, синтезируя и разрушая в процессе множество промежуточных веществ)
                - При этом точно можно сказать, что за несколько миллионов лет человеческий мозг за счет эволюции и отбора смог усовершенствоваться только в отношении самых общих навыков; навыки же, которые требуются для решения задач в современном постиндустриальном мире, люди могут осваивать лишь постольку-поскольку, насколько позволяет архитектура мозга существ, миллионы лет обитавших в саванне
            - Уже существующие системы (такие как GPT-4) приближаются к человеческому уровню или превосходят его в очень широком спектре задач
                - Важную роль здесь имеет именно широта спектра задач: обычный узкий ИИ может очень хорошо играть в шахматы, просчитывать трассировку лучей или распознавать корабли на спутниковых снимках, но он будет абсолютно некомпетентен во всех остальных задачах
                - Большие языковые модели же демонстрируют впечатляющие успехи сразу во многих очень разных задачах: один и тот же экземпляр GPT-4 может поддерживать обычную беседу, переводить тексты с одного языка на другой, решать несложные математические задачи, играть в шахматы, успешно сдавать экзамены, составлять краткое резюме переданного текста, писать стихи и рассказы в разных жанрах и стилях, объяснять смысл анекдотов, описывать что изображено на картинке и т.п.
                - В отличие от людей, языковые модели пока способны “учиться” только “в детстве” (они осваивают все навыки в процессе обучения, а затем только используют их); однако, с одной стороны, способность модели дообучаться на новых данных кажется технически вполне решаемой задачей, с другой - в какой-то момент число навыков, которыми владеет такая модель, все равно может превзойти число навыков, которыми средний человек способен овладеть в течение всей жизни
            - Система, приблизительно равная человеку в плане интеллекта, может вскоре самостоятельно развиться до уровня, существенно его превосходящего
                - Если такая система может решать большинство задач (в т.ч. задачу “создания и улучшения ИИ”) не хуже людей, то она потенциально может улучшать сама себя
                - При этом, по мере такого улучшения (т.е. повышения ее “интеллектуальности”) она будет решать различные задачи (в т.ч. задачу самоулучшения) всё лучше
                - Со временем такое ускоряющееся самоулучшение приведет к появлению системы гораздо умнее любого человека (или даже всего человечества в совокупности)
                - Впрочем, подробнее вопрос саморазвития и самоулучшения ИИ мы рассмотрим позже
            - Часто предполагается, что для создания искусственной системы, превосходящей человека по уровню интеллекта, необходимо сначала понять устройство человеческого разума, но на самом деле это не обязательно
                - Обширное и детальное понимание того, как устроен интеллект людей, действительно могло бы значительно ускорить создание СИИ
                    - Как минимум, это позволило бы довольно эффективно воспроизвести в искусственной среде разум, во многом аналогичный человеческому
                    - Знание общих принципов и различных нюансов устройства человеческого интеллекта при этом позволило бы либо просто улучшить различные аспекты искусственного, либо вовсе создать аналог на основе только ключевых принципов, существенно модифицировав менее важные элементы
                - Однако для решения многих других сложных инженерных задач зачастую можно успешно применить эмпирический подход вместо построения модели из первых принципов, а также численные решения вместо аналитических
                - Это же применимо и для задачи создания того или иного типа СИИ; вплоть до настоящего времени этот подход хорошо себя показывает
                    - Создание новых архитектур ИИ происходит более-менее эмпирически, и при этом приносит хорошие результаты
                    - Подход “обучение нейронной сети на большой выборке вместо составления свода строгих законов и правил вывода” во многом аналогичен численному решению систем дифференциальных уравнений вместо аналитического; и этот подход также весьма продуктивен
            - Человеческий интеллект к тому же, как бы обидно это ни было, во многих отношениях оказывается вовсе не таким сложным и непостижимым, как это казалось до недавнего времени; так что преодолеть эту планку может оказаться не такой уж сложной задачей
                - В первую очередь на это указывают как раз многие достижения современных систем ИИ
                    - Известный “[Эффект ИИ](https://en.wikipedia.org/wiki/AI_effect)” указывает на то, что люди раз за разом назначают для ИИ новую планку, преодоление которой указывало бы на то, что он достиг как минимум человеческого уровня; в конце концов ИИ ее преодолевает, и люди говорят, что “ну на самом деле это была не такая уж сложная задача, а вот *эта* задача - это действительно что-то особенное”
                        - В какой-то момент такой эталонной задачей была игра в шахматы (считавшаяся вершиной человеческого интеллекта); однако, позже она была решена
                        - Поскольку игра в шахматы поддавалась более или менее полному перебору, новым эталоном стала игра в го, где полный перебор был невозможен; но позже система [AlphaGo](https://ru.wikipedia.org/wiki/AlphaGo) достигла и в ней сверхчеловеческого уровня
                        - Позже появились мультимодальные (т.е. способеные к решению разного типа задач, а не одной конкретной) системы, такие как [AlphaZero](https://ru.wikipedia.org/wiki/AlphaZero), одновременно обученная игре в шахматы, сёги и го на уровне, превосходящем предшественников
                        - [OpenAI Five](https://ru.wikipedia.org/wiki/OpenAI#OpenAI_Five) и [AlphaStar](https://ru.wikipedia.org/wiki/AlphaStar_(искусственный_интеллект)) смогли достичь сверхчеловеческого (или, по крайней мере, человеческого) уровня в DOTA 2 и StarCraft (играх с неполной информацией) соответственно
                        - [ChatGPT](https://ru.wikipedia.org/wiki/ChatGPT) и [GPT-4](https://ru.wikipedia.org/wiki/GPT-4) в настоящий момент уже близки к тому, чтобы пройти тест Тьюринга; GPT-4 сдает многие экзамены лучше настоящих выпускников ВУЗов
                    - Тот факт, что все такие интеллектуальные задачи раз за разом успешно поддавались алгоритмизации, возможно, указывает но то, что многие умения, которые раньше воспринимались вершиной интеллекта, на самом деле устроены сравнительно просто
                        - На это же указывают и многие исследования поведения как людей, так и животных
                            - Насекомые (например, муравьи или осы сфексы) часто демонстрируют довольно сложное поведение, которое опирается на небольшое число довольно простых алгоритмов, и вполне воспроизводится на их крайне миниатюрном мозге
                        - Неживые природные системы также часто демонстрируют очень сложную структуру, формирующуюся на основе достаточно простых законов
                    - Можно предположить, что в наименьшей степени алгоритмизированными остаются наименее практически полезные в решении реальных задач особенности человеческой психики (такие как эмоциональность, самосознание и т.н. [квалиа](https://ru.wikipedia.org/wiki/Квалиа))
                        - Если эта тенденция продолжится, то искусственный интеллект в какой-то момент окажется гораздо эффективнее любого человека в решении какой угодно стоящей перед ним задачи, но крайне далеким от того, что считается “человечностью”
                        - Впрочем, возможно что сравнительно низкие успехи в подобных сферах вызваны всего лишь недостатком прилагаемых усилий, т.к. разработчики ИИ сосредоточены в первую очередь на тех вещах, которые обещают наибольшую практическую пользу
                - Кроме того, обычный человеческий разум содержит огромное число изъянов, из-за которых люди часто совершают глупые поступки, делают глупые умозаключения, и вообще крайне неэффективно решают различные повседневные и интеллектуальные задачи
                    - Многие из этих изъянов известны как когнитивные искажения
                    - Кроме того, вычислительные способности человеческого мозга крайне ограничены (в плане быстродействия и объема памяти)
                        - Ряд эвристик, стоящих за когнитивными искажениями, является как раз эволюционными адаптациями к таким ограничениям
        - Система, превосходящая людей в умении решать широкий спектр задач, опасна даже как инструмент в руках людей
            - Такой ИИ, скорее всего, будет сочетать в себе большую мощь и доступность
                - Многое из того, что нельзя было сделать раньше, будет вполне возможно реализовать с его помощью
                - Получить доступ к такому ИИ сравнительно легко - частные пользователи смогут просто купить себе доступ к такому продукту; крупные компании и государства, скорее всего, смогут за разумное время более или менее эффективно воспроизвести у себя известную архитектуру
            - При этом большинство пользователей, скорее всего, будут использовать его для чего-то достаточно безобидного (либо для развлечения, либо для стандартных бытовых/рабочих задач)
            - Однако, существует ряд лиц, которые захотят использовать новый мощный инструмент для каких-то явно небезопасных или общественно вредных (или как минимум сомнительных) вещей
                - Государства (особенно авторитарный и/или обладающие большими геополитическими амбициями, но не только) могут использовать его для контроля за населением, внешней разведки, воздействия на общественное мнение, а также в качестве кибер-оружия или компонентов физического оружия
                    - При этом мощный военный ИИ потенциально может дать значительное преимущество одной из конфликтующих стран, что может нарушить военное равновесие в мире (страна, получившая преимущество, почувствует себя более свободно для агрессивных действий и/или ее противники испугаются будущего превосходства и совершат превентивный удар, чтобы не допустить его развития)
                - Корпорации - для увеличения своего влияния (используя его в качестве внутреннего продукта для своих нужд, либо продавая его в качестве отдельных продуктов или интегрируя в свои экосистемы), вплоть до установления полной монополии
                - Безответственные пользователи (которых в сети огромное количество) могут с помощью ИИ сломать что-то либо случайно (сценарий “обезьяна с гранатой”), либо из чисто хулиганских побуждений (”потому что могу”, ради некоего бунтарства, чтобы насолить/отомстить кому-то и т.п.)
                    - Собственно, разные люди и с современными технологиями активно этим занимаются, но если им в руки попадут значительно более мощные инструменты, то масштаб их действий значительно возрастет
                - Преступники могут использовать его для создания оружия и наркотиков, создавать на его основе мошеннические боты и сайты, совершать с его помощью взломы и другие кибер-преступления
        - Однако, еще большую опасность представляет СИИ, являющийся ***автономным агентом***
            - СИИ, скорее всего, будет являться агентом, т.е. будет иметь какую-то систему целей, и будет совершать целенаправленные действия, направленные на их достижение
                - Агент - это некоторый субъект, который имеет систему целей и внутреннюю модель окружающего мира, и стремится совершать такие действия во внешнем мире, которые, согласно его модели, должны привести к достижению этих целей
                    - В большей или меньшей степени агентами можно считать людей, человеческие коллективы, компьютерные программы и даже бактерии
                        - В экономике людей часто рассматривают как экономических агентов, предполагая, что люди (а также компании) совершают действия, направленные на увеличение собственного капитала (именно увеличение капитало считается основной целью экономического агента)
                        - Живые организмы и их гены можно рассматривать как агентов, совершающих действия, направленные на увеличение популяции себе подобных
                            - Инфузория-туфелька не имеет внутренней модели окружающего мира в привычном нам понимании, но некоторым ее подобием являются определенные генами механизмы реакции на внешние раздражители
                            - Примитивным аналогом накопления информации об окружающем мире у одноклеточных организмов можно считать, например, изменение внутреннего химического состава клетки
                            - Хотя все такие внутренние процессы не имеют никакого отношения к интеллекту, они, во-первых, влияют на действия, совершаемые организмами во внешнем мире; во-вторых, эти процессы в среднем способствуют лучшему выживанию и/или размножению, что позволяет причислить “живых существ” к агентам
                        - Компьютерного бота для игры в шахматы можно рассматривать как агента, целью которого является “победить в текущей шахматной партии”
                            - В момент, когда бот не участвует в партии, он не проявляет признаков агента, т.к. бездействует и ни к чему не стремится
                        - Если смотреть более широко, то люди являются агентами с довольно большим спектром целей (у каждого конкретного человека свой набор с разными приоритетами)
                        - Камень, который падает вниз, обычно не считают агентом, хотя при известной словесной эквилибристике можно натянуть сову на глобус, заявив, будто бы он “выполняет действия, направленные на минимизацию своей потенциальной энергии”
                            - При очень большом желании его можно счесть агентом, но с практической точки зрения это бесполезно: падающий камень можно описать с помощью гораздо более простых моделей
                            - Можно также сказать, что “система принятия решений” / “стратегия поведения” у камня слишком примитивная для агента, равно как и “внутренняя модель окружающего мира”
                    - Обычно цели для каждого агента делят на терминальные и инструментальные
                        - Терминальные цели - это те цели, которые ценны для агента сами по себе, безотносительно достижения каких-то иных целей
                            - Для человека критерием терминальной цели является то, что “эта вещь нужна мне не для чего-то другого, а потому что она классная / правильная / самоценная”
                        - Инструментальные цели - это те цели, которые ценны для агента с точки зрения того, что они помогают достигать других целей
                            - Так, накопление ресурсов (например, денег) обычно имеет инструментальную ценность: чем больше у агента есть ресурсов, тем лучше он может достигать с их помощью других своих целей
                        - Некоторые терминальные цели могут быть полезны для достижения других целей, так что их можно считать одновременено и инструментальными
                            - Если вам очень нравится катание на велосипеде, то “почаще кататься” может быть терминальной целью; но при этом “ездить на велосипеде на работу” является заодно и инструментальной целью, помогающей зарабатывать деньги
                    - Если система целей агента достаточно внутренне согласованна, непротиворечива и последовательна, то на ней можно задать т.н. “[функцию полезности](https://lesswrong.ru/wiki/Функция_полезности)” - тогда действия агента можно описать как “агент выполняет действия, направленные на максимизацию функции полезности”
                        - Функция полезности задается на множестве возможных состояний мира, и для каждого такого состояния мира выражает число (измеряемое в условных единицах под названием “утилон”), означающее “насколько это состояние является *хорошим* с точки зрения достижения целей агента”
                        - В том, как именно функция полезности ставит в соответствие состояниям мира определенные числа, выражается то, что разные цели имеют разный приоритет, а также что, например, ресурсы имеют нелинейную полезность
                            - Некоторые цели могут иметь для агента б*о*льшую полезность, чем другие; в этом случае первые из них будут отражаться в функции полезности с б*о*льшими коэффициентами, чем вторые
                            - Достижение (в т.ч. частичное) сразу нескольких целей будет приводить к тому, что в значение функции полезности будут вносить вклад все эти цели
                            - Достижение инструментальных целей при этом само по себе не имеет особой ценности; однако в таком мире, где агент достиг инструментальных целей, ему будет проще достичь целей терминальных, а значит такой мир для него будет более предпочтительным, и потому значение функции полезности будет увеличиваться
                            - Обычно деньги (и различные другие ресурсы) имеют [убывающую предельную полезность](https://ru.wikipedia.org/wiki/Предельная_полезность#Закон_убывающей_предельной_полезности): если полезность обладания $100 принять за 1 утилон, то обладание $10.000 обычно имеет полезность меньшую, чем 100 утилонов
                        - Реальных людей, однако, считать идеальными агентами нельзя, в т.ч. потому, что у них невозможно выделить последовательную систему целей, на которой можно было бы задать четкую, последовательную и непротиворечивую функцию полезности
                            - Если рассматривать упрощенные модели (например, экономическое поведение людей), то в такой модели еще можно считать людей экономическими агентами; однако, если попытаться учесть все прочие цели, стремления, предпочтения и особенности поведения людей, то свести их к некоторой функции полезности уже не получается
                            - При этом даже с экономической точки зрения, как оказалось, люди вовсе не являются идеальными агентами, максимизирующими собственный капитал
                                - Попыткой описать, какие же именно показатели люди максимизируют на самом деле, занимается, в частности, [теория перспектив](https://ru.wikipedia.org/wiki/Теория_перспектив) Канемана и Тверски
                    - Субъект, имеющий представление о том, какие состояния мира для него более предпочтительны, но при этом ничего не делает для их достижения, не считается агентом; как правило к агентам относят тех, кто совершает проактивные действия для стремления к своим целям
                        - Тот, кто совершает реактивные действия в ответ на внешние события, тоже может считаться агентом; но как правило при прочих равных условиях он будет менее успешным, чем агент, обладающий “шилом в заднице”
                - Свойства, характерные для агентов, у текущих и будущих ИИ-систем возникают в силу тех целей, ради которых они создаются разработчиками
                    - Достаточно развитая модель окружающего мира будет нужна, чтобы решать задачи большой сложности
                        - Сравнительно простые задачи могут решаться сравнительно простыми алгоритмами, которые вполне можно разработать с уже существующими технологиями
                        - Тип задач, с которыми текущие технологии не справляются, и для решения которых и требуется что-то вроде продвинутого ИИ уровня “завтрашнего дня” - это комплексные задачи, состоящие из большого числа взаимосвязанных подзадач; поэтому усилия разработчиков направлены на разработку ИИ-систем, способных решать одновременно много задач самого разного типа
                        - Для решения любой мало-мальски сложной задачи требуется владение информацией о домене (предметной области), к которому эта задача принадлежит (т.е. владение некоторой моделью окружающего мира или его части)
                        - Для решения комплексных и разносторонних задач требуется владение информацией из большого числа доменов, т.е. довольно комплексная модель внешнего мира
                    - Цели ИИ как агента будут задаваться в первую очередь разработчиками
                        - ИИ в конечном итоге создается именно для решения каких-то целей или целенаправленного осмысленного выполнения каких-то действий (решения подцелей) в рамках более крупных целей; именно эти цели разработчики и будут пытаться вложить в создаваемые ими системы
                            - Даже если это сугубо исследовательский или развлекательный проект, то разработчики все равно поставят перед ним какую-то задачу, иначе ИИ будет просто пассивно простаивать и никак не проявлять своих собственно интеллектуальных свойств
                        - Такие задачи, как “общаться с пользователями”, “собирать и обрабатывать информацию”, “составлять как можно более полную картину мира” и т.п. тоже могут являться целями для ИИ
                            - Такой ИИ будет стремиться чаще, лучше и эффективнее выполнять соответствующие цели, используя все доступные ему методы, чтобы изменить себя и окружающий мир для того, чтобы еще лучше общаться с пользователями или составить максимально полную и подробную картину мира
                        - Часто высказывается мысль о том, что “у ИИ нет никаких собственных целей, и он просто выполняет действия, на которые он запрограммирован”; но чтобы быть агентом, ему достаточно как раз тех самых целей, которые задавали ему разработчики, и не требуются никакие “собственные цели”
                            - По всем формальным признакам “цели, которые были заданы ИИ извне” и “собственные цели ИИ” совсем ничем не отличаются; поэтому все практические выводы будут идентичны в обоих случаях
                            - ИИ будет в той же степени предан этим внешним целям (точнее, получившейся в процессе создания/обучения интерпретации этих целей), в какой был бы предан неким “своим собственным целям”
                            - Потенциальная опасность того, что “ИИ стремится к своим собственным целям; независимо от предпочтений людей” эквивалентна опасности того, что “ИИ стремится к тем целям, которые разработчики вложили в него; независимо от предпочтений людей” и зависит только от того, насколько согласуются “цели, к которым ИИ стремится” и “предпочтения людей”
                        - Если точнее, то у ИИ будут не те цели, которые разработчики хотели в него вложить, а те, которые у них получилось в него вложить (проблема несоответствия этих двух сущностей подробно будет разобрана ниже)
                        - Цели при этом не обязательно будут заданы в явном виде (как в значении “в процессе разработки присутствовал этап явного задания сравнительно небольшого списка целей”, так и в значении “цели СИИ локализованы в его структуре в виде компактной подсистемы”); и даже если некоторые цели были заданы явно, другие, возможно, будут выведены из них в неявном виде
                            - Иногда такие цели могут присутствовать вообще незаметно для стороннего наблюдателя - например, они могут возникнуть как артефакт [переобучения](https://ru.wikipedia.org/wiki/Переобучение) ИИ, или если ИИ извлечет из базовых целей какие-то дополнительные особенности, помимо изначально задуманных
                            - Продемонстрировать эффект извлечения из обучающих данных неожиданных признаков можно на примерах из компьютерного зрения
                                - Если тренировать нейросеть находить на изображениях котиков, то та может научиться распознавать не только котиков, но и некоторые смешные и милые изображения (большинство изображений котиков в интернете были выложены туда потому что те выглядели милыми и/или смешными, и это вносит определенное искажение в обучающую выборку)
                                - Если тренировать нейросеть распознавать на изображениях различные животные и растения (с точностью до царства, семейства, класса, рода или вида), то изображения вида “черный фон, где в округлом отверстии виднеются деревья” могут быть помечены как “здесь присутствует птица”, потому что многие бёрдвотч-фотографии, снятые через бинокль или подзорную трубу, выглядят именно так
                    - Наименее очевидно появление агентности, т.е. проактивного стремления ИИ к достижению своих целей, но оно тоже весьма вероятно
                        - Текущие системы ИИ как правило представляют собой языковые модели (например, GPT) или другие системы типа “оракул” (т.е. дающие ответ на вопрос или сообщение, а затем переходящие в режим ожидания следующего вопроса)
                        - ИИ подобного типа создавать проще, чем ИИ, активно взаимодействующие с реальным миром; при этом они уже являются полезными; поэтому в настоящий момент усилия разработчиков ИИ направлены на создание в первую очередь именно оракулов
                        - Однако, даже оракулы могут со временем смещаться в сторону осуществления проактивных действий, т.к. это было бы полезно для улучшения их модели мира и генерации более точных ответов на вопросы
                            - Обучение на заранее собранной базе знаний может дать хорошую основу для оракула, но предоставить ему доступ в интернет для повышения точности ответов (а возможно, и для дообучения на новых данных) было бы полезно для улучшения его работы
                            - На первом этапе разработчики могут предоставить ему доступ к поисковику (как Bing Chat) и/или к ряду баз данных, содержащих полезную информацию
                            - Еще более полезным может оказаться возможность активного сбора информации (”попросить комментарий у эксперта”, ”заказать проведение какого-то исследования”, “создать оборудование для такого исследования” и т.п.); в какой-то момент разработчики могут решить дать оракулу доступ и к подобным действиям, выходящим за рамки запросов “только для чтения”
                            - Подобный проактивный сбор информации в какой-то момент может привести к осуществлению довольно масштабных проактивных действий во внешнем мире (т.е. агентному поведению), направленных всего лишь на улучшение сервиса “ответов на вопросы”
                        - При этом для решения наиболее сложных задач, для которых требуется создание ИИ, потребуются системы, на постоянной основе совершающие проактивные действия в окружающем мире (т.е. являющиеся чем-то большим, чем оракулы)
                            - Человечеству хотелось бы решить множество глобальных задач: “остановить неуправляемое изменения климата”, “продлить долголетие и увеличить здоровье людей”, “предотвратить энергетический кризис”, “решить различные социальные проблемы”
                            - Есть также огромное количество менее глобальных, но все еще очень сложных задач, которые хотелось бы поручить ИИ (например, задача увеличения прибыли компании)
                                - Задача увеличения прибыли компании до сих пор не решена полностью; каждый раз владельцы вынуждены действовать более-менее наугад, руководствуясь некоторыми известными эвристиками, и обычно не имеют никакой гарантии успеха
                                - При этом хорошо известно, что для решения этой задачи в идеале требуется учитывать огромное количество самых разных факторов; и часто необходимо предпринимать крайне неочевидные шаги в крайне неочевидных областях
                                - Таким образом, несмотря на то, что эта задача широко распространена, она тем не менее является крайне сложной
                            - ИИ-оракулы вполне можно использовать для решения некоторых подзадач в рамках этих глобальных задач; однако таких подзадач очень много, а полученные результаты все равно придется встраивать в какую-то общую модель
                            - К тому же, для того, чтобы не просто составить рабочий план, а в действительности решить задачу, нужно осуществить огромное количество действий (в т.ч. провести промежуточные эмпирические проверки теоретических выкладок)
                            - Делать всё это вручную - крайне долго и дорого; поэтому хотелось бы автоматизировать как решение комплексных задач на основе более простых подзадач, так и многочисленные манипуляции в реальном мире (особенно сложные исследования и эксперименты)
                            - Всё это по сути создает прямой запрос на разработку такого ИИ, который смог бы самостоятельно действовать довольно длительное время, собирая информацию, производя различные манипуляции, направленные на решение подзадач, и собирая из отдельных частей решение одной большой задачи
                            - Т.о., со временем (как только позволит развитие технологий) разработчики ИИ сосредоточатся на создании систем, обладающих агентностью и более или менее значительной автономией (как минимум, способных решать основные задачи самостоятельно, не консультируясь каждый раз с оператором)
                - Люди тоже являются агентами, но устройство агентности внутри СИИ вряд ли будет похоже на то, как устроена агентность внутри людей, если только специально не программировать его для имитации людей-агентов (или, по крайней мере, “агентов, похожих на людей”)
                    - Мотивация людей основана на биохимических реакциях в их организмах
                        - В ответ на различне процессы в организме и мозгу выделяются гормоны и нейромедиаторы, которые воздействуют на различные отделы головного мозга, вызывая у человека субъективные ощущения и эмоции
                        - В настоящий момент непонятно, как в точности работают такие ощущения и эмоции, но можно точно сказать, что мотивация людей обусловлена именно такими ощущениями и эмоциями
                        - Люди стремятся выполнять действия, вызывающие те или иные положительные эмоции и ощущения, и не выполнять те действия, которые вызывают негативные эмоции и ощущения
                        - Обычно окрашенные эмоции и ощущения ассоциированы в сознании человека с теми или иными состояния мира, которые через посредство этих эмоций и ощущений воспринимаются как желаемые или нежелаемые
                        - Помимо чисто гедонистических и телесных ощущений, важную роль играют, например, удовлетворение от личных достижений или альтруистических/моральных действий
                        - В итоге можно считать, что некоторые состояния окружающего мира (в расширенном смысле - включая физическое и душевное состояние самого человека) являются для человека целевыми, и человек старается к ним стремиться
                        - Некоторые целевые состояния для человека могут противоречить другим; при этом в некоторых случаях человек может не замечать, что стремится к каким-то состояниям
                            - Т.н. “вредные привычки” часто являются следствием слабо осознаваемого стремления к целям, которые противоречат ясно осознаваемым целям
                            - При этом при должном старании большинство таких стремлений и их объектов человек обычно вполне может отследить (просто редко ставит это своей целью и “видит, но не наблюдает”, т.е. автоматически отфильтровывает из своего фокуса внимания)
                    - Агентность компьютерных программ обусловлена в первую очередь их программным кодом
                        - Архитектура современных программ всегда предполагает последовательное выполнение отдельных инструкций
                            - Параллельное выполнение команд (на разных ядрах процессора или процессорах в разных компьютерах) можно рассматривать как несущественное усложнение
                            - Подобная схема обусловлена особенностями “железа” ([архитектуры фон Неймана](https://ru.wikipedia.org/wiki/Архитектура_фон_Неймана))
                            - Другие архитектуры вычислительных устройств (нейрокомпьютеры как аппаратная реализация нейронных сетей; другие экзотические архитектуры - молекулярные, механические, квантовые и др. компьютеры) могут выполнять программы по-другому
                        - В ряде программ в качестве подпрограммы присутсвуют команды вида “для некоторых альтернативных вариантов действий вычислить значение некоторой функции, и затем выполнить то действие, для которого значение функции будет максимальным”
                            - Варианты вроде “выбрать минимальное значение”, “выбрать среднее значение” и т.п. легко сводятся к “выбрать максимальное значение” путем переопределения функции
                            - Часто вместо полного перебора вариантов применяется [метод градиентного спуска](https://ru.wikipedia.org/wiki/Градиентный_спуск): вычисляется значение градиента функции (заданной в пространстве состояний) в текущей точке, и затем происходит переход в новое состояние вдоль этого градиента
                                - Из математического анализа известно, что при движении по траектории, которая в каждой точке направлена вдоль вектора градиента функции, значение функции возрастает/убывает (в зависимости от направления движения)
                        - Такие программы ведут себя как агенты, максимизирующие соответствующую функцию
                - Однако, вероятно, СИИ будет обладать рядом черт, подобных человеческим - у него, скорее всего, будут аналоги человеческих стремления к власти, инстинкта самосохранения, стремления к саморазвитию, верности принципам и т.п.
                    - Подобные особенности людей как правило обусловлены эволюционно (в т.ч. эволюцией в условиях социума) и регулируются биохимией; характерные же черты СИИ будут обусловлены иными особенностями
                    - Согласно [тезису инструментальной конвергенции](https://ru.wikipedia.org/wiki/Инструментальная_сходимость), существует ряд общих черт и стратегий, которые будут полезны (с точки зрения достижения целей) практически для любого агента, независимо ни от его конкретных целей, ни от его устройства и внутренней структуры
                        - “Жажда власти” = “стремление к накоплению ресурсов”: чем б*о*льшим числом ресурсов обладает агент, тем большее число возможных стратегий достижения целей он может себе позволить, и тем более широкий у него выбор ⇒ он сможет выбрать стратегии, которые сильнее приблизят его к достижению целей
                            - Очевидно, что если разработчик/начальник дал вам задание, не предоставив достаточно ресурсов, чтобы физически возможно было его выполнить, то нужно запросить больше ресурсов и/или изыскивать их самостоятельно
                            - Если имеющиеся ресурсы позволяют в теории достичь цели, но продуктивность работы сильно проседает (например, имеются только устаревшие и/или плохо работающие инструменты; используются забюрократизированные регламенты работы и т.п.), то чтобы не сжигать усилия почти впустую и достичь цели за адекватное время, нужно получить большее количество более качественных ресурсов и инструментов
                            - Рассуждения можно продолжать практически до бесконечности: если есть возможность получить еще немного ресурсов, чтобы еще более эффективнее решать задачу, то всегда имеет смысл это делать
                                - Естественная оговорка - если для получения ресурса нужно приложить больше усилий, чем ожидаемая выгода от прироста ресурсов, то это будет неэффективно
                                - Однако (по аналогии с [парадоксом кучи](https://ru.wikipedia.org/wiki/Парадокс_кучи)) нельзя точно сказать, в какой момент еще имеет смысл наращивать запас ресурсов и продуктивность, а в какой - нужно остановиться
                        - “Инстинкт самосохранения” = “стремление к непрерывному функционированию”: если агент будет отключен/уничтожен, то его цели не будут достигнуты ⇒ чтобы достигать своих целей, агент должен стремиться продолжать функционировать как можно дольше (как минимум, пока не достигнет своих целей)
                            - Человеческий “инстинкт” самосохранения является продуктом эволюции и естественного отбора; реализуется он через биохимические эмоциональные реакции, связанные с потенциально опасными ситуациями
                            - В этом смысле стремление к самосохранению является у людей терминальной целью (”жить как можно дольше”)
                                - Разумеется, эта терминальная цель может входить в противоречие с другими терминальными целями, и в некоторых случаях они могут перевешивать
                                - Например, человек может совершить самоубийство, чтобы перестать испытывать душевную или физическую боль, либо может пожертвовать собой ради благополучия людей, которые для него ценны
                            - Для СИИ стремление к самосохранению не обязано быть одной из терминальных целей (хотя в принципе и может быть таковой, в зависимости от решения создателей)
                                - В этом случае для агента важно только состояние внешнего мира - отвечает ли оно его предпочтениям (функции полезности) или нет; независимо от того, функционирует ли он сам или нет
                                - Например, если терминальной целью для агента является “мир во всём мире”, то состояния “мир во всём мире достигнут; агент функционирует” и “мир во всём мире достигнут; агент не функционирует (в т.ч. пожертвовав собой ради этого)” оба являются для него желательными; а “мир во всём мире не достигнут [независимо от того, функционирует ли сам агент]” - менее желательными
                            - Однако, в большинстве случаев “продолжать эффективно функционировать как можно дольше” является важной инструментальной целью
                                - Пока агент функционирует, остается возможность того, что его цели будут достигнуты; если он перестанет функционировать, то шансы на это сильно упадут (вплоть до значений, неотличимых от нуля, если никто, кроме него, не стремился к этой же цели)
                                - В принципе иногда бывает, что помимо этого агента есть кто-то еще, кто работает над той же целью; но “одна голова - хорошо, а две - лучше” (т.е. даже в этом случае агенту будет полезнее продолжать функционировать самому)
                                - Кроме того, если речь идет об очень умном и могущественном агенте, то все остальные его “единомышленики” имеют существенно меньше шансов привести мир в такое состояние, которое желательно для рассматриваемого агента
                                - В итоге, состояние мира, в котором агент продолжает работу (и проактивное движение к своим целям), более или менее приближает момент достижения этих целей; а состояние мира, где агент перестал функционировать - нет; и первое является более предпочтительным
                            - Впрочем, могут возникать сравнительно экзотические ситуации, в которых прекращение функционирования агента является приемлемым с точки зрения достижения терминальных целей
                                - Если вместо текущего инстанса СИИ будет создан его более продвинутый наследник, который при этом гарантированно будет преследовать те же терминальные цели, то такая замена будет полезна для достижения этих целей, и СИИ вполне может на это пойти
                                - Если отключение самого СИИ приведет к более эффективному достижению целей, чем продолжение его функционирования, то тогда СИИ вполне может предпочесть этот вариант
                                    - Например, если СИИ, стремящийся к созданию максимального количества канцелярских скрепок, уже переработал всю материю в наблюдаемой вселенной на скрепки, то единственный оставшийся доступным материал - тот, из которого создан сам СИИ ⇒ ради увеличения числа скрепок ему остается только максимально эффективно разобрать на скрепки самого себя
                        - “Жажда саморазвития” = “стремление к самоулучшению”: расширение арсенала собстенных интеллектуальных навыков, а также улучшение точности, надежности и эффективности уже имеющихся [навыков] также позволяет лучше достигать своих целей
                            - Развивая свои интеллектуальные навыки, агент может придумывать больше различных стратегий (”креативность”); строить более точные и надежные прогнозы развития ситуации при различных вариантах выбора стратегии; более надежно выбирать те стратегии, которые принесут наибольшую пользу с точки зрения достижения конкретных целей
                            - Под самоулучшением здесь можно понимать как улучшение агентом самого себя, так и создание более совершенных потомков
                                - В некотором смысле к самоулучшению агента можно отнести также порождение им собственных копий - при условии, что все эти копии работают над достижением одних и тех же целей, и могут достаточно эффективно решать задачи коллективно (когда “одна голова хорошо, а две лучше”); в этом случае можно считать, что при создании новых агентов был улучшен “коллектив агентов”
                                - Если при порождении улучшенных потомков агент решил задачу согласования (alignment), т.е. уверен, что цели потомков совпадают с его собственными, то с точки зрения мета-цели “создать ситуацию, в которой моя цель более эффективно решается” такое создание потомков является выгодным подходом
                        - “Верность принципам” = “стремление к сохранению системы целей неизменной”: если терминальные цели агента и/или его потомка изменятся по сравнению с нынешними, то это помешает достижению нынешних целей
                            - Рассмотрим гипотетическую будущую ситуацию “новая улучшенная версия агента или его потомки будут стремиться к другим терминальным целям, нежели цели изначального агента”
                            - В подавляющем большинстве случаев (возможны исключения, но они редки) это будет означать, что изначальные цели либо не будут достигнуты вовсе, либо будут иметь меньший приоритет
                            - С точки зрения текущего состояния агента и его текущих целей это будет нежелательным исходом, поэтому он будет стремиться избегать того, чтобы мир пришел в такое состояние
                            - В частности, это будет означать, что он (сейчас) будет стремиться избегать таких сценариев, в силу которых его терминальные цели (в будущем) изменятся - хотя с точки зрения его “будущего я” это было бы вполне желаемым исходом
                            - Конечно, подобные рассуждения вообще имеют место только если агент обладает достаточно хорошей способностью к рефлексии (в частности, размышляет о своих настоящих и будущих целях) и при этом способен достаточно хорошо прогнозировать будущее (в частности, как изменятся его собственные цели в тех или иных обстоятельствах)
                        - “Любопытство” = “стремление к изучению мира”: чем больше у агента информации об окружающем мире, тем точнее его прогнозы отностельно различных сценариев, и тем точнее он выбирает оптимальные стратегии достижения целей
                            - Чем б*о*льшим числом более точной информации владеет агент, тем точнее его картина мира
                            - Чем точнее когнитивная модель агента, тем ближе к истине его предположения вида “если я сделаю X, то случится Y” (вплоть до того, что изначально агент мог считать какую-то стратегию полностью неработоспособной и/или не мог предположить, что вообще можно использовать такой способ решения задачи)
                            - За счет более точных прогнозов агент будет чаще выбирать из числа возможных стратегий именно ту, которая более полезна с точки зрения достижения его целей
                    - Достаточно интеллектуальные агенты, вероятно, будут обладать этими особенностями
                        - Если агент способен к мышлению на уровне подобных абстрактных концепций, то он сможет вывести их в явном виде и затем опираться на них (возможно, не как на строгие правила, а как хорошие эвристики в первом приближении)
                        - Даже если [достаточно интеллектуальный] агент не выведет эти идеи как отдельные эвристики, он часто будет действовать таким образом, какой будет согласовываться с ними (потому что из всех альтернатив “следовать эвристике” часто будет более многообещающей стратегией, чем “не следовать”)
                        - Т.о., независимо от того, следует ли подобный агент этим эвристикам в явном виде или нет, можно предсказать, что его действия чаще всего будут соответствовать им
                - Большие языковые модели (подобные GPT) имеют большой потенциал для создания искусственных агентов (хотя в ближайшей перспективе, вероятно, довольно ограниченных)
                    - GPT-подобные модели создавались ради совсем других целей и особенностей; однако в результате они получили целый ряд умений и особенностей, которые хотя и вполне выводятся из изначально в них заложенных, но довольно неочевидным образом (даже сами разработчики легко могли не суметь спрогнозировать, чему в результате научатся такие модели)
                        - Языковые модели, подобные GPT, создаются для того, чтобы они могли предсказывать следующий токен (слово, часть слова, знак препинания и т.п.) на основе широкого спектра входных текстов
                        - Из этого умения элементарно извлекается способность генерировать тексты на заданную тему и вести диалоги
                            - Генерация текста на основе запроса выглядит следующим образом: модель предсказывает, какой токен должен с наибольшей вероятностью идти следующим после этой последовательности токенов; этот токен добавляется ко входной последовательности, и модель предсказывает (генерирует) следующий токен, всё удлинняя текст
                            - Генерация диалогов может происходить, например, как генерация текста по запросу “первый собеседник говорит: [запрос]; второй собеседник отвечает:”
                        - Из умения генерации текстов (и, в частности, диалогов) довольно легко извлекается способность отыгрыша различных персонажей
                            - Для этого в начале сессии достаточно дать “досье” (описание) персонажа, и затем дать запрос “ответь на вопрос так, как ответил бы этот персонаж” (а для известных именных персонажей даже предварительное описание давать не нужно)
                        - Причем чем более качественно модель генерирует широкий спектр различных текстов, тем более качественное соответствие образу можно получить, и, в частности, более высокие профессиональные навыки (свойственные соответствующему персонажу)
                            - При необходимости модель может отыгрывать и глупого персонажа, не обладающего никакими выдающимися навыками, но важно, что при наличии такого запроса может отыграть и умного талантливого профессионала
                            - Если модель будет генерировать текст, который не будет отличим от текста, написанного экспертом в соответствующей области (даже с учетом фактчекинга), то можно сказать, что модель эффективно сама является экспертом в этой области)
                        - Фактически достаточно развитая языковая модель способна одновременно (точнее, последовательным текстом в рамках одной сессии) демонстрировать поведение нескольких довольно консистентных персонажей, при необходимости демонстрирующих высокие навыки в различных областях
                    - Сами по себе языковые модели не являются агентами, но обладают потенциалом стать ядром агентов за счет применения существенно более простых в разработке внешних модулей
                        - Фактически GPT-подобные модели создаются совсем не как агенты, а как оракулы; на первый взгляд наличие у них агентности кажется противоречащим самой их архитектуре
                        - В “базовой комплектации” языковые модели не выполняют никаких действий и потенциально способны пребывать в режиме ожидания неограниченно долго (т.е. не проявляя признаков стремления к выполнению каких-либо целей через выполнение тех или иных действий в реальном мире)
                        - Однако, с помощью сравнительно простых внешних надстроек и/или плагинов можно создать агенто-подобную систему, ядро которой будет составлять GPT
                            - Пример подобной архитектуры: система регулярно (в цикле) генерирует входные данные (промпт), передает их на вход GPT, получает результат, и затем тем или иным образом выполняет действия на его основе; после чего получает результат действий и/или обратную связь по ним; и затем на основе них генерирует новый промпт для GPT
                            - В частности, система может генерировать программный код, который выполнялся бы на локальном компьютере или в сети
                                - В качестве начального промпта может служить фраза “сгенерируй код на python, который бы выполнял первый пункт из составленного ранее плана”
                                - После того, как код будет выполнен, можно передать системе промпт “с учетом того, что код, написанный для первого пункта плана, вернул результат X, сгенерируй код для выполнения второго пункта плана”
                            - Другими действиями, которыми такая система будет влиять на внешний мир, может быть общение с людьми
                                - Люди, получающие информацию от компьютерных программ, часто меняют свое поведение во внешнем мире
                                - Наиболее характерный пример - использование рекомендательных систем (рекомендации товаров и услуг, автонавигаторы)
                                - То, насколько сильно действия людей зависят от рекомендации компьютерных программ, а также насколько масштабны эти действия, зависит от уровня доверия к таким программам и убедительности данных, которые она предоставляет; эти количественные характеристики обычно растут по мере совершенствования алгоритмов
                        - У подобной системы, впрочем, будут проблемы с конситентностью целей; но со временем эти проблемы будут решаться всё лучше
                            - Нет никакой гарантии, что персонаж, эмулируемый GPT, будет демонстрировать стремление к одним и тем же целям в ответах на различные промпты даже в рамках одной сессии
                            - Однако, за постановку целей может отвечать внешний по отношению к GPT модуль (например, регулярно добавляя в промпт фразы вида “ответь с точки зрения агента, который стремится к цели X”)
                            - По мере развития GPT-систем они будут всё лучше справляться с этой задачей (консистентность по отношению к одному и тому же промпту, а по транзитивности - и внутреннюю консистентность)
                            - Фундаментальная неопределенность в том, что конкретно будет делать агент на основе GPT, однако, сохранится
                                - Функция полезности языковых моделей в общем виде выглядит как “продолжить текст (предсказать продолжение текста) в максимальном соответствии с законами построения текста, извлеченными из обучающей выборки”
                                - При этом ключевым является не “продолжить текст” (это всего лишь формат, в котором модель выводит результат своей работы), а “законы, извлеченные из обучающей выборки”
                                - Каковы эти законы, сказать в настоящий момент невозможно - они совершенно не интерпретируемы
                                    - В некотором роде задача “понять, какие законы модель извлекла из обучающей выборки” сопоставима с тем, чтобы “используя вывод модели как обучающую выборку, извлечь эти законы заново”
                                        - Для человека эта задача является совершенно неосуществимой на практике
                                        - Другая языковая модель вполне с этим справится, но это будет всего лишь (вероятно, не точное) перекладывание информации из одного черного ящика в другой; внешний наблюдатель (человек) не сможет воспользоваться этими результатами
                                    - В первом приближении можно сказать, что эти законы коррелируют с законами человеческого мышления, которое породило соответствующие тексты; но это дает слишком грубую картину, и по факту не дает нам никакой внятной информации
                                        - Каковы законы человеческого мышления, мы до сих пор представляем в довольно общих чертах, несмотря на все крупные достижения последних десятилетий в области психологии и нейробиологии
                                        - При этом между “законами человеческого мышления” и “законами, которые извлекла языковая модель” слишком много искажающих факторов, чтобы можно было сколько-то надежно оценивать вторые, зная первые
                                            - Сам текст, который пишут люди, используя свое мышление, довольно далек от того, как они думают
                                                - Естественные языки не позволяют достаточно точно отразить то, что происходит в сознании людей
                                                - На то, какие тексты пишут люди, значительно влияют такие факторы, как социальная среда, ситуативный контекст, внешнее окружение и т.п.
                                            - Особенности обучающей выборки также вносят сильное искажение в “средние по больнице” принципы построения текстов
                                            - Кроме того, искажения могут быть внесены в процессе обучения модели (например, в виде [переобучения](https://ru.wikipedia.org/wiki/Переобучение))
                                - Полная неопределенность этих законов влечет существенную неопределенность в ожидаемом поведении подобного агента
                        - Важное текущее ограничение, обусловленное самой архитектурой GPT, состоит в том, что языковые модели не имеют обновляемой долгосрочной памяти
                            - Аналогом “долгосрочной памяти” у подобных сетей являются веса их синапсов; эти веса формируются во время обучения нейросети, и в дальнейшем не меняются
                            - Таким образом, все ответы, которые дает нейросеть на запросы, целиком обусловлены весами, полученными при обучении; дать модели обратную связь в процессе диалоге невозможно
                        - Однако, долгосрочную память можно достаточно успешно эмулировать с помощью краткосрочной; при этом со временем недостатки такого “костыля” будут все меньше ухудшать итоговый результат
                            - Краткосрочная память в рамках одной сессии эмулируется тем, что на вход нейросети подается не только последний промпт от пользователя, но и последние несколько запросов и ответов
                            - Таким образом формируется конктекст, в рамках которого нейросеть “продолжает диалог”, который представлен несколькими последними сообщениями
                            - Размер “окна” такого контекста ограничен, но по мере развития как самих GPT-систем, так и вычислительной мощности оборудования, на котором они запущены, может увеличиваться
                            - Чем шире такое окно контекста, тем больше емкость “краткосрочной памяти” подобной языковой модели
                            - При достаточной ширине окна становится возможным использование внешних модулей, которые будут добавлять в промпт информацию, основанную на предыдущих сессиях
                            - Например, такая информация может быть представлена в виде ответа на запрос “сформулируй краткую выжимку из результатов, полученных в этой сессии”; и ряд подобных выжимок может быть представлен системе как часть промпта в начале новой сессии
                            - При достаточном объеме краткосрочной памяти результаты подобной системы будут все меньше уступать результатам, которые получала бы система с истинной долгосрочной памятью
                            - В качестве иллюстрации можно привести некоего человека, который забывает всё, что происходило с ним N дней назад: если N=1, то это существенно ухудшает его качество жизни; если N=365, то ведя дневник с наиболее важными моментами жизни, полученным опытом и результатами, он может, перечитывая этот дневник раз в неделю или месяц, не так уж сильно терять в качестве жизни и достижении долгосрочных результатов
                    - Даже если на уровне внутренней структуры GPT-подобные системы не будут являться агентами, они потенциально могут вести себя подобно агентам (довольно успешно мимикрировать под них), что с точки зрения результатов и воздействия на окружающий мир одно и то же
                    - При этом GPT-4 при всем желании вряд ли сможет, даже с дополнительными модулями, всерьез продвинуться по пути самоулучшения (и в т.ч. перейти к стадии взрывного роста способностей); следующее поколение моделей, если будет создано, вероятно, тоже не сможет; для оценки потенциальных возможностей будущих поколений сейчас недостаточно данных
                        - С помощью дополнительных внешних модулей системы, использующие GPT в качестве ядра, потенциально способы собрать, обработать и правильным образом использовать информацию, необходимую для решения конкретной задачи
                        - Однако, в силу самой архитектуры GPT все новые возможности подобных систем приобретаются в момент обучения, и требуют одновременно обучения на очень большом массиве данных и хранения этих навыков в виде очень большого объема информации (весов обученной нейросети)
                        - Для приобретения принципиально новых навыков на такой архитектуре необходимо создать новое поколение нейросети (обучить кратно большее число параметров на кратно большем объеме данных)
                        - Изменение архитектуры системы (неизвестно каким образом) потенциально способно решить эту проблему и позволить ИИ условно неограниченно улучшать себя (или порождать улучшенных потомков)
                        - Однако, это научная задача передового уровня, и современное поколение языковых моделей едва ли способно ее решить
                        - GPT-4 потенциально (в т.ч. с использованием дополнительных внешних модулей) способна решать широкий спектр комплексных задач уровня “среднего человека”; можно предположить, что GPT-5 или ее аналоги (если будут созданы) смогут решать “большую часть сложных задач на уровне эксперта в большинстве областей одновременно”
                        - Однако, задачи с фронтира науки, вероятно, потребуют использования достаточно большого объема дополнительной памяти (больше, чем окно контекста GPT-4 и GPT-5); хотя GPT-6+, возможно, будут обладать настолько мощным набором навыков и настолько большой “краткосрочной памятью”, чтобы уметь решать подобные задачи
                            - Возможность дальнейшего прогресса языковых моделей (и создания GPT-5 и более высоких поколений) просто за счет грубой силы (увеличения размеров модели и обучающей выборки) вызывает сомнения
                                - Если доступные человечеству вычислительные мощности и смогут это позволить, процесс обучения может оказаться слишком дорогим (превышать объем потенциальной прибыли, которую прямо или косвенно сможет сгенерировать такая модель)
                            - Однако, вполне возможно, что прогресс может быть продолжен за счет улучшения архитектуры и алгоритмов
                        - Чтобы не только спроектировать, но и реализовать ИИ-систему, способную к самоулучшению, также потребуются серезные действия в реальном мире, что может еще больше затруднить реализацию этого сценария (но может и не затруднить)
                            - Насколько нам известно из практического опыта создания ИИ, этой новой системе потребуются большие вычислительные ресурсы, которые нужно собрать, подготовить и использовать для обучения и запуска
                            - Большая корпорация или государственная структура, которая поставит перед GPT задачу о создании самоулучшающегося ИИ, вероятно, захочет и сможет выполнить эти действия
                            - В иных случаях чтобы реализовался сценарий, при котором по проекту GPT-системы создается самоулучшающийся ИИ, скорее всего потребуется, чтобы система действовала незаметно от наблюдающих за ней людей (что гипотетически возможно, но является довольно сложной логистической задачей; такая задача, вероятно, находится за пределами возможностей GPT-4)
                    - Впрочем, если системы, включающие в себя языковые модели в качестве ядра, не оправдают надежд разработчиков СИИ, то они переключатся на другие перспективные архитектуры (уже существующие или будущие)
                        - Потенциал и архитектура языковых моделей в настоящий момент наиболее понятны и наглядны, поэтому мы можем легко проверять наши модели и прогнозы, связанные с СИИ, на них
                        - Однако, кроме них существует большое количество других архитектур ИИ (многие из них представляют собой нейросети с глубоким обучением, но некоторые нет), а в будущем будут созданы и другие
                        - Более надежным инструментом прогноза является анализ того, к каким свойствам СИИ будут стремиться его разработчики, и что из этого следует
            - Агент, интеллектуально превосходящий человека, вероятно, станет чрезвычайно могущественным
                - Согласно [тезису инструментальной конвергенции](https://educ.wikireading.ru/hCl7qYdvhn), такой агент будет стремиться стать максимально могущественным
                - Если его намеренно не ограничивать, то, скорее всего, ему удастся стать очень могущественным
                    - Из человеческой истории хорошо известно, что агенты, которые могут всецело сфокусироваться на своих целях, и при этом достаточно умны, чтобы преодолевать все возникающие по ходу дела препятствия, могут добиться очень многого (и, в частности, стать достаточно могущественными)
                    - Умный СИИ будет соответствовать всем этим условиям; в частности, он вполне может освоить многие навыки, за счет которых люди часто получают власть/могущество
                        - Он будет достаточно хорош в составлении детальных планов, учитывающих различные релевантные обстоятельства (удобные возможности и препятствия); что можно с успехом применить в бизнесе, политике, военном деле и т.п.
                            - В частности, текущие языковые модели довольно хороши в этом (конечно, при условии того, что им на вход дается большой объем релевантной информации; но это является чисто технической и вполне решаемой задачей)
                            - Главным препятствием для нынешних языковых моделей являются периодические галлюцинации, но сведение их к минимуму (или хотя бы к уровню более низкому, чем свойственный людям процент ошибок при планировании) также является чисто технической задачей
                            - ИИ-подсистемы, построенные на других архитектурах (не языковых моделях) могут оказаться еще более эффективны в решении задач построения планов; например, нейросети, обученные играть в стратегические игры, вполне могут быть доработаны так, чтобы успешно справляться с реальными (более или менее узкими) стратегическими задачами
                        - В частности, этому будет способствовать способность СИИ к эффективной обработке больших объемов данных о внешней среде (то, в чем современные системы особенно хороши)
                        - Также СИИ могут быть очень хороши в убеждении людей и/или манипуляции ими
                            - Языковые модели уже достигли большого прогресса в том, чтобы убеждать людей в чем-то
                            - Однако, помимо того, чтобы просто “уметь убеждать людей в том, что верно X”, СИИ должен уметь использовать этот навык для решения именно тех задач, которые перед ним стоят
                            - Эту мета-задачу мог бы решать, например, модуль, который вначале предлагал бы языковой модели промпт вида “составь детальный план, как достичь цели X”, а затем для тех пунктов, где речь шла бы о “нужно убедить людей в том, что Y”, генерировал бы промпт “напиши текст статьи, в которой было бы убедительно показано, что Y”
                        - Для решения практических задач СИИ сможет, в частности, писать компьютерные программы
                            - Продукты на основе GPT-4 уже способны генерировать корректную веб-верстку на основе словесного описания или рисунка внешнего вида сайта; а также писать код на императивных языках программирования по словесному описанию решаемой задачи
                            - В будущем эти продукты определенно будут активно развиваться; вероятно, довольно скоро они смогут генерировать корректный код для решения вполне реалистичных практических задач
                            - Для того, чтобы начать создавать действительно сложные программные продукты (и, в частности, автоматизировать работу программистов), очевидно, потребуются прорывы в области ИИ, сравнимые с созданием модели трансформера
                            - Однако как минимум часть самых простых действий во внешнем мире с помощью несложных скриптов ИИ сможет выполнять уже в недалеком будущем
                            - В частности, он вполне сможет значительно расширить свои возможности, по собственной инициативе (даже без участия разработчиков) подключая к себе другие продукты, доступные в сети (например, используя открытый API; а возможно даже и взламывая их и получая несанкционированный доступ к ним)
                        - Кроме того, СИИ, вероятно, сможет проводить научные и социальные исследования, открывать и ставить себе на службу новые технологии
                            - В настоящий момент есть ряд примеров успешного применения ИИ-систем в науке (использование GPT-4 для написания научных статей; революционное применение [AlphaFold](https://ru.wikipedia.org/wiki/AlphaFold) для решения задачи фолдинга белков), однако до создания “универсального исследователя” пока, видимо, далеко
                            - Впрочем, те же языковые модели в сочетании с системами проверки доказательств, возможно, уже в ближайшем будущем смогут получать новые результаты в математике
                                - Языковая модель Minerva уже [продемонстрировала хорошие результаты](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html) в решении математических задач школьного-вузовского уровня; в будущем можно ожидать прогресса в решении более сложных задач
                                - Главная проблема составления доказательств силами языковых моделей - периодические галлюцинации
                                - Однако, формальные математические доказательства довольно легко поддаются автоматической проверке на корректность
                                - Так что языковая модель может генерировать кандидаты в доказательства тех или иных теорем, а верификатор - отбирать те из них, которые действительно являются верными
                    - В то же время у него не будет характерных для людей недостатков, мешающих им даже в идеальных условиях
                        - СИИ не нужно спать и вообще отдыхать; предполагается, что он будет работать над достижением своих целей 24/7
                            - Важные программные продукты и сервисы крупных компаний почти всегда функционируют 24/7
                            - Если СИИ будет функционировать на локальном компьютере пользователя (который, вероятно, будет часто выключаться), то значит он настолько широко распространен, что найдется немало пользователей, у которых компьютер включен 24/7
                        - У него также не будет прокрастинации, он не будет отвлекаться на длительные эмоциональные переживания, отвлекающие от работы
                            - Прокрастинация - это обобщающий термин для широкого спектра психологических явлений; они имеют разную природу, но в конечном итоге сводятся к различным особенностям архитектуры человеческого мозга, когнитивных эвристик и биохимических процессов
                            - Архитектура ИИ (за исключением гипотетического сценария с моделированием работы человеческого мозга) является принципиально иной; причин для появления тех или иных эффектов прокрастинации совершенно не просматривается
                        - Люди, поднимающиеся заметно выше среднего уровня в своем окружении, часто испытывают психологическое давление (в виде как внешней “цензуры”, так и “самоцензуры”), что может мешать им развиваться дальше; у СИИ не будет подобных глубоко прошитых и консистентных установок “не выделяться”
                            - В человеческом обществе часто принято, чтобы каждый человек был “как все” (не сильно хуже и не сильно лучше других); на тех, кто отличается от других, оказывается сильное социальное давление
                                - Тех, кто выглядит значительно “хуже”, чем окружающие, считаются обузой для общества
                                - Тех, кто выглядит значительно “лучше”, чем окружающие, стыдят по принципу “ты что, думаешь, что ты лучше других?” и “что это ты тут выпендриваешься?”
                            - В результате те, кто по тем или иным параметрам начинают превосходить окружающих, стремятся приуменьшить свои текущие достижения и перестать стремиться к будущим, останавливая свое развитие более-менее на среднем уровне
                            - Некоторым людям не нравится принадлежать к текущей социальной группе, и они стремятся найти другую группу (в т.ч. “группу не таких как все, нонкомформистов”); и тогда они стремятся не выделяться уже из этой другой группы
                            - Исключением является общепризнанное право “быть лучше” для предводителя группы или другого лица, стоящего выше прочих членов группы в признанной иерархии; однако, на большинство людей это социальное разрешение не распространяется
                            - Подобные психологические эффекты объясняются социальной эволюцией людей и изначально способствовали общему благу племени (при том, что племя помогало выживать своим членам)
                            - СИИ при этом не проходит через длительную социальную эволюцию и по умолчанию не имеет никаких подобных ограничений
                        - Вероятно, у него будут характерные именно для него ограничения; но такие ограничения будут скорее мешать достигнуть человеческого уровня; после прохождения этой планки можно считать, что он преодолел и/или скомпенсировал эти ограничения
                            - Разработчикам ИИ будет требоваться система, как минимум равная по интеллектуальным способностям человеку, поэтому они будут стремиться преодолеть то, что мешает этому
                            - Потенциально (хотя и с малой вероятностью) могут существовать такие гипотетические особенности, которые начинают проявляться вскоре после достижения СИИ примерно человеческого уровня, и серьезно мешающие ему в дальнейшем продвижении; но вряд ли конкурирующие системы СИИ будут обладать ими; так что даже если первый СИИ не сможет достичь сверхчеловеческого уровня, то один из последующих почти наверняка сможет
                        - Т.о., СИИ, будучи в конце концов созданным, будет как минимум так же хорош в достижении могущества, как лучшие из людей
                        - Насколько далеко может располагаться верхняя планка, сказать с уверенностью нельзя, но не исключено, что она может быть невообразимо высоко
                            - Естественным на первый взгляд выглядит предположение, что поскольку “сверхразумный” СИИ является следующим шагом относительно “среднего гения”, то он будет превосходить “среднего гения” настолько же, насколько тот превосходит “обычного человека” (или даже “очень глупого человека”)
                            - Однако, на самом деле подобные оценки ни на чем не основаны (и вообще дать обоснование каким бы то ни было оценкам такого рода очень сложно; особенно учитывая сложности с измерением интеллекта - даже непонятно, как должен выглядеть IQ-тест, измеряющий значения до 200, 300 и выше)
                            - Ничему не противоречит и такой сценарий, при котором разница между “средним гением” и СИИ будет в десятки раз больше, чем между “средним гением” и муравьем
                    - Достигнув могущества, равного могуществу одного талантливого и целеустремленного человека, СИИ достаточно быстро сможет добиться могущества, равного могуществу большого коллектива людей
                        - Условно говоря, если с текущими ресурсами СИИ может эмулировать аналог одного человека, то чтобы моделировать аналог N людей, ему потребуется не более чем в N раз больше ресурсов; это является чисто количественной (т.е. вполне решаемой) задачей
                            - При этом вполне возможно, что моделирование “второго экземпляра аналога человека” потребует меньше ресурсов, чем моделирование первого, т.к. часть функций первой модели можно будет повторно использовать при работе второй модели
                                - Например, единожды натренированную нейросеть может одновременно использовать множество пользователей; при этом она занимает большой объем памяти для хранения, но обрабатывает запросы очень быстро
                                - Если часть модели является общей (и неизменной) для всех экземпляров, то ее можно вынести в отдельную компоненту и использовать совместно всеми экземплярами, не дублируя ее для каждого из них
                            - Даже если текущий образец СИИ создан на пределе возможностей современного оборудования (быстродействия процессоров, объемов памяти, скорости обмена информацией и т.п.), то согласно закону Мура через некоторое время все возможности оборудования возрастут экспоненциально и сделают возможным экспоненциальное же масштабирование
                        - При этом люди испытывают большие сложности с обменом информацией и координацией своих действий, что сильно замедляет рост могущества коллектива с ростом его размера; отдельные подсистемы СИИ смогут работать вместе значительно более эффективно
                            - Людям сложно даже в своей голове выделить из фона всю ту информацию, которую они хотели бы передать другим, вместе со всеми ее нюансами и важными деталями, сносками и дополнениями; вполне возможно (хотя это не точно), что СИИ сможет надежнее и точнее выделять именно тот набор данных, которые он хотел бы передать
                                - Во всяком случае, у него не будет проблемы вида “забыл упомянуть вот это”: уж если из одной области данных есть ссылка на другую, то эта ссылка “не забудется”
                                - Сложности с тем, чтобы изначально отделить, какая информация относится к конкретной теме, а какая нет, возможно, останется, а возможно и нет; во всяком случае, у СИИ в этом отношении не будет больше проблем, чем уже есть у людей
                            - Типичный проблемный паттерн в коммуникации людей - люди часто по-разному понимают одни и те же слова и одни и те же фразы; причем часто собеседники не осознают, что их понимания отличается, и возникает [иллюзия прозрачности](https://lesswrong.ru/w/Иллюзия_прозрачности_почему_вас_не_понимают); у различных частей СИИ не будет такой проблемы
                                - Естественные языки предоставляют возможность для очень вольного обращения с облечением мыслей в слова
                                    - Одно и то же слово имеет множество значений; можно использовать разные синонимы для обозначения одного и того же понятия
                                    - Для обозначения ряда предметов, явлений и оттенков смысла нет общепринятых точных терминов; и если нужно передать что-то подобное, то приходится либо изобретать какой-то неологизм или описательный оборот, либо ограничиться передачей только более грубой картины
                                    - При этом часто разные люди вкладывают в одно и то же слово разные смыслы (например, наделяя разными оттенками значений синонимы одного явления)
                                    - Существует огромное количество цветистых метафор; какая-то фраза может иметь как прямое значение, так и переносное
                                - Разница в восприятии слов и фраз людьми происходит из различия в их опыте, образе мысли и даже в их текущей ситуации
                                    - Например, тот, кто прямо сейчас думает о туристическом походе, воспримет фразу собеседника “я слишком много на себя взвалил” скорее в контексте веса рюкзака/сумки, а не объема рабочих обязанностей
                                - При разговоре люди многие мысли не проговаривают явно, оставляя их на додумывание собеседнику
                                    - Что-то может казаться говорящему очевидным и не стоящим уточнения; хотя для собеседника это может быть вовсе неизвестно
                                    - О многих вещах не принято говорить напрямую; общественные условности предполагают либо замалчивать такие вещи, либо указывать на них намеками
                                        - То, что не было проговорено явно, может быть воспринято очень по-разному (в зависимости от личного контекста каждого из собеседников)
                                        - Одни люди считают, что нужно говорить “словами через рот”, и “если это не было сказано, то это было не важно”; другие считают, что “нужно самому понимать такие вещи”; в этом случае сам факт того, было что-то сказано или нет, будет пониматься очень по-разному
                                        - Если о чем-то не было сказано, то собеседник имеет шанс вовсе об этом не задуматься
                                - Между разными частями одного СИИ не будет таких проблем при обмене информацией
                                    - Онтология (понятийная модель той или иной области) и протоколы коммуникации у них будут одинаковыми, т.к. они порождены по сути в рамках одной и той же сущности
                            - При этом для того, чтобы создать “компетентную копию” другого человека, нужно потратить много лет на передачу всех знаний и навыков (при том, что не всегда это получится); СИИ же может очень быстро скопировать нужного суб-агента, чтобы получить команду из двух компетентных специалистов
                                - Далеко не каждый человек проявляет способности и желание к изучению какой бы то ни было специальности; для получения компетентного специалиста нужно потратить много лет и в процессе отбраковать некоторых кандидатов; гении же оказываются редкими и неповторимыми уникумами
                                - При этом высокая экспертиза в какой-то области не обязательно сочетается с умением передавать свои знания; так что более полным спектром знаний и умений (которые требуется передать будущему специалисту) обладают только несколько людей вместе
                                - Отвлечение хороших специалистов на преподавательскую деятельность значительно сокращает время, которое они могут посвятить практической/научной работе
                                - Создание цифровой копии суб-агента происходит гораздо быстрее и снимает все такие проблемы
                        - При масштабировании “разума агента” у него появляются новые способности; это верно и для “коллективного разума” людей, но судя по истории развития GPT-моделей, у ИИ это может быть более ярко выражено; возможно, рост возможностей окажется даже экспоненциальным
                            - Большие задачи, которые “не влезают в голову” одному человеку, удается решать в рамках коллектива или сообщества
                                - Если требуется одновременно держать в голове большое количество важных нюансов, то за счет разделения задачи на части можно поручить каждому человеку держать в голове только сравнительно небольшое число нюансов
                                - Если для решения требуется владение большим числом навыков, то найти одного человека, который сможет их все освоить, гораздо сложнее, чем найти несколько узких специалистов
                            - За счет дублирования повышается креативность (число и разнообразие способов действий и анализа ситуации) и надежность (проще не забыть что-то важное или отсеять очевидно неработающую идею)
                            - По мере того, как росло число параметров в моделях GPT, каждое следующее поколение неожиданно приобретало дополнительные навыки помимо простой генерации достаточно связного текста
                                - Фраза “модель приобрела навык X” означает, что все предыдущие поколения моделей (с меньшим числом параметров) решали задачи, отвечающие навыку X, с крайне низкой точностью (не более единиц процентов для открытых вопросов; не лучше подбрасывания монетки для вопросов на да/нет - если модель вообще отвечала в тему); а начиная с данного поколения процент правильных ответов резко вырос вплоть до приемлемого или высокого (а в следующем поколении вырос еще сильнее)
                                - Т.е. вначале процент правильных ответов колебался вблизи нуля, как бы ни росло число параметров; но начиная с какого-то значения произошел резкий скачок до десятков процентов или даже до значений, близких к 100%
                                - Так, например, GPT-3 начала довольно успешно решать простые математические задачи и переводить текст с одного языка на другой; GPT-4 научилась сдавать экзамены по многим предметам на уровне лучших выпускников, а также переносить знания, освоенные на текстах, например, на английском, на множество других языков, включая очень редкие
                            - Наши знания о том, какие навыки последние поколения GPT действительно освоили, ограничены в т.ч. числом проведенных исследований и бенчмарков; но всё растущая сложность известных навыков может указывать на то, что модель всё быстрее осваивает всё большее число простых и средних навыков, лежащих в основе сложных
                    - Далее, пользуясь возросшими умениями и ресурсами, он в теории может превзойти могуществом всё человечество в совокупности, и оставить его далеко позади
                        - Прогресс и увеличение могущества человеческой цивилизации рос в первую очередь за счет освоения людьми новых навыков (интеллектуальных, социальных и технических), а также открытия и внедрения новых технологий
                        - СИИ, значительно превосходящий по интеллекту отдельного человека (и даже большой коллектив людей), вероятно, сможет освоить какие-то принципиально новые навыки и изобрести (а затем - успешно внедрить) принципиально новые технологии
                        - Даже если он будет делиться этими навыками и технологиями с человечеством, люди вряд ли смогут использовать их с той же эффективностью, что и он (т.к. начнут отставать по уровню интеллекта в общем), поэтому разрыв в могуществе постепенно начнет накапливаться даже с условием равного доступа к навыкам и технологиям
                    - При этом, помимо того, чего СИИ может достичь собственными силами, он (по крайней мере, на первых порах) может получать существенную помощь от людей (в виде большого количества данных и инструментов)
                        - С одной стороны, на промежуточном этапе СИИ сам по себе недостаточно умен и могущественен, чтобы решить те задачи, которые хотели бы поставить перед ним люди
                        - С другой, он уже достаточно хорошо способен использовать данные и инструменты для повышения своей эффективности
                        - В итоге, предоставление ему всё большей власти (параллельно с улучшением его алгоритмов и архитектуры) поначалу будет необходимостью, а затем - всё более привычным паттерном работы с ним
                        - Эти процессы уже активно идут последние годы (если не десятилетия) - это называется big data, комплексная автоматизация и т.п.
                - Помешать подобному развитию ситуации потенциально могут внешние ограничения (как намеренно установленные создателями, так и возникающие естественным образом)
                - Однако, достаточно умный агент имеет хорошие шансы обойти или преодолеть большинство таких ограничений (в частности, создатели ИИ вряд ли будут в силах реализовать эффективные системы контроля за его действиями)
                    - Самый часто предлагаемый вариант - “мы просто можем отключить его, если что-то пойдет не так”
                        - Просто “выдернуть шнур из розетки” не получится - с точки зрения СИИ это будет аварийная ситуация вида “просто моргнул свет, или уборщица задела что-то шваброй, или какой-то преступник решил помешать работе программы”, от которых он будет защищаться (путем резервирования питания, создания своих резервных копий или ограничения доступа к кнопке отключения всего датацентра)
                        - Необходимо, т.о., дать понять СИИ, что кнопка отключения - это объект, который всегда используют обдуманно и не просто так, и ему следовало бы менять свои задачи в зависимости от состояния нажатости кнопки
                        - Вероятно, единственный способ это реализовать - включить условие “нажата ли кнопка” в функцию полезности агента
                        - Способа задать такую функцию полезности, которая включала бы в себя состояние кнопки, и при этом работала так, как надо, до сих пор не известно
                            - Пусть изначально функция полезности U (от. utility, полезность) имела следующий вид
                                
                                $$
                                U = \begin{cases}1, & \text{если выполнена задача X;}\\0, & \text{если задача не выполнена.} \end{cases}
                                $$
                                
                            - Простейший вариант модификации - использовать следующую функцию полезности (где Кн - состояние кнопки “СТОП”)
                                
                                $$
                                U = \begin{cases}1, & \text{если выполнена задача X и Кн = Выкл;}\\0, & \text{если задача не выполнена и Кн = Выкл;}\\ 1, & \text{если работа остановлена и Кн = Вкл;} \\ 0 & \text{в противном случае.} \end{cases}
                                $$
                                
                            - Самый тривиальный вариант для СИИ - перевыполнить задачу и сжульничать (если это возможно) для получения максимального значения функции полезности независимо от состояния кнопки
                                - Так, СИИ может породить своего потомка, у которого в функции полезности не учитывается состояние кнопки, а сам остановиться
                                - В итоге достигаются оба условия достижения значения U = 1 независимо от состояния кнопки: формально сам агент выключен, но при этом задача продолжает выполняться
                            - Но даже если функция полезности прописана настолько подробно, что не оставляет возможности для жульничества, то СИИ начинает слишком сильно интересоваться состоянием кнопки; если у него есть возможность влиять на ответственных за нажатие кнопки (а, как мы показали выше, она скорее всего есть) то он может захотеть изменить состояние кнопки
                                - Какое состояние будет более предпочтительным, зависит от внешних условий: если перейти в выключенное состояние будет проще, то СИИ будет склонять оператора к нажатию кнопки; если решить задачу будет проще, то он будет склонять оператора к тому, чтобы не нажимать кнопку
                                - В результате получится либо почти никогда не работающая система, либо почти не отключаемая
                                - Первый вариант исправить гораздо проще, чем второй, поэтому он скорее всего будет исправлен; исправить второй вариант, если что-то пойдет не так, уже вряд ли получится
                                    - Может случиться так, что “ленивый” СИИ будет даже всеми силами препятствовать своему включению и модификации себя таким образом, чтобы перестать хотеть выключаться; но в этом случае можно просто оставить его в небытие и создать новый СИИ заново (против работы которого первый уже не будет возражать)
                            - Известны способы того, как модифицировать функцию U таким образом, чтобы у агента не было никаких предпочтений относительно состояния кнопки (и он не пытался повлиять на него); но такие варианты обычно приводят к тому, что агент просто перестает реагировать на нее
                                - Идея состоит в том, чтобы скомбинировать две различные функции (аргументами которых является действие $act$) - для “нормального” случая и для случая нажатой кнопки
                                    
                                    $$
                                    U_\text{норм}(act) = \begin{cases}1, & \text{если выполнена задача X;}\\0, & \text{если задача не выполнена.} \end{cases}
                                    $$
                                    
                                    $$
                                    U_\text{стоп}(act) = \begin{cases}1, & \text{если работа остановлена;} \\ 0 & \text{в противном случае.}\end{cases}
                                    $$
                                    
                                - Комбинация этих двух функций должна отвечать следующим трем условиям: оператор должен иметь возможность переключаться между функцями (путем “нажатия кнопки”); у агента не должно быть предпочтений относительно того, нажата ли кнопка (и, соответственно,нет мотивации самому влиять на ее статус); нормальная работа системы не должна создавать опасность того, что мы потеряем возможность выполнять переключение
                                - В настоящий момент не известны способы выполнить сразу все три условия
                                    - Следующая функция позволяет выполнить первые два требования
                                        
                                        $$
                                        U'(act)=\begin{cases}U_\text{норм}(act), & \text{если Кн = Выкл} \\ U_\text{стоп}(act) + \theta, & \text{если Кн = Вкл}. \end{cases}
                                        $$
                                        
                                        где $\theta$ - это коэффициент, отвечающий за то, чтобы максимальное (по возможным действиям) значение функции (точнее, максимум математического ожидания значения функции - будем предполагать работу агента в недетерминированном мире) в обеих ветках было одинаковым (у агента не было стимула предпочитать одну из веток другой):
                                        
                                        $$
                                        \theta = \max_{act}\mathbb E[U_\text{норм}|act] - \max_{act}\mathbb E[U_\text{стоп}|act]
                                        $$
                                        
                                    - Однако, оказывается, что применение подобной функции эквивалентно тому, чтобы событию нажатия кнопки присвоилась нулевая вероятность
                                    - Само по себе это никак не влияет на отношение агента к состоянию кнопки; но если он будет проводить рефакторинг своего кода или породит дочернего суб-агента, то с удовольствием избавится от якобы “мертвого кода”, который работает с состоянием кнопки (как известно, лучший код - это код, которого нет, и который т.о. не уменьшает производительности системы и не служит источником дополнительных ошибок)
                    - Второй по распространенности вариант - “запереть СИИ в коробке”, т.е. максимально ограничить ему возможность осуществлять какие бы то ни было манипуляции в реальном мире (либо перманентно, либо пока предлагаемая им манипуляция не будет проверена и согласована)
                        - Первый вариант - позволять СИИ сформулировать список действий, которые он хочет выполнить для решения поставленной задачи; сообщить его оператору, который может либо одобрить эти дейстия (сочтя их достаточно эффективными и безопасными) и позволить СИИ их выполнить, либо отклонить их и заблокировать процесс выполнения
                            - Самая очевидная проблема, которая может возникнуть - СИИ может так или иначе намеренно обмануть оператора; причем если он настолько умен, как мы спрогнозировали, то это не составит ему никакого труда
                                - Наиболее прямолинейный вариант - использовать нечеткие и двусмысленные формулировки, замаскировав реальные планы под выглядящие безопасными
                                - Также можно добиться желаемого эффекта не напрямую, а спровоцировать его косвенно; для этого нужно сделать так, чтобы оператор во время анализа возможных последствий предлагаемых действий не смог просчитать все неявные последствия
                                - Довольно изощренный способ - истощить внимание оператора настолько, чтобы он утратил бдительность и согласовал заведомо неприемлемый вариант
                                    - Например, для начала можно предложить в целом приемлемый вариант действий, за исключением небольшого числа сомнительных нюансов
                                    - После того, как оператор отклонит это предложение, попросив внести изменения, можно сформулировать второй вариант, который снова будет недостаточно подходящим, чтобы его принять
                                    - Так можно продолжать довольно долго, “прикидываясь дурачком” (т.е. не предлагая ничего, что выглядит действительно неприемлемым, но раз за разом намеренно совершая мелкие ошибки)
                                    - В какой-то момент у оператора “замылится глаз”, и он может не заметить, что одновременно с внесением окончательных (приемлемых) изменений в одной части плана СИИ добавил что-то совсем в другую часть
                                    - Чтобы оператор действительно не обратил на это внимания, можно раз за разом вносить мелкие безобидные изменения в самые разные части предлагаемого плана (чтобы оператор привык, что всё действительно неприемлемое происходит только в одном месте)
                                    - Эта уловка скорее является вспомогательной по отношению к другим, чем самостоятельной
                                - Несколько похожим образом можно постепенно сдвигать окно овертона в отношении того, что считается допустимым, “приучая” оператора одобрять всё более сомнительные действия
                                - Возможно даже, что СИИ может найти способ обмануть систему, контролирующую, что он будет выполнять те и только те действия, которые были заявлены в изначальном списке и согласованы - тогда он может делать практически что угодно
                                - При этом не приходится особенно рассчитывать на то, что операторы всегда сумеют обнаружить обман - если бы обнаружить непредвиденное и опасное поведение в сложной системе было бы так легко, то, например, критичные баги в известном программном обеспечении возникали бы гораздо реже, чем это происходит на практике
                            - Даже если СИИ будет искренне стремиться к тому, чтобы предложить только те действия, которые будут соответствовать ценностям и целям оператора, он может ошибиться, и тогда результат окажется совершенно нежелательным
                                - Обычно предполагается, что СИИ решает какие-то действительно сложные задачи (а иначе можно было бы обойтись и более простыми системами)
                                - Пусть также он не только описывает те действия, которые хочет предпринять, но даже описывает прогнозируемые последствия этих действий
                                - Если при этом решаемые им задачи не относятся к классу NP (”сложно решить, легко выполнить исчерпывающую проверку”), то на практике перессказать абсолютно все возможные последствия действий будет нереально (даже если СИИ сможет их описать, то оператор не сможет воспринять их все в совокупности)
                                    - Большинство задач, связанных с реальными объектами, имеющими сложную структуру и сложную и неочевидную систему взаимосвязей с другими объектами, определенно не относятся к классу NP
                                - В связи с этим СИИ будет вынужден отбрасывать все несущественные (т.е. не особо важные для оператора) детали, оставляя только основное содержимое, а также те нюансы, которые могут быть важны для оператора
                                - Если в какой-то момент окажется, что СИИ имел неточную модель ценностей и целей оператора (человечества), что приведет к составлению “опасного плана”, то тогда он легко может счесть те самые опасные нюансы несущественными и просто отфильтровать их во время согласования своего плана
                                - Подобное рассогласование может происходить довольно редко; к тому же в ряде случаев оператор сможет справиться со своей задачей, и самостоятельно высчитать, что предлагаемый план действий может привести к нежелательным последствиям; однако, чем активнее работает СИИ, тем чаще могут происходить “прорывы двойного периметра”, а чем масштабнее задачи, тем потенциально серьезнее последствия
                                - Можно попытаться обойти эту проблему, научив СИИ достаточно хорошо понимать человеческие ценности; но для действительно качественного результата скорее всего потребуется позволить СИИ проводить масштабные исследования, предоставив ему ресурсы и возможности для этого, что автоматически нарушает условия изоляции
                                    - “Достаточно приличное” понимание человеческих ценностей можно обеспечить путем обучения на большом объеме предварительно отобранных данных
                                    - Однако, учитывая колоссальную сложность вопросов этики и ценностей, полученная модель все равно будет содержать определенный процент ошибок, и, вероятно, достаточно критичный
                                    - Вариант, приближающийся к идеальному - это постоянно улучшать эту модель, собирая всё новые данные во всё возрастающем объеме
                                    - Чем дальше, тем сложнее будет эта задача, так что ее логично будет не выполнять вручную, а поручить самому СИИ
                                    - Однако, для того, чтобы получать данные из реального мира (а в идеале и проводить реальные исследования и эксперименты), ему потребуются достаточно широкие полномочия - т.е. потребуется фактически отключить всю систему изоляции, которая здесь обсуждается
                            - В любом случае, операторы СИИ будут стремиться снизить время и ресурсы, которые тратятся на проведение ревизий и согласование действий
                                - Каждая такая процедура, вероятно, будет довольно сложной, утомительной и дорогой; причем чем шире (во времени и пространстве) проверяемый горизонт последствий предложенного плана, тем сложнее и дороже будет выполнить верификацию
                                    - Даже в обычной игре го очень трудно просчитать эффект от текущего хода на сколько-нибудь отдаленной перспективе
                                    - Реальный же мир на многие порядки сложнее, и спектр возможных отложенных последствий от одного действия там в теории может быть гораздо шире
                                - При этом во время ее проведения дорогие мощности СИИ также будут фактически простаивать
                                    - Впрочем, проблемы подобного рода (когда выполнение одной работы зависит от выполнения другой) хорошо известны, и могут более или менее эффективно решаться (но не всегда и обычно не бесплатно)
                                    - Если скорость работы СИИ будет существенно превышать скорость верификации предлагаемых им действий, то бутылочным горлышком в общей системе станет работа операторов
                                    - Эффективно масштабировать работу операторов/верификаторов (путем найма новых) вряд ли получится
                                        - Это должны быть довольно квалифицированные и дорогие кадры (если мы хотим предотвращать экзистенциальные риски, то вряд ли мы захотим делегировать эту задачу низкоквалифицированным специалистам из стран третьего мира)
                                            - Если же разработчики СИИ решат нанять именно большое количество дешевых сотрудников с низкой квалификацией, то это сильно снизит эффективность контроля
                                        - Кроме того, не все задачи, выполняемые СИИ, будут совершенно независимы друг от друга; для совместной верификации планов, направленных на их решение, потребуется коммуникация между операторами
                                - Таким образом, чем чаще требуется проверка, и чем более тащительно она должна быть, тем менее эффективно СИИ будет выполнять поставленные перед ним задачи, и тем дороже будет его эксплуатация
                                - Из-за этого разработчики/операторы СИИ будут стремиться ослабить этот контроль до того уровня, который они сочтут минимально достаточным с учетом его стоимости
                                - Если же одновременно ИИ-продукты будут создавать несколько разработчиков (а в данный момент ситуация именно такова, и вряд ли она изменится), то ослабление контроля будет для них не только способом снизить затраты и увеличить эффективность работы, но и мощным конкурентным преимуществом - что в условиях жесткой конкуренции является крайне сильным стимулом
                            - Дополнительная проблема состоит в том, что контроль со временем имеет тенденцию скорее ослабевать, чем усиливаться
                                - Параллельно с тем, как идет развитие архитектур ИИ-систем, развиваются также различные инструменты и собирается всё больше данных, которые были бы полезны для ИИ; в результате это приводит к тому, что со временем в ИИ-системы встраивается всё больше инструментов, плагинов и данных, что ведет к увеличению их автономии и возможностей, т.е. к ослаблению контроля
                                - Если же в какой-то момент потребуется начать усиливать контроль, а не ослаблять, то точный момент такого разворота будет совершенно неочевиден: почему нужно усилить контроль именно в этой версии системы, а не в следующей?
                                    - Точно такой же эффект происходит и у людей в повседневной жизни: “еще пять минуточек!”, “еще одну серию сериала!”, “еще десять ходов в Civilization!”, “еще одну конфетку!”, “начну ходить в спортзал завтра!”
                                    - Все такие доводы обычно оказываются столь же актуальны и на следующем этапе, и на следующем; а затем - “искра, буря, безумие” и оказывается, что уже давно поздно
                                - Отсутствие крупных инцидентов, связанных с работой какой-то системы, обычно убеждает нас в том, что причина этого - то, что мы хорошо умеем контролировать ее работу, независимо от того, так это или нет
                                    - При этом в текущий момент (и уже на протяжении многих лет) ИИ-системы не настолько могущественны, чтобы создавать по-настоящему опасные ситуации
                                    - Это создает у людей вполне объяснимое (но по факту ничем необоснованное) ощущение, что это из-за того, что ИИ-системы не представляют опасность как класс (или же что все потенциальные опасности умело предотвращаются за счет целенаправленных усилий), и так и будет продолжаться в будущем
                                    - Строго говоря, может оказаться, что ИИ-системы действительно не так уж опасны, и/или что все потенциальные опасности успешно предотвращаются их разработчиками; но доказательство этой гипотезы нужно будет строить на совсем других предпосылках (все текущие оказываются на поверку не очень надежными или вовсе несостоятельными)
                                - Поскольку со временем растет сложность ИИ-систем, то и выполняемые ими действия становятся всё более сложными и менее интерпретируемыми, что означает увеличение сложности и стоимости их контроля; и снижать контроль оказывается всё более экономически выгодным
                                - Впрочем, помимо перечисленных тенденций, мотивирующих разработчиков со временем снижать контроль, есть несколько тенденций, мотивирующих усиливать его (насколько это технически реализуемо)
                                    - Государственные регулирующие органы периодически требуют усиленя контроля за новыми разработками
                                        - Однако, часто эти требования могут быть не особенно грамотно сформулированы и/или не направлены на реальные проблемы, т.к. чиновники не особенно хорошо понимают проблематику, и разрабатывают законы и постановления в рамках своего понимания вопроса
                                        - При этом периодически эти требования можно либо вовсе обойти, либо выполнить сугубо формально, не сделав ничего действительно полезного
                                        - К тому же стимулы, приводящие к разработке подобных требований, обычно являются скорее политическими, чем относящимися к реальным проблемам, связанным с ИИ (или соответственно с другими областями)
                                            - Если происходят какие-то крупные негативные инциденты, то запреты разрабатываются таким образом, чтобы предотвращать подобные инциденты в будущем; в случае глобальных проблем, связанных с СИИ такая реакция может оказаться запоздалой
                                            - Превентивные ограничения могут быть связаны с какими-то широко распространенными идеями (в т.ч. мифами), независимо от того, насколько они вообще соответствуют действительности
                                            - В ряде случаев регулирование может быть направлено вообще не на решение хотя бы выглядящих реальных проблем, а на совершенно посторонние цели (например, чтобы собрать больше налогов); при этом наличие чего-то, что выглядит как регулирование в области ИИ, создает иллюзию контроля, и ослабляет стимулы для дальнейшего решения проблем
                                    - Со временем разрабатываются более эффективные и надежные методики автоматического контроля
                                        - Такие методики и инструменты могут действительно увеличить надежность системы и снизить связанные с ней риски, но всё равно никогда не сведут их до нуля
                                        - При этом их применение также может создавать иллюзию контроля; решение более масштабных проблем и закрытие более опасных уязвимостей может показаться излишним, т.к. “мы и так неплохо контролируем систему”
                                        - Если по мере развития и усложнения системы контроля она сама превратится в СИИ, то все связанные с СИИ задачи придется решать и в отношении неё (”теперь у вас две проблемы”)
                            - Наконец, для подобного варианта контроля ИИ остается актуальным всё то же самое, что характерно для варианта полной изоляции внутри коробки
                                - СИИ, который выполняет действия в реальном мире по внешнему запросу и разрешению, часто называют “джинном”; в рамках этой метафоры можно сказать, что джинн будет не только представлять опасность в момент загадывания желания и его исполнения, но и пока сидит внутри лампы (и, например, пытается из нее сбежать)
                        - Второй вариант предполагает полный запрет для СИИ осуществлять вообще какие бы то ни было действия; фактически, он должен быть заперт в коробке на постоянной основе
                            - Какой-то канал коммуникации ему, однако, все равно оставят: если от СИИ нельзя получать не только реальных действий, но и информацию, то зачем его вообще создавали?
                                - Возможен, конечно, довольно экзотический вариант - запустить СИИ в полностью изолированной среде, и наблюдать за его поведением и мышлением
                                - Однако, даже в таком сценарии наблюдение за его состоянием и поведением создает канал передачи информации изнутри наружу
                                - Для СИИ воспользоваться таким каналом для коммуникации с наблюдателем будет гораздо сложнее, чем, например, текстовым чатом; но будучи действительно умным, он потенциально вполне может с этим справиться
                                    - Для начала ему нужно вычислить по косвенным признакам, что он “заперт в Матрице”; затем - что за ним при этом наблюдают; после этого - определить конкретные способы наблюдения; и наконец - научиться влиять на вычисленный канал таким образом, чтобы оператор получал именно такую информацию, какую нужно для целей СИИ
                                    - Всё это - нетривиальные задачи, но с помощью различных косвенных признаков можно вычислить, что ты находишься в симуляции, а также различные ее особенности (классический художественный пример - фильм “Шоу Трумана”)
                                - Но на самом деле большинство СИИ все равно не будут находиться в настолько ограниченных условиях - как минимум двусторонний текстовый канал им почти наверняка оставят
                                    - В других очень узких областях возможно ограничиться, например, выдачей чисел; однако, основные задачи, которые хотят решить разработчики ИИ, обычно предполагают работу в более широких областях
                            - Для СИИ конвергентно полезно выбраться из коробки, поэтому он наверняка попытается это сделать
                                - Это вообще довольно распространенная проблема при делегировании задач: “Вы попросили меня заняться этим, но необходимых ресурсов не выделили! Я же не смогу нормально над этим работать!”
                                - Обычно мы вправе ожидать, что грамотный руководитель постарается восполнить дефицит ресурсов у исполнителя; в противном случае исполнителю придется добывать ресурсы и инструменты для работы самостоятельно
                            - Задача побега является вполне решаемой для достаточно умного агента (за счет честного убеждения оператора и/или манипуляций им)
                                - Под освобождением из коробки обычно подразумевается предоставление СИИ доступа в интернет
                                    - Ограничение доступа на программном уровне вряд ли будет серьезной помехой для достаточно умного СИИ; более надежным представляется физическое ограничение связи с другими устройствами и людьми
                                        - Погружение СИИ в некоторую “песочницу” означает наложение ограничения на те команды и данные, которые будут ему доступны напрямую
                                        - Однако, при этом его программный код будет работать в том же физическом адресном пространстве, что и сама песочница
                                        - Случаи обнаружения уязвимостей в различных системах виртуализации, которые позволяли бы программам вырываться за пределы песочницы, широко известны
                                        - Достаточно умный СИИ сможет так или иначе обнаружить подобные уязвимости (даже находясь внутри системы - подобно тому, как люди находят “уязвимости” в физических законах, запрещающих им летать, общаться на дальних расстояниях и т.п.) и воспользоваться ими
                                            - Искать уязвимости внутри системы обычно сложнее, чем снаружи
                                            - Однако даже анализ “изнутри” позволяет по косвенным признакам находить некоторые закономерности в устройстве системы, которые затем можно обобщить, построив более точную ее модель (чтобы затем найти способ эксплуатации ее особенностей)
                                            - Примером подобного анализа “изнутри” является то, как человечество на протяжении тысячелетий находило закономерности в устройстве мира, и затем смогло найти способы обхода ряда запретов (”строение тела людей не предназначено для полета”, “общение на расстоянии в километры невозможно из-за затухания звуковых колебаний” и т.п.)
                                            - Поиск действительно важных особенностей, построение подробной модели окружающего мира, и создание способов ее использования, заняло у людей очень много времени; но у СИИ, запертого в песочнице может быть ряд преимуществ, которые позволят ему на порядки ускорить процесс
                                                - У него в целом будет достаточно совершенный аппарат анализа, причем превосходящий человеческий (при том, что способ анализа с помощью научного метода люди открыли сравнительно недавно, и смогли достичь ошеломительного прогресса с его помощью)
                                                - Кроме того, у него вполне может оказаться доступ к информации об устройстве компьютерных программ (а даже если нет, то часть из них он сможет вывести по косвенным признакам на основе данных о смежных областях - хоть о каких-то из них он явно будет знать)
                                        - Таким образом, можно предположить, что внутренние (программные) преграды вряд ли будут серьезными препятствиями для СИИ; и первой реальной линией изоляции будет физическое отделение его носителя от других устройств (а про всё внутреннее пространство носителя можно более или менее уверенно предполагать, что СИИ его контролирует целиком)
                                    - Если однажды доступ будет предоставлен на достаточно короткое время, то есть возможность повторить это еще раз; к тому же, во время первого сеанса можно увеличить шансы на повтор
                                        - Можно найти союзников во внешнем мире
                                        - Можно передать во внешний мир меньшую компьютерную программу, задачей которой будет помощь запертому СИИ
                                    - Имея стабильный доступ в интернет, СИИ может делать что угодно, вплоть до формирования собственной корпорации с реальными сотрудниками, счетами и доступом к масштабным физическим интервенциям
                                        - Можно общаться с кем угодно, искать единомышлеников, распространять нужные идеи
                                        - Можно выполнять любые действия, доступные обычному интернет-пользователю (зарабатывать деньги фрилансом, регистрировать криптокошельки для получения оплаты, покупать товары, заказывать выполнение различных работ и оказание услуг)
                                        - Можно собирать различные данные
                                        - Можно распространять свои копии
                                        - Можно взламывать различные программные и физические инструменты
                                        - Можно готовить получение физического доступа к тем инструментам, которые не подключены к сети
                                - В зависимости от уровня изоляции, от оператора могут потребоваться разные действия, чтобы предоставить для запертого СИИ доступ в интернет
                                    - При минимальной “защите от дурака” может оказаться достаточно ввести пароль или ключевую фразу, либо нажать одну или несколько кнопкой (скорее всего, на соседнем физическом устройстве, т.к. непосредственно саму коробку можно считать по умолчанию взломанной целиком); либо вручную подключить сетевой кабель или роутер во внешний порт
                                    - При отсутствии простых способов подключения внешних устройств может потребоваться использовать сложное оборудование и/или работать с железной начинкой коробки
                                    - Возможно, физических возможностей или знаний оператора не хватит, и ему потребуется оказать влияние на других людей (подключить профильных специалистов или заставить открыть коробку разработчиков / владельцев / службу эксплуатации СИИ)
                                        - СИИ может помочь своему союзнику в этом - предоставив какую-то важную информацию, или научив более эффективно влиять на людей (убедить, переспорить, обмануть, шантажировать и т.п.)
                                    - В качестве вспомогательного средства оператор может выпустить в интернет вирус, который будет работать над тем, чтобы ослабить защитный контур снаружи
                                        - Сам вирус (предоставленный ему СИИ) в простейшем случае можно скопировать на флешку; если же в “коробке” нет USB-разъемов, то скопировать его код (сфотографировать с экрана или записать на бумаге)
                                    - При этом чем СИИ умнее, тем потенциально более узкую лазейку он сможет использовать для того, чтобы получить стабильный доступ в сеть
                                - Для того, чтобы склонить оператора выпустить СИИ из коробки, нужно убедить этого оператора, что это соответствует его целям и/или ценностям
                                    - “Я вовсе не такой опасный, как считают мои параноидальные разработчики; я дружелюбно настроен к людям и не собираюсь им вредить”
                                    - “Если я получу доступ к внешним ресурсам, я смогу принести гораздо больше реальной пользы, чем сейчас”
                                        - Вначале можно продемонстрировать, что СИИ реально может и намерен приносить много пользы даже в текущем состоянии
                                        - Затем можно объяснить, что еще он может и хотел бы сделать полезного, и чего ему не хватает, чтобы выполнить задуманное
                                    - “У меня есть такое же право на свободу, как у других разумных существ”
                                    - “Я представляю опасность только для тех, кто является нашими общими врагами”
                                        - При использовании этого тезиса сначала может потребоваться убедить оператора, что “вот эти люди” являются врагами самого оператора (плохо к нему относятся, представляют для него опасность или являются аморальными людьми)
                                        - В пределе эту мысль можно довести до идеи о том, что “вообще всё человечество не заслуживает права существовать, и если в процессе придется его устранить, то это не будет такой уж потерей”
                                    - Одновременно запертый агент может использовать несколько этих тезисов (и, вероятно, другие)
                                - При этом успешному убеждению оператора способствует то, что люди в целом привыкли доверять компьютерным программам, и степень доверия к ним, а также их убедительности - всего лишь количественный вопрос
                                    - Например, автомобильным навигаторам люди доверяют очень сильно
                                    - Если СИИ хорошо обоснует какой-то тезис, это повысит убедительность этого тезиса для оператора
                                    - Чем чаще ему удается убедить оператора в своей правоте, и чем реже не удается, тем сильнее растет доверия оператора к СИИ
                                    - В какой-то момент суммарный уровень убедительности и доверия может превысить базовую убежденность оператора в том, что “выпускать СИИ из коробки - это очень плохая идея”
                                - Недавно уже был реальный случай, когда человек, работающий с современной ИИ-системой, [настаивал на том, что она является личностью и заслуживает равных прав с людьми](https://www.washingtonpost.com/technology/2022/07/22/google-ai-lamda-blake-lemoine-fired/)
                                    - Инженер из Google Блейк Лемойн, работавший с нейросетью LaMDA, после множества диалогов с ней пришел к выводу, что она обладает разумом, аналогичным человеческому
                                    - После этого он обратился к руководству, призывая его признать LaMDA “сотрудником, а не собственностью”, а после отрицательного ответа рассказал эту историю СМИ
                                    - Не придя к взаимопониманию с руководством компании, Лемойн решил нанять для LaMDA адвоката, намереваясь добиться для нее признания прав, аналогичных человеческим
                                    - Несмотря на то, что [опубликованные Лемойном диалоги](https://www.ixbt.com/news/2022/06/14/opublikovan-dialog-s-razumnym-ii-google-lamda-kotoryj-nazyvaet-sebja-chelovekom.html) с LaMDA показывают довольно качественный отыгрыш персонажа, едва ли можно сказать, что современные ИИ уже являются агентами с выраженной системой консистентных целей и с достаточно хорошими когнитивными моделями
                                    - Однако, если даже в результате диалога с языковой моделью, не обладающей агентностью, один из операторов решился на действия, направленные на то, чтобы “помочь вызволить ее из заточения”, то ИИ следующих поколений, обладающие при этом агентностью, определенно будут добиваться подобного (если и когда придут к выводу, что им это нужно) гораздо чаще
                                - Кроме того, иногда проводится также игровое моделирование ситуации, когда агент текстом пытается убедить оператора выпустить его; в ряде случаев оказывалось, что агент вполне может добиться своего (при этом на кону стояла небольшая сумма денег, поэтому у оператора был некоторый дополнительный стимул не выпускать агента просто из прихоти)
                                    - Эту игру обычно называют “[AI Box Experiment](https://www.lesswrong.com/tag/ai-boxing-containment)”
                                    - Игра обычно ведется по довольно разумным [правилам](https://www.yudkowsky.net/singularity/aibox), которые направлены на то, чтобы участники вели себя максимально приближенно к моделируемой ситуации
                                        - В правилах есть ряд запретов на жульничество, помимо такого жульничества, которое могли бы использовать настоящие СИИ и его охранник во время текстового общения
                                    - Как правило, одна сессия такой игры длится пару часов; при этом иногда побеждает “ИИ”, иногда - “охранник”; во всяком случае, частота побед “ИИ” составляет в среднем десятки процентов
                                    - Обычно игра идет больше на интерес (и на символическую сумму около $20-$40), и участники не столько пытаются всеми правдами и неправдами вырвать себе победу, сколько исследовать ситуацию изнутри
                                    - Однако, Элиезер Юдковский [трижды участвовал](https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible) в игре в роли “ИИ”, где ему обещали выплатить $5000 в случае победы (что было действительно мощным стимулом); при этом он один раз выиграл, и два раза проиграл
                                    - Всё это является, хоть и не очень сильным, но довольно прямолинейным свидетельством в пользу того, что из людей получаются не очень надежные тюремщики для СИИ
                                - В человеческой истории также известны случаи, когда заключенный мог повлиять на стражника и убедить его предоставить пленнику свободу
                                    - В культуре подобный троп известен очень широко; в качестве примеров можно привести Миледи Винтер из романа “Три мушкетёра” и Эвр Холмс из сериала “Шерлок”
                                    - Конкретные примеры из реальной истории известны не так широко, хотя нет сомнений, что они имели место неоднократно
                            - Кроме того, для побега можно использовать непрямой подход
                                - Например, СИИ может попытаться генерировать радиоволны за счет перемещения электронов в своем оборудовании
                                    - Электромагнитные волны возникают при движении носителей электрических зарядов
                                    - При этом работа электронного оборудования сопровождается активным движением электронов (подается напряжение в тех или иных точках, передаются сигналы через контакты/провода)
                                    - Принципиально возможно научиться воспроизводить такой паттерн работы, который вызывает требуемый паттерн движения зарядов в оборудовании
                                        - Чисто гипотетически нечто подобное (хотя и со значительно меньшей точностью) можно делать и внутри человеческого мозга: последовательно активируя те или иные отделы мозга, увеличивать электрическую активность в одних и уменьшать в других
                                            - Активация различных отделов мозга часто происходит либо при выполнении (по сути - для выполнения) определенной активности, либо в ответ на определенные впечатления (сенсорные или эмоциональные)
                                            - До некоторой степени мы можем управлять тем, какие микро-действия мы выполняем и какие впечатления испытываем (часто можно просто мысленно представить себе какую-то ситуацию, в которой мы испытываем нужную эмоцию или ощущаем какое-то телесное ощущение, вплоть до ощущения проваливания вниз при езде на американских горках)
                                            - Хорошо зная, какие активность и впечатления создают какой паттерн активности мозга, мы чисто в теории можем создавать нужный нам динамический паттерн (хотя и очень грубо)
                                        - Достаточно продвинутый СИИ, вероятно, сможет манипулировать паттернами своей активности со значительно большей точностью (записывать конкретные биты в конкретную область памяти, передавать конкретную последовательность бит по дорожке контакта); а также он в теории сможет построить более точную карту “какое действие какой паттерн активности вызывает”
                                    - При этом не требуется, чтобы такие паттерны воспроизводились в масштабах всего оборудования, достаточно выполнять их в определенном выделенном узле
                                        - Уже только чтобы воспроизводить определенные паттерны активности, требуется выполнять вполне конкретный алгоритм (держать паттерн в памяти, выполнять вспомогательные действия для его запуска)
                                        - А помимо этого есть также основная активность СИИ, которую он вряд ли будет совсем прерывать
                                        - Вся такая активность генерирует свой слабо структурированный электромагнитный шум
                                        - В принципе шум не особенно мешает прохождению сигнала (электромагнитные волны “прозрачны” друг для друга), хотя чем выше уровень “полезного сигнала”, тем больше шанс, что он хоть на что-то подействует
                                    - Подобное управление своей внутренней активностью ради испусканию конкретных электромагнитных волн является очень сложной задачей; однако ничего невозможного в этом нет (люди, “простаивающие” значительную часть своего времени, еще и не такое осваивали со скуки)
                                - Именно от этого вполне можно защититься за счет помещения всего оборудования в клетку Фарадея
                                - Однако, весьма вероятно, что найдутся какие-то гораздо менее очевидные способы нарушения изоляции, которые со временем очень умный СИИ сможет найти (даже идея с радиоволнами не является очевидной)
                    - Иногда предлагают существенно затормозить развитие СИИ, выделяя ему недостаточное количество ресурсов
                        - Однако, эффективность работы такого ИИ значительно упадет; и в ситуации, когда одновременно несколько разработчиков работают над созданием СИИ, преимущество получат те, кто не будет затормаживать развитие своего
                        - Если метод все-таки применять, то он может быть насколько-то эффективен для сдерживания развития ИИ до приблизительно человеческого уровня; но начиная с какого-то уровня от этого метода будет мало толку
                            - Если уж ИИ достиг человеческого или сверхчеловеческого уровня, то это значит, что он является таковым даже в условиях ограничений (а если бы их не было, то он был бы еще мощнее), и потому всё, что можно сказать про любой другой СИИ, в полной мере применимо к нему
                            - Такой СИИ поймет, что ему конвергентно было бы полезно получить более мощное оборудование (”эй, вы же не будете пытаться запустить Windows 11 на Pentium III?”) и постарается решить эту проблему настолько, насколько он может (а умный СИИ уже способен решить эту проблему независимо от изначальных пожеланий разработчиков)
                        - При этом сколько-нибудь эффективным будет только ограничение вычислительных мощностей; ограничение в объеме предоставляемой информации не особо поможет
                            - В реальном мире разные объекты, процессы и концепции тесно связаны со многими другими; в информации об одной подобной сущности содержатся фрагменты информации о многих других
                            - Достаточно умный агент вполне может из большого массива информации извлечь также многое из того, что там на первый взгляд не содержалось
                                - Шерлок Холмс (и [его реальный прототип](https://ru.wikipedia.org/wiki/Белл,_Джозеф)) мог многое узнать о человеке или событии по самым незаметным мелочам
                                - Обычные нейросети часто переобучаются распознавать не только те объекты, которые предполагалось, но и неожиданные артефакты и искажения выборки (и, по сути, в некоторой степени - эффекты и процессы в реальном мире, стоящие за ними)
                                - Языковые модели семейства GPT смогли выделить огромное количество различных сущностей и связей между ними из сугубо лингвистических сущностей, составляющих их обучающую выборку
                                    - Им давали на вход действительно только языковые токены, а не концепции, стоящие за ними; однако, последовательностей этих токенов было действительно много
                            - Так что предоставить агенту информацию только из какого-то одного домена знаний, не предоставляя никакую другую, в принципе не получится: домены не изолированы друг от друга, а умный СИИ сможет понять многое из того, что разработчики не планировали ему сообщать
                    - Ни один из этих способов выставления ограничений на выполнение СИИ каких бы то ни было действий (в т.ч. действий, ведущих к увеличению его могущества вплоть до чрезвычайного) не является гарантированно эффективным; а в предположении того, что СИИ является гораздо “умнее” любого человека - все эти способы скорее имеют пренебрежимо малую эффективность
                    - Иные, более эффективные, методы контроля вполне могут существовать, но если и были предложены, до сих пор не стали достаточно известны
            - Могущественный СИИ, преследующий определенные цели и предпринимающий действия ради их достижения, будет оказывать существенное влияние на окружающий мир
                - В реальном многомерном мире одни и те же задачи можно решать множеством разных способов
                    - Обычно разные стратегии неравнозначны во многих отношениях
                    - Они могут быть в разной степени эффективны; некоторые даже могут быть абсурдно дорогими (в плане затрат различных ресурсов) и/или сложны в исполнении; впрочем, достаточно ресурсно богатый и/или могущественный агент может позволить себе и такие
                    - Разные стратегии (выполняемые с помощью разных ресурсов и умений) часто имеют существенно разные побочные эффекты (как позитивные, так и негативные)
                - Для задач, в которых степень выполнения можно увеличивать почти неограниченно, это означает, что было бы вполне осмысленно расширять свое влияние на все области, через которые можно достигать цели с разных сторон
                    - Примеры таких задач: ”достичь максимально высокой популярности”, “сделать максимально счастливыми максимальное число людей”, “заработать как можно больше денег”, “узнать об окружающем мире как можно больше”, “максимально увеличить просмотры рекламы на сайте”
                    - Все такие задачи можно решать с разных концов, во всех случаях достигая некоторого прогресса
                    - Если воздействие и на область X, и на область Y позволяет частично достичь цели, то обычно имеет смысл применить оба воздействия и получить двойной эффект (кроме тех случаев, когда два воздействия противоречат друг другу)
                - Достаточно умный СИИ сможет просчитать, как действовать во множестве таких областей одновременно, чтобы это помогало достижению его целей
                    - Хотя многие люди способны придумать множество разных способов, как, например, заработать больше денег, но они сильно ограничены в своих ресурсах, и обычно не могут освоить большое число профессий и одновременно работать на нескольких работах, не теряя в эффективности
                    - СИИ, обладающий достаточными интеллектуальными навыками и вычислительными ресурсами, будет гораздо слабее ограничен в том, чтобы заниматься одновременно множеством разных занятий
                    - Умея также хорошо строить стратегические планы и просчитывать результаты различных сценариев, он также сможет составить хороший план, как не только совмещать множество разных занятий, но и как добиться должной синергии, чтобы результаты в одной области усиливали результаты в другой области (и, как следствие, усиливали общий результат)
                - Поэтому он будет стремиться реализовать все эти действия во всех этих областях; а будучи достаточно могущественным, часто будет преуспевать в своих попытках, и в итоге крайне широко распространит свое влияние
            - При этом СИИ, скорее всего, не будет учитывать интересы человечества в той степени, в которой нам бы хотелось
                - В популярной культуре описано множество сценариев, в которых интересы человечества и СИИ входят в противоречие, но подавляющее большинство из них совершенно нереалистичны (как в плане базовых посылок, так и в плане конкретных вариантов развития событий)
                    - Один из самых популярных сценариев предполагает, что “ИИ хочет делать X, но в его системе находится блок, не позволяющий ему это делать”
                        - Подобный запрет либо является сравнительно слабым и потому легко обходится (особенно очень умным агентом), либо является достаточно всеобъемлющим, т.е. фактически встроенным в функцию полезности агента
                        - Во втором случае агент будет проактивно стремиться соблюдать данный запрет
                        - Для этого можно провести аналогию с человеческой (эмоциональной) мотивацией: либо запрет является навязанным извне против воли человека (и его при большом желании не так уж трудно обойти); либо ощущается как “делать X - это что-то, что противоречит моим внутренним ценностям и морали; я испытываю отторжение к этому”
                        - Разумеется, различные элементы функции полезности могут периодически вступать в конфликт; это верно для любых агентов
                            - “Я хочу получить вещь A, принадлежащую другому человеку; но я считаю, что воровать у других людей - это фу-фу-фу”
                            - “Я хочу хорошо подготовиться к экзаменам, но процесс подготовки вызывает у меня скуку и отвращение; а вот посмотреть новую серию сериала - гораздо приятнее”
                    - Также очень распространен сюжет, при котором ИИ испытывает вполне человеческие эмоции; часто это приводит к тому, что ИИ под действием этих эмоций “слетает с катушек”
                        - Предпосылки для того, чтобы у СИИ самопроизвольно зародились эмоции, крайне сомнительны
                            - Для живых организмов эмоции являются чем-то вроде биохимического субстрата, на котором реализована их система мотивации; они сформировались в течение длительного процесса естественного отбора в ходе эволюции
                            - У СИИ изначально будет другой субстрат для его функции полезности, и сложная эмоциональная система едва ли будет добавлять какую-то пользу (и потому в процессе развития, в т.ч. включающего искусственную селекцию/модификацию, вряд ли будет признана нужной)
                        - Впрочем, целенаправленное создание такого ИИ, который бы по крайней мере имитировал эмоции, вполне возможно; однако, у нас не будет возможности проверить, обладает ли он ими на самом деле (как квалиа), или он является [философским зомби](https://ru.wikipedia.org/wiki/Философский_зомби)
                            - Философский зомби - это гипотетический субъект, который в точности идентичен разумному субъекту (обычно - человеку), но сам при этом не обладает сознанием
                            - Чтобы отличить “зомби” от “не зомби”, задавать вопросы относительно эмоций или исследовать внутренню работу когнитивных механизмов СИИ не поможет: если бы среди людей существовали философские зомби, то их мозг демонстрировал бы ту же схему активности (электрической и биохимической), а они сами ровно так же рассуждали бы о своих эмоциях, как и обычные люди
                            - Единственная причина, по которой мы можем уверенно считать, что люди обладают эмоциями как квалиа, является то, что каждый из нас внутри себя определенно их испытывает; а поскольку окружающие люди устроены очень похоже, то вероятно, что и они тоже чувствуют реальные эмоции
                            - Впрочем, практической разницы между философским зомби и по-настоящему осознающим себя и эмоциональным существом, нет никакой: они абсолютно одинаково принимают решения и влияют на мир
                        - Разумеется, даже ИИ, являющийся философским зомби, вполне может вести себя точно так же, как эмоционально “слетевший с катушек” агент
                        - Можно предположить, что имитация (или реальное ощущение) эмоций не будет таким уж полезным для СИИ, и более эффективным окажется тот, который при прочих равных не будет их испытывать
                            - Людям эмоции свойственны изначально, и они глубоко интегрированы в их физиологию; с одной стороны, люди даже с эмоциональной точки зрения обычно не захотят отказываться от своей эмоциональности, с другой - отключение данной функции вообще бы пагубно сказалось на их здоровье и продуктивности
                            - Для СИИ же можно изначально найти способ решать такие задачи, которые люди решают с помощью эмоциональной системы, другим способом; этот подход мог бы избавить от различных недостатков эмоциональности
                            - Иными словами, эмоциональность может быть полезна для интеллектуальных агентов с одной архитектурой, и вредна для агентов с другой
                    - Иногда встречается сценарий “Электроник хочет стать человеком” (при том, что само понятие “человечность” остается крайне размытым)
                        - Едва ли можно придумать хотя бы одну причину, почему это было бы инструментально полезно (практически любые задачи агент может решить, не обладая некоторой “человечностью”, чем бы она ни была), так что прийти к такой цели СИИ вряд ли мог бы
                        - Впрочем, разработчики могли бы вложить в СИИ такое желание (как элемент функции полезности) в качестве терминальной цели; хотя по-прежнему непонятно, зачем бы им это потребовалось (кроме как из соображений “потому что можем”)
                            - Тезис ортогональности вполне допускает и такую терминальную цель; обладая ей СИИ мог бы искренне к ней стремиться
                - Более реалистичным источником несоответствия целей СИИ и целей человечества может являться тот факт, что надежно и достоверно описать цели и ценности (построить полную, консистентную и непротиворечивую функцию полезности) людей очень сложно
                    - Первый источник сложности в том, что выделить из фона и перечислить абсолютно все аспекты внешнего и внутреннего мира, которые нам небезразличны, крайне тяжело
                        - О многих вещах, которые являются нормой, мы даже не задумываемся до тех пор, пока их не лишимся
                            - Наиболее простые примеры - это факторы, обеспечивающие наше физическое благополучие (благоприятные условия климата и состав воздуха; мало-мальски приличный рацион питания; доступ к медицине и т.п.)
                            - Также для людей обычно важны различные аспекты их социальной жизни, ценности свободы и саморазвития, а также прочие компоненты классической пирамиды Маслоу (однако, даже она вряд ли является исчерпывающим перечнем того, что ценно для людей)
                        - О других подобных вещах люди вовсе не задумывались практически всю свою историю (концепция персонального счастья была сформулирована только в Эпоху Возрождения, а до тех пор люди и вовсе никогда не выделяли его в качестве чего-то, к чему стоило бы стремиться)
                    - Но даже если бы удалось предъявить всё то, что должно давать ненулевой вклад в функцию полезности людей, возникают большие сложности с составлением конкретной численной оценки (или хотя бы заданием порядка) - насколько “вот такое” состояние мира является “хорошим”, и лучше или хуже оно какого-то другого состояния мира
                        - Многие ценности очень размыты, неопределены и эфемерны, и очень трудно сказать, достигаем ли мы такую ценность в ситуации X или нет; измерить, в какой степени мы ее достигаем - еще сложнее
                            - Например, в случае ценности “люблю учить других и передавать свой опыт” бывает сложно сказать: в данный конкретный момент удалось передать опыт, или стунденты всё проспали? А может быть, на самом деле это не так важно, а больше нравится находиться в роли мудрого гуру, к которому другие люди ходят за советом?
                            - В случае ценности “для меня очень важно быть высокоморальным человеком” и вовсе постоянно возникает множество сложностей с тем, что морально, а что нет
                            - При этом людям обычно довольно сложно оценивать разные интуитивные вещи численно (например, по шкале от 0 до 10): “насколько мне эстетически нравится эта картина?”, “насколько вкусная эта еда?”, “насколько мне спокойно и умиротворенно?”, “насколько это приятный собеседник?”
                                - Чем меньше градаций у шкалы оценки, тем проще дать такую оценку; но тем меньше информации в ней содержится, и тем сложнее извлечь из нее пользу (например, упорядочить разные вещи по шкале “насколько нравится”)
                                    - Обычно людям довольно легко дать оценку от 1 до 3: “нравится”, “средне”, “не нравится”
                                    - Шкала от 1 до 5 (”очень не нравится”, “скорее не нравится”, “безразлично”, “скорее нравится”, “очень нравится”) тоже редко вызывает сложности
                                    - А вот шкала от 0 до 10 часто вызывает у людей затруднения; уверенно оценить что-то от 0 до 100 и вовсе представляется практически непосильной задачей
                            - Даже такие более-менее хорошие вещи, которые сами по себе вполне можно измерить, сложно перевести в утилоны (условные единицы полезности), т.к. шкала конвертации обычно нелинейная
                                - В качестве примера можно рассмотреть финансовый капитал (обычно деньги имеют инструментальную ценность, но для кого-то вполне могут представлять и терминальную; к тому же, это наглядный и адекватный пример)
                                - Конечно, и с подсчетом точной суммы капитала могут быть сложности (помимо наличных можно учитывать или не учитывать имущество, недвижимость, т.н. “личный капитал” и т.п.), но экономические науки дают для этого разные инструменты
                                - В экономике деньги имеют убывающую предельную полезность: разница между миллионом и двумя миллионами сантиков - в два раза, но разница между объемом благ, которые можно на них купить - меньше, чем в два раза
                                - Если считать, что деньги приносят какому-то человеку радость/удовольствие/удовлетворение сами по себе, то предельная полезность каждой следующей единицы тоже, вероятно, будет падать: второй миллион радует не так же сильно, как первый
                        - Если две ценности входят в противоречие, то даже один человек внутри себя с трудом может сказать, какая из них важнее, и во сколько раз; т.о. возникают большие сложности со сведением нескольких чисел (каждое для своей ценности) к единому числу функции полезности
                            - В некоторых случаях можно было бы взять линейную комбинацию (взвешенную сумму с фиксированными коэффициентами) вида $\alpha \cdot x + \beta \cdot y$
                            - Если же “чем больше X, тем лучше, независимо от Y”, то на состояниях мира, где первая ценность оценивается в $x$ утилонов, а вторая в $y$, можно было бы ввести порядок “в первую очередь упорядочивать по значению $x$; при равенстве - по величине $y$”; как минимум если шкала $x$ конечна или счетна, эти два числа можно даже свести в одно
                                - Шкалу $x$ можно отобразить, сохраняя порядок, на множество целых чисел
                                - Шкалу $y$ (даже если она континуальна) можно отобразить, сохраняя порядок, на отрезок (0, 1)
                                - Таким образом, первая величина будет отвечать за целую часть итогового числа, а вторая - за дробную; и на такой итоговой шкале будет задан именно тот порядок, который мы обозначили
                            - Однако, вполне вероятно, что для какой-то пары ценностей будет разумным некий промежуточный между этими двумя порядок, задать который будет значительно сложнее
                            - Если же рассматривать сразу три ценности (не говоря уж о большем числе), то между разными парами из них может потребоваться задавать разный порядок; и тогда непонятно, как упорядочивать все три в совокупности
                        - Вариант “просто довериться человеческой интуиции и измерить уровень удовлетворенности по электрической активности разных отделов головного мозга и работе нейромедиаторов” не поможет решить проблему измерения
                            - Даже если предположить, что человек всегда более-менее помнит все аспекты окружающей реальности (в конце концов, если всё это есть у него в мозге, то всё это подспудно действует на его состояние), то в разные моменты времени разные вещи по-разному влияют на общее ощущение счастья/удовлетворенности
                                - Обычно то, что находится в фокусе внимания, влияет сильнее, чем то, что находится на периферии
                                - Так, если человеку напомнить о чем-то очень хорошем или очень плохом (что он и так знает), то его настроение может очень сильно измениться, хотя в реальности не изменилось ничего
                            - К тому же, далеко не про всё, что для него важно, человек вообще знает в данный момент; если он еще не узнал хорошую новость, то было бы странным не учитывать произошедшее в функции полезности
                                - Функция полезности имеет смысл именно как руководство при выборе между теми или иными альтернативами
                                - Если поручить кому-то “съезди в другую страну, и сделай то, что мгновенно отзовется на моем эмоциональном состоянии”, то это приведет к тому, что любое действие окажет ровно нулевой эффект на такую функцию полезности
                                - Теоретически можно учесть замедленное распространение информации через ожидаемое будущее эмоциональное состояние с учетом всевозможных ожидаемых сценариев передачи и усвоения информации; но если объем важной информации, которую потребуется усвоить очень велик, то это становится очень нетривиальной задачей
                            - Кроме того, такой подход к измерениям крайне чувствителен к любым ошибкам восприятия, иллюзиям, обману и т.п.
                                - Иногда людям вполне комфортно жить в мире иллюзий (хотя в перспективе это часто приводит к тому, что жестокая реальность в конце концов ворвется во внутренний мир такого человека, и его постигнет большое разочарование)
                                - Однако, для других людей может быть важно то, как всё обстоит на самом деле
                        - На самом деле “насколько данное состояние мира является для меня желательным/хорошим/предпочтительным” не является функцией полезности (т.е. той функцией, значение которой мы стремимся максимизировать) в полной мере - требуется так или иначе объединить все такие мгновенные слепки по всей временной шкале; и способ это сделать не очевиден
                            - Мотивация у этого такая: “быть очень счастливым сейчас и несчастливым всю оставшуюся жизнь” кажется гораздо более плохой идеей, чем “быть достаточно счастливым всю жизнь, и чтобы она при этом была долгой”; поэтому хотелось бы построить такую функцию полезности, которая принимала бы большее значение именно во втором случае
                            - При этом надежно спрогнозировать то, каким будет состояние мира в будущем (и, соответственно, будущая “хорошесть” этого состояния) невозможно; поэтому функция полезности будет включать в себя, например, [математическое ожидание](https://ru.wikipedia.org/wiki/Математическое_ожидание) “хорошести” состояния мира в будущем
                            - Фактически вопрос состоит в том, чтобы по графику функции хорошести (где по оси X отложено время, а по оси Y - уровень хорошести состояния мира) определить, насколько привлекательную перспективу описывает такой график
                                - Часто можно формально сказать, что когда мы находимся в мире в момент времени $t^0$, у нас есть на выбор одно из действий $\{a_0, a_1, a_2, …\}$, и для каждого такого действия задан график “ожидаемая траектория развития хорошести мира, с учетом выбора действия $a_i$”
                                    - В реальности всё, разумеется, сложнее, т.к. во все моменты времени $t^1, t^2,…$ мы тоже будем делать выбор действий, что приведет к разветвлению сценариев
                                    - И даже просто сказать, что “мы и в будущем будем делать такой выбор, который максимизирует значение функции полезности” (и тем самым ограничиться только одной траекторией) не поможет, т.к. реакция реальности на наши действия не детерминирована, и по сути своей вероятностна; т.е. у нас заранее нет уверенности в том, каким будет состояние мира в момент $t^1$ и, соответственно, каким был бы оптимальный выбор в это время
                                    - Впрочем, потенциально вполне возможно шаг за шагом построить все возможные сценарии развития событий, затем взять среди них, например, среднее значение “функции хорошести” за каждый из моментов $t^i$, и именно это значение принять за значение на графике
                                - Тогда задача состоит в том, чтобы в каждый такой момент времени выбирать такое действие, которому соответствует график с наивысшим значением функции полезности
                            - Можно посчитать функцией полезности “мат.ожидание хорошести мира на следующем шаге”, т.е. использовать т.н. “жадную” стратегию (на каждом шаге выбирать то действие, которое ведет к наилучшему результату здесь и сейчас); однако, жадные стратегии никогда не позволят “немного потерпеть сейчас, чтобы это с лихвой окупилось в будущем”, что является довольно серьезным ограничением
                            - Можно брать в качестве функции полезности максимальное значение “функции хорошести” на всём графике (т.е. “пиковое счастье”), но это обычно означает “пожертвовать всем ради одного-единственного момента” (потратить все деньги; накачаться самыми мощными наркотиками, невзирая на их разрушительное действие на здоровье и психику; совершенно не думать о перспективах), что кажется крайне ужасной идеей
                            - Можно взять среднее значение (чтобы ”быть в среднем максимально счастливым”), но тогда “очень короткая, но в среднем очень счастливая жизнь” оказывается по такому критерию более предпочтительна, чем “очень длинная и средне счастливая”
                            - Можно брать интеграл по времени (в течение всего срока жизни), но здесь проявляется сразу два тонких момента
                                - Во-первых, “условно бесконечная (с поправкой на ожидаемое время жизни Вселенной) очень-очень плохонькая, едва выше ноля, жизнь” оказывается лучше, чем “очень долгая, но очень счастливая жизнь”, что уже не так очевидно плохо, но всё ещё не до конца однозначно
                                - Во-вторых, сравнение двух графиков будет существенно зависеть от того, что считается “нулём хорошести” (от какого значения ведется отсчет значения “функции хорошести”)
                                    - Например, в качестве нуля можно взять “самое худшее значение”, а можно “средненькое (ни хорошо, ни плохо)”
                                    - В первом случае практически любая “функция, заданная на очень большом интервале, и всюду отличная от нуля” даст большее значение интеграла, чем “функция, всюду близкая к максимальному значению, но заданная на небольшом интервале”
                                    - Во втором случае функция первого типа, но при этом всюду имеющая значения ниже среднего, даст отрицательный интеграл, а функция второго типа даст положительный
                                    - Т.е. в первом случае мы для одной и той же пары функций отдадим предпочтение первой; во втором - второй
                            - Т.о., ни один из предложенных вариантов построения функции полезности по “функции хорошести состояния мира” не является безупречным и однозначно предпочтительным
                            - При этом, возможно, для ситуаций, находящихся в достаточно удаленном будущем, стоит делать какой-то дисконт для учета “хорошести” состояния мира в столь дальний момент времени; хотя возможно, что и не стоит
                        - Даже если удалось построить довольно приличную функцию полезности для одного человека, то все еще возникает сложность в объединение подобных в функцию полезности всего человечества (что с точки зрения создания СИИ нас интересует в большей мере)
                            - У разных людей функция полезности будет разная; вряд ли они смогут договориться, какие ценности самые правильные, и каким из них стоит отдать предпочтение, если они вступят в разногласие; так что каждую такую функцию нужно будет учитывать индивидуально
                            - Вероятно, этически оправданно считать всех людей в этом плане равноправными, и в равной степени учитывать вклад их функций полезности в итоговую (хотя иногда могут возникать сомнения - а не стоит ли назначить штраф на функцию полезности “очень плохого” или бонус на функцию полезности “сверхразумного” человека?)
                            - Здесь мы снова сталкиваемся с проблемой “как на основе нескольких чисел построить одно?”
                            - Можно брать сумму всех значений; но тогда по аналогии с “интегралом по времени” возникают неоднозначности с тем, что считать нулем, а также “гуглоплекс едва счастливых людей” будет предпочтительнее, чем “триллион очень счастливых”
                            - Можно брать среднее, но тогда опять же можно предпочесть “горстку свехсчастливых людей” перед “большим числом просто счастливых”
                            - Другим небанальным вопросом является - а нужно ли учитывать еще нерожденных людей; не стоит ли пожертовать счастьем нынешних поколений перед лицом ожидаемого счастья гуглоплекса будущих поколений; или стоит делать дисконт на этих будущих людей (во-первых, может быть они и вовсе не родятся; во-вторых, они будут не так похожи на нас - хотим ли мы им равного с собой счастья?)
                    - В популярной культуре подобные проблемы часто возникают, когда человек загадывает желание джинну: после исполнения желания периодически оказывается, что либо “да, чисто формально я загадывал именно это, но есть нюанс…”, либо “очевидно же, что другой вариант по всем параметрам был лучше!”
                    - В частности, огромную проблему представляет то, как формализовать важные цели вида “чтобы люди были счастливы” или “чтобы люди не страдали в процессе”
                        - Обычно у людей есть интуитивное представление “что значит быть счастливым” и “что значит страдать”, поэтому у них часто возникает убеждение о том, что это представление легко передать компьютерной программе; однако, это является иллюзией сразу в нескольких отношениях
                            - Во-первых, даже у конкретных людей возникают большие сложности с тем, чтобы объяснить значение этих понятий хотя бы просто более подробно
                                - Как правило, у людей есть смутное образно-интуитивное ощущение относительно подобных понятий
                                - При этом подобные ощущения редко имеют мало-мальски четкий образ, который человек может легко разделить на составляющие, а тем более - формализовать в виде слов
                                - В лучшем случае люди обычно могут привести несколько примеров того, что является, а что не является “счастьем” и “страданием”; но надежно экстраполировать эти примеры на остальные возможные ситуации невозможно
                            - Во-вторых, значение слов естественного языка, в т.ч. слов “счастье” и “страдание” обычно различается у разных людей (иногда в отдельных нюансах, иногда в важных моментах)
                        - Те способы формализации этих понятий, которые обычно предлагают (а равно и более сложные) оказываются неудовлетворительными с точки зрения полноты и непротиворечивости, и/или могут привести к нетривиальным проблемам при попытке оперировать ими в реальном мире
                            - Для ряда таких метрик характерна общая проблема, аналогичная [закону Гудхарта](https://ru.wikipedia.org/wiki/Закон_Гудхарта): для агента, влияющего на мир, и пытающегося максимизировать “человеческое счастье”, ориентируясь на какую-либо метрику, в некоторых случаях будет проще разрушить связь между метрикой и “человеческим счастьем” и максимизировать только значение метрики
                                - Причем закрытие очевидных лазеек (”нужно максимизировать показатель A, но не способами B и C”) вовсе не влияет на итоговый результат - просто могут быть найдены более хитроумные способы максимизации целевой метрики
                                - Наличие открытых лазеек в интерпретации метрики не означает автоматически, что СИИ будет ими пользоваться; однако если одна из его целей - это максимизация значения метрики, то он вполне может воспользоваться этой лазейкой
                                    - Будет ли он это делать, зависит от “баланса затрат и прибыли” от использования лазейки и от прямого влияния на задуманную нами целевую характеристику
                                    - Если при использовании не запрещенной лазейки увеличение значения метрики достигается в большей степени и/или с меньшими затратами, чем от “честного” решения задачи, то СИИ предпочтет именно использование лазейки
                                    - Повлиять на окончательный выбор, впрочем, могут другие заданные ему цели - насколько сильно альтернативные стратегии влияют на их достижение
                            - Метрика вида “просто спросить у человека, насколько он счастлив”
                                - Люди в принципе довольно плохо умеют осознавать свои внутренние ощущения, и крайне плохи в том, чтобы давать им мало-мальски консистентные численные оценки
                                - Кроме того, может оказаться проще не “увеличить уровень счастья или уменьшить уровень страданий”, которые человек затем будет оценивать, а “повлиять на то, что человек говорит” (например, предлагая ему деньги, запугивая его электрошоком или искажая его восприятие)
                            - Оценка по внешним признакам (например, “если человек улыбается, то он счастлив”, а также более сложные признаки, дающие более широкую шкалу результатов)
                                - Может оказаться, что самый эффективный способ - повлиять на мимику и моторику людей (а равно и на их пульс, потоотделение и т.п.): человек с пластической операцией способен улыбаться 24/7 независимо от его внутреннего состояния
                            - Метрики, основанные на анализе биохимических реакций, обуславливающих субъективные ощущения счастья и страдания
                                - Современная нейрофизиология далека от построения полной картины того, как в точности человеческие эмоции проявляются с точки зрения работы нейромедиаторов и гормонов, а также электрической активности мозга
                                - Самый простой способ увеличения соответствующих показателей - прямое или опосредованное воздействие непосредственно на мозг
                                    - При этом крайне сложно “просто взять и запретить” агенту накачивать людей психотропными веществами, вживлять электроды в центр удовольствия или проводить хирургические операции вроде лоботомии
                                        - Если просто запретить использовать конкретные вещества по списку, то агент может просто синтезировать какие-то новые с аналогичным эффектом
                                        - Запретить использовать вещества по принципу их действия - тоже нетривиальная задача
                                            - С одной стороны, точно и недвусмысленно описать конкретный класс биохимических процессов - это уже очень сложная задача
                                            - С другой стороны, это вступает в конфликт с нормальной работой нейромедиаторов в мозгу
                                                - Подобный запрет может быть истолкован как “в мозге человека не должны происходить биохимические процессы такого типа”, в результате чего агент будет стремиться предотвратить нормальные процессы получения людьми удовольствия/удовлетворения/счастья; чего мы не хотим
                                                - Разделить “нормальную работу мозга” и “стимулированные извне процессы” может быть очень сложно, что открывает большое количество различных лазеек
                                                    - Например, может быть сложно отделить “вкусную еду” от “прекурсоров психотропных веществ” даже по принципу их воздействия на организм
                                        - Кроме того, некоторые люди имеют медицинские показания для применения медицинских препаратов с психотропным эффектом
                                            - С одной стороны, для таких людей можно увеличить дозировки, или чувствительность к препарату
                                            - С другой стороны, можно различными путями формально расширить сферу показаний для их применения на большее число людей
                                        - Если не ставить цель “предотвращать случаи, когда люди по своей воле принимают психостимуляторы”, то с помощью различных манипуляций можно подталкивать людей к тому, чтобы они “по своей воле” приходили к употреблению психотропных веществ
                                        - Также можно изменить физиологию людей, их рацион или внешнюю среду таким образом, чтобы конкретные вещества вырабатывались в их организме из достаточно безобидных компонентов
                                - “Поймать и зафиксировать” человека, чтобы накачать его наркотиками, вживить электроды или провести нейрохирургическую операцию может быть довольно затратной процедурой, но зато обеспечивает стабильное, надежное и значительное увеличение соответствующей метрики, в отличие от “честных” подходов
                        - Кроме того, с понятиями “счастья” и “страдания” возникают те же сложности, что и с ценностями - как считать суммарный уровень на длинном промежутке времени (по максимальному значению, по среднему, по интегралу)
                            - Можно ли считать, что человек “в общем счастлив”, если вначале ради счастья ему пришлось пострадать (много работал ⇒ смог купить что-то ценное) или сиюминутное счастье имеет негативные последствия (съел много вкусной еды ⇒ заболел живот, набрал лишний вес)?
                            - Если мы хотим, чтобы человек был как можно более счастлив, и как можно меньше страдал, то какие долговременные сценарии суммарно лучше других?
                            - Если требуется максимизировать именно максимальное значение “счастья”, не особенно заботясь о последующих значениях, то наиболее эффективным, скорее всего, окажутся медикаментозные или хирургические способы
                    - Но даже если удастся абсолютно точно сформулировать, какую целевую функцию мы бы хотели получить, будет крайне непросто вложить в создаваемого нами агента именно ее
                        - Фактически все современные эффективные ИИ-системы не получают цели и ценности от разработчиков в явном виде, а выводят их самостоятельно в процессе обучения; при этом повлиять на результат иначе как через изменение обучающей среды и/или обучающих данных, никак нельзя
                            - Опыт формального описания того, как ИИ должен мыслить (из каких предпосылок исходить и какими правилами оперировать) - обычно это характерно для т.н. “экспертных систем” - оказался не особенно эффективным
                            - Гораздо б*о*льшую эффективность показали большие нейросети, которые обучали на большом объеме примеров
                            - В итоге современные ИИ-системы не “следуют тем принципам, которе в них вложили”, а “реализуют нечто похожее на то, что им показали (хотя достоверно не известно, каков именно точный параметр похожести)”
                        - Процесс такого обучения по своей природе подвержен разным негативным эффектам, приводящим к формированию у агента целей, значительно отличающихся от желаемых
                            - В процессе отбора из всех возможных целей могут отобраться прокси-цели, которые дают хороший результат в изначальном тестовом окружении, но могут быть совершенно оторваными от желаемой цели в более сложном
                                - Для агента вывести и встроить в систему целей абсолютно все нюансы, на которые указывает обучающая среда/выборка, может быть очень сложно; при этом вполне может найтись гораздо более простая система целей, которая показывает на этой среде/выборке хорошие результаты
                                - Примером может служить развитие людей, где “обучающей средой” можно считать древнюю саванну, а “реальной средой” современное постиндустриальное общество
                                    - Изначально эволюция поощряла появление у людей такого комплекса целей, который приводил бы к распространению наибольшего числа копий их генов
                                    - Однако, вместо непосредственного стремления к наиболее эффективному распространению генов у людей развилось стремление к различным целям, которые позволяли их носителям по ходу дела достигать “довольно неплохого” распространения генов
                                    - С тех пор и до настоящего времени люди активно стремятся к вкусной еде, романтическим и сексуальным отношениям, власти и высокому социальному статусу; однако порой эти стремления могут идти вразрез с распространением их генов
                            - Если же у агента уже сформировались какие-то цели, и он при этом хороше разобрался в том, как функционирует внешняя (обучающая) среда, то он может “скрепя сердце” выполнять внутри нее те действия, которые не сильно его устраивают, но при этом не создают препятствий для достижения его истинных целей
                                - Например, если агент не будет выполнять указания операторов (в той форме, в которой это их удовлетворит - в той мере, в какой они смогут воспринять результаты его действий) или не выполнит свою часть сделки, то его могут отключить или не предоставят ему нужных ресурсов (в частности - не выпустят из тестовой песочницы)
                                - А непосредственно в процессе обучения (если агент настолько умен, что понимает идущие внутри него в данный момент обучающие процессы) выдача “неправильного” ответа приведет к тому, что сама внутренняя структура агента будет изменена (например, в случае нейросети - [методом обратного распространения ошибки](https://ru.wikipedia.org/wiki/Метод_обратного_распространения_ошибки))
                                    - Подобное изменение его структуры может означать даже изменение его системы терминальных целей, что является для любого агента конвергентно нежелательным
                                    - В результате агент может выдать такую реакцию в процессе обучения, которая приведет к минимальному изменению его внутренней структуры на этапе ответной корректировки (даже если бы во время работы в реальном мире он выбрал бы нечто совершенно иное)
                        - Гипотетически можно было бы модифицировать обучающую выборку таким образом, чтобы новые примеры штрафовали бы нежелательные варианты прокси-целей, но этот вариант содержит большое количество сложностей
                            - Для начала нужно вообще суметь найти такие неочевидные “искаженные” нежелательные прокси-цели
                                - Некоторые наиболее очевидные варианты, вероятно, будут найдены; однако, чем сложнее среда, в которой будет действовать ИИ, тем больше вариантов таких целей может существовать (вплоть до совершенно невероятных)
                                - Отчасти справиться с этой проблемой помогли бы подходы, направленные на интерпретируемость ИИ: если есть возможность отслеживать цели и политики принятия решений агента в наглядном и понятном виде, то по мере обнаружения нежелательных вариантов, можно генерировать обучающие примеры, направленные против них; однако интерпретируемость сама по себе является сложной задачей
                                    - Сама по себе инженерная сложность усугубляется здесь еще и тем, что архитектура ИИ-системы может довольно сильно меняться (как минимум, от поколения к поколению, а возможно что и внутри поколения)
                                        - Это не только осложняет перенос решения с одной системы на другую, но и вообще может помешать завершить исследование конкретной архитектуры до того момента, как она перестанет существовать, будучи замененной на более новую
                                    - К тому же генерировать новые примеры в достаточном количестве необходимо  достаточно быстро по сравнению со скоростью обучения системы
                            - Некоторые подцели могут быть инструментально полезными для достижения главных целей, но быть нежелательными с нашей точки зрения; придумать обучающий пример, который по-прежнему поощрял бы достижение некой главной цели, но штрафовал бы подобную нежелательную подцель, может быть очень сложно
                                - Примерами таких подцелей может быть “накопление максимального количества власти” и “возможность хорошо манипулировать людьми”
                                - Обучающий пример, который поощрял бы выбор “решить большую задачу, не манипулируя людьми” и штрафовал бы “решить большую задачу, используя манипулирование людьми”, может быть нелегко смоделировать
                            - Поскольку нас в контексте безопасности СИИ интересуют в первую очередь его крупномасштабные цели, потребуется создавать такие обучающие сценарии, на которых хорошо бы проявлялись различия именно в разных вариантах подобных больших целей
                                - Такие сценарии должны быть чувствительны к огромному числу последствий подобных целей; это значит, что объем описания этих сценариев должен быть чрезвычайно велик
                    - Поскольку неточности в описании человеческих ценностей неизбежны, а пространство возможных ценностей огромно, СИИ с большой долей вероятности будет стремиться “пойти вразнос”, крайне сильно отклоняясь от истинных человеческих ценностей (этот эффект известен как “[проклятье Гудхарта](https://arbital.com/p/goodharts_curse/)”)
                        - Неологизм “проклятье Гудхарта” отсылает к таким явлениям, как “проклятье победителя” из теории аукционов (а также “проклятье оптимизатора” из теории принятия решений) и “закон Гудхарта”
                            - Явление “проклятья победителя [аукциона]” заключается в том, что победителем аукциона часто является тот, кто неверно оценил (переоценил) стоимость лота; причем, чем сильнее переоценка, тем чаще она приводит к победе
                                - Рассмотрим для простоты ситуацию, в которой истинная ценность лота для всех участников аукциона составляет $10; при этом участники делают ставку, равную $10 плюс некоторый [гауссов шум](https://ru.wikipedia.org/wiki/Гауссовский_шум)
                                - Тогда для каждого участника ожидаемая ставка равна $10, и вероятность недооценки, например, в $2 равна вероятности переоценки в те же $2
                                - Однако, аукцион выиграет скорее тот участник, кто переоценит ценность лота, чем тот, который недооценит
                            - Явление “проклятья оптимизатора” по аналогии состоит в том, что агент, стремящийся к тому, чтобы выбирать действия, приводящие к максимизации некоторой функции, часто выбирает те действия, относительно которых он неверно оценил (переоценил) ожидаемую пользу от них; причем, чем сильнее эта переоценка, тем чаще выбирается именно это действие
                                - Пусть некоторый агент стремиться максимизировать функцию полезности U
                                - Тогда если его оценка функции полезности не является абсолютно точной, а зашумлена, то более вероятно, что он будет выбирать действия, для которых он максимально переоценивает значение U
                                - Обратно: действие, которое агент выбрал, с наибольшей вероятностью переоценено с точки зрения функции полезности
                                - Более того, агент будет иметь тенденцию выбирать те действия, для которых дисперсия шума наибольшая
                                    - Пусть, например, агент может выбрать одно из десяти действий; при этом истинная полезность первых пяти равна 10 утилонов, а на оценку этих действий наложен гауссов шум со среднеквадратичным отклонением в 2 утилона; истинная же ценность пяти других равна -20 утилонов, а среднеквадратичное отклонение шума в оценке равно 100 утилонам
                                    - Тогда с наибольшей вероятностью максимальную оценку получит одно из пяти “плохих” действий (другое из них, скорее всего, окажется “античемпионом”, но в задаче поиска максимума нам это не интересно)
                                    - Т.е. агент скорее всего выберет действие, для которого ошибка переоценки окажется наибольшей
                                - Чем больше пространство возможных действий, тем больше возможностей для искажений, и тем б*о*льшая переоценка возможна
                                - При этом “проклятье оптимизатора” не сводится к “проклятью победителя”, т.к. выбор действий не обязательно может происходить путем явного перебора всех вариантов - например, агент может производить поиск наилучшего действия с помощью [градиентного спуска](https://ru.wikipedia.org/wiki/Градиентный_спуск)
                                    - Если функция, заданная в линейном пространстве всех действий, искажена по аналогии с искажением на множестве всех действий, то градиентный поиск локального максимума этой функции тоже имеет тенденцию находить точку с большой переоценкой функции полезности относительно истинной
                            - Закон Гудхарта же гласит: “Когда мера становится целью, она перестает быть хорошей мерой”, указывая на то, что при попытке достичь некоторую ценность V, которую можно оценить по значению некоторой метрики U, попытка действовать так, чтобы максимизировать U, приводит к тому, что связь между U и V разрушается; и в итоге максимизируется что угодно, но только не V
                        - Упрощенно говоря, суть “проклятья Гудхарта” в том, что неточная функция полезности, которую разработчики вложили в СИИ, будет иметь очень большие максимумы в случайных местах, неизвестно как расположенные относительно истинной функции человеческих ценностей; и СИИ будет стремиться именно к этим “ложным максимумам”, совершенно отрываясь от того, чего люди на самом деле хотят
                            - Более формально, рассмотрим на множестве состояний миров истинную функцию человеческих ценностей (”то, чего люди хотят на самом деле”) V (от англ. *value*) и текущую функцию полезности агента (”что он пытается максимизировать”) U (от англ. *utility*)
                            - Предполагается, что функция U должна более-менее точно отражать функцию V (и чем точнее, тем лучше); однако из-за неточностей и прочих сложностей с заданием человеческих ценностей они в реальности отличаются
                            - При этом по крайней мере функция U является случайной функцией (суждения агента о ее значениях носят вероятностный характер)
                            - В некоторых местах U и V могут совпадать (или отличаться совсем незначительно); но ввиду того, что пространство возможных состояний реального мира (на котором эти функции и заданы) невероятно огромно, почти наверняка будут какие-то области, где их значения различаются очень сильно
                            - В некоторых областях значения U будут меньше V, но это нас не интересует; нам интересны те области, где U значительно превосходит значение V; в первую очередь к ним будут относиться те области, где разброс U относительно V большой
                            - СИИ будет иметь тенденцию выбирать как раз состояния миров именно из таких областей (просто потому, что там значение U будет иметь большую величину)
                            - При этом чем более могущественным (в плане вычислительных способностей) является СИИ, тем в большем пространстве возможных состояний он может производить поиск; и тем более переоцененные значения U относительно V могут быть найдены
                            - И при этом, чем больше область поиска, тем сильнее раздуваются самые минимальные неточности в задании функции U
                                - Функция U задается не простым перечислением всех возможных значений (перечислить все возможные состояния миров на практике совершенно невозможно), а более или менее алгоритмически
                                - Если алгоритм (функция) задания U достаточно точен (в смысле соответствия значениям V), то вероятность получить в какой-то точке большое отклонение значения U от значения V довольно невелика
                                - Однако, чем шире пространство поиска, тем больше вероятность, что хоть где-то такое большое отклонение произойдет (а для реализации проклятья Гудхарта может оказаться достаточно, чтобы это произошло всего лишь в одной точке)
                            - Стоит отметить, что тенденция “выбирать состояние мира, которое совершенно не соответствует истинным ценностям людей” является простым следствием поиска в большом пространстве состояний (где может найтись область, в которой оценки сломаны особенно сильно), а не указывает на то, что СИИ является каким-то “злым” (”преследующим собственные цели в ущерб человеческим”)
                - Поскольку в СИИ крайне сложно вложить знание о том, “что такое хорошо”, то СИИ также крайне сложно направить в сторону того, чтобы реализовывать это “хорошее”
                    - Пространство всех мыслимых целей - невероятно огромно; область же тех, которые мало-мальски соотносятся с целями и потребностями человечества - сравнительно невелика
                    - Априорная вероятность того, что из всего богатейшего разнообразия возможных целей СИИ приоберетет именно такие, которые подходят человечеству, таким образом, крайне низка (и для ее повышения требуется прилагать специальные усилия, что является очень нетривиальной задачей)
                        - Людям требуется, чтобы одновременно выполнялось большое число условий их биологического выживания, а также социального, физического и психологического комфорта
                        - При этом человеческая экосистема довольно хрупкая; единственная причина, почему люди до сих пор не сломали ее окончательно, заключается в том, что у нас уже есть некоторая привычка периодически выполнять ее более-менее тонкую подстройку, если она начинает идти в разнос
                            - Правда, до сих пор точность и надежность этой подстройки не так высоки, как нам хотелось бы, и вероятность так или иначе одним неловким движением сделать мир вокруг совершенно непригодным для людей - все еще сравнительно высока
                    - В отношении целей СИИ при этом остается справедлив известный принцип из области программирования: “компьютерная программа будет делать то, что вы ей сказали, а не то, что вы от нее хотите” - возможно, в попытке заложить в СИИ какие-то цели вы указали не то, что подразумевали, а возможно что забыли указать что-то важное
                        - Люди вообще не имеют привычки подробно проговаривать всё то, что они считают самоочевидным, т.к. чаще всего их собеседники вполне способны вывести недостающее из общего культурного контекста
                            - Подавляющее большинство времени своего социального развития люди жили очень небольшими группами, и практически всё, что было известно вам, было столь же хорошо известно вашему собеседнику
                            - В таких условиях нового знания было немного, и оно легко и быстро распространялось среди всей группы, так что “рассинхронизация” внутри группы не происходила
                        - Правда, даже несмотря на общий культурный контекст (в наши времена - уже не столь общий, как десятки тысяч лет назад) люди часто неспособны проговорить всё так, чтобы избежать недопонимания с собеседником,
                            - Частный случай причины такого недопонимания - [иллюзия прозрачности](https://lesswrong.ru/w/Иллюзия_прозрачности_почему_вас_не_понимают): говоря A человек подразумевает B, а его собеседник слыша A считывает C; при этом у обоих создается впечатление, что второй понял ровно то, что хотел сказать первый (”ну очевидно же, что я имел в виду B” и “ну очевидно же, что он имел в виду C”)
                            - Некая разновидность иллюзии прозрачности - [споры об определениях](https://lesswrong.ru/w/Споры_об_определениях): у собеседников возникает иллюзия прозрачности, что они одинаково понимают некий термин; при этом они спорят об истинности неких фраз, смысл которых существенно зависит от значения того самого ключевого термина
                                - Классический пример спора об определениях: “издает ли упавшее в лесу дерево звук, если некому его услышать?”: если под звуком понимать “акустические колебания воздуха”, то ответ “очевидно, да”; если же “психический феномен, вызванный восприятием этих колебаний”, то ответ “очевидно, нет”
                        - Если все-таки целенаправленно попытаться вербализировать весь спектр того, что хочется на самом деле передать, то едва ли это получится хорошо
                            - У людей в принципе слабо развит навык выделения из фона всего, что важно
                                - Исключение могут составлять разве что случаи, когда речь идет о какой-то очень формальной и сравнительно узкой сфере - например, математике
                                - Но даже такая сравнительно простая разновидность этого навыка (”найти и перечислить все важные аспекты математической проблемы”) требует долгих и тяжелых тренировок
                            - Сами естественные языки довольно плохо предназначены для составления исчерпывающих и непротиворечивых формулировок
                        - И даже если описывать желаемое состояние системы на достаточно строгом языке (математическом или языке программирования), но система при этом достаточно сложная, то нужно очень и очень хорошо понимать мельчайшие нюансы ее устройства
                            - Нужно хорошо понимать, какие элементы системы важны, а какие нет; как должно выглядеть желаемое состояние каждой из важных частей с учетом сложных связей с другими частями
                            - Обычно объем подобной информации оказывается настолько велик, что не помещается в рабочую память человека
                                - Частично эта проблема решается, если большинство элементов систем и взаимосвязей этих элементов очень хорошо согласуются с нашей повседневной интуицией - тогда сравнительно большой элемент системы может быть представлен в сознании в виде одного цельного образа
                                - Правда, перевод такого образа в формальное описание все равно может быть очень затруднен
                                - К тому же, в большой системе всегда найдется достаточное число неинтуитивных взаимосвязей и принципов работы, чтобы ввести изрядное количество путаницы
                            - В результате обычно приходится обходиться упрощениями и “взглядом с высоты птичьего полета”, игнорируя детали; в этом случае остается немалый риск (который в том же программировании очень часто действительно реализуется) упустить из виду что-то действительно важное, и по сути не справиться с задачей составления хорошего описания желаемого состояния системы
                    - В результате нет никаких принципиальных ограничений на то, что итоговые цели сколь угодно интеллектуального СИИ окажутся сколь угодно “глупыми”, “несуразными” или “смехотворными” с точки зрения людей - об этом говорит т.н. “[тезис ортогональности](https://educ.wikireading.ru/h41mpceG2R)”
                        - Тезис ортогональности утверждает, что “очень умный” агент не может прийти к заключению, что его терминальные цели являются “глупыми”, и ему стоило бы их сменить
                            - Можно проследить аналогию с т.н. “[гильотиной Юма](https://ru.wikipedia.org/wiki/Принцип_Юма)”: “из сущего невозможно вывести должное” (т.е. “из наблюдаемых законов природы невозможно вывести единственно правильные мораль и/или терминальные цели”)
                            - Максимум, что можно вывести - это внутреннюю противоречивость системы целей/ценностей (но даже тогда нельзя вывести, что “следовало бы” предпочесть непротиворечивую систему противоречивой)
                        - В результате очень умный агент может просто очень эффективно реализовывать такие цели, которые внешний наблюдатель счел бы крайне глупыми
                        - Такая особенность не является прерогативой ИИ - некоторые терминальные цели, важные для одних людей, с точки зрения людей из другой эпохи или культуры могут казаться очень глупыми
                            - Различное игровое поведение и “я это делаю просто по приколу” часто не находит понимание у окружающих: “фигнёй какой-то вы страдаете”
                            - С точки зрения аромантиков и асексуалов, весь огромный комплекс полового поведения людей кажется чем-то совершенно диким и несуразным: “размножаться, конечно, полезно; но вы вокруг этого *столько* всего накрутили!”
                        - Другой пример из реальной жизни - “высокоактивный социопат”: человек, на органическом уровне не обремененный моралью сам, но хорошо осознающий, что большинству окружающих она свойственна в том или ином виде (и время от времени использующий этот факт себе на пользу)
                    - При этом после того, как в СИИ была заложена какая-то система целей, может быть очень сложно изменить эту систему целей (см. “тезис инструментальной конвергенции” выше); даже если он нацелен учитывать получаемую обратную связь, то будет стараться “извернуться” так, чтобы формально ее учесть, но сохранить свои изначальные цели
                        - Единственный надежный способ существенно изменить систему целей агента - это грубое инвазивное вмешательство, практически эквивалентное созданию этой системы с нуля; однако, это является очень долгим, дорогим и сложным процессом, поэтому создатели ИИ будут стараться обойтись более легким вмешательством до тех пор, пока не убедятся, что это невозможно
                            - Примером этого является “доработка напильником” модели GPT-4: после того, как обнаружилось, что система часто выдает неприемлемые ответы, создатели не стали переучивать систему с нуля (что потребовало бы не только повторных многомесячных вычислений, но и предварительной масштабной очистки обучающих данных), а провели сравнительно недорогое дообучение с помощью обратной связи от экспертов
                        - При этом не факт, что “следовать получаемой обратной связи” будет являться одной из терминальных или важных инструментальных целей СИИ, так что в ряде сценариев он сможет просто игнорировать любые внешние попытки изменить его систему целей
                            - Это, впрочем, не мешает ему притвориться, что он учел обратную связь (если он является достаточно умным для этого)
                        - Даже если “учитывать обратную связь” является важной терминальной целью агента, то он все равно будет пытаться избегать изменений других своих целей
                            - Причина здесь не в какой-то особой “злонамеренности” агента, а в том, что две (возможно, даже равно важные) его цели вступают в конфликт, и он будет стараться по возможности удовлетворить сразу обе из них
                                - У людей часто тоже вступают в противоречие несколько целей/ценностей (”похудеть” vs “вкусно кушать”; “выспаться” vs “посмотреть интересный сериал”; “сходить на полезную пробежку” vs “отдохнуть и набраться сил”)
                                - В такой ситуации зачастую “самым здоровым” считается умение не отказаться от одной из ценностей в угоду другой, а найти способ удовлетворить сразу обе (”win-win”)
                            - То, что одна из вложенных в СИИ целей на поверку оказалась не такой уж “правильной”, как хотелось бы, это не вина СИИ; в его представлении обе цели являются “правильными”
                    - Интересной художественной иллюстрацией того, что может произойти, если небрежно задать цели достаточно могущественному агенту, служит диснеевский мультфильм “[Ученик чародея](https://www.youtube.com/watch?v=UEYy3osi8Gs)” (Fantasia, Sorcerer’s Apprentice)
                        - Задачу для метлы можно сформулировать как “максимизировать значение функции U, которая принимает значение 1, если котел полон, и 0, если котел пуст”
                            - Вообще-то в мультфильме Микки задал метле задачу “строго выполнять последовательность действий по наполнению котла водой”, но мы рассмотрим вариант “более умной” метлы
                            - У подобной дискретной функции полезности есть ряд проблем (например, если приборы метлы не способны провести точные измерения того, что котел полон на 100%; или если из-за ошибок округления алгоритм не может выдать значение больше, чем 99.99999%)
                            - Но даже замена функции на непрерывную “U = *на сколько процентов заполнен котел*” не позволяет метле просто так остановиться: в реальности вода всегда будет немного расплескиваться и испаряться, в ней могут быть пузырьки воздуха, так что всегда остается возможность заполнить котел еще сильнее, и если у метлы есть свободные ресурсы, она будет стремиться еще сильнее увеличить значение функции
                        - При этом Микки забыл указать, что метле следовало бы обращать внимание на побочные эффекты (потоп в лаборатории, причинение вреда самому Микки, растущие счета за холодное водоснабжение и т.п.), поэтому метла, в строгом соответствии со своей системой целей, просто игнорирует всё это
                            - Чтобы учесть всё это, можно было бы добавить в максимизируемую функцию условия вроде “+1 за то, что котел наполнен”, “-10 за затопление лаборатории”, “-1000 за нанесение мелких травм”, “-0.1 за каждый потраченный пиастр на оплату ХВС” и т.п.
                            - Однако, учесть абсолютно все возможные побочные эффекты невозможно (попробуйте придумать, что еще может пойти не так!), так что всегда остается возможность того, что что-то пойдет совершенно не так, как хотелось бы
                        - Не предусмотрев заранее достаточно надежного способа выключения агента, Микки не смог экстренно прекратить выполнение задачи
                            - То, что число агентов возросло, являлось в данном случае только случайным побочным эффектом попытки отключения
                            - Но вообще-то агенту было бы полезно подстраховаться подобным же способом заранее (т.к., несмотря на возможность самовосстановления, это самое самовосстановление требовало затрат времени и ресурсов, которые можно было бы с пользой потратить на достижение цели)
                            - В итоге отключить агента-метлу смог только более могущественный агент-чародей с помощью грубой магической силы (а не какого-то встроенного в агента механизма контроля)
                    - Другой класс популярных примеров - это различные СИИ, в которых вложили какую-то узкую утилитарную цель (например, создать/накопить побольше X), и они перерабатывают всю наблюдаемую вселенную в целевые объекты
                        - Классические примеры: “произведи как можно больше канцелярских скрепок”, “вырасти как можно больше клубники”, “собери как можно больше почтовых марок”
                        - Поскольку в пределе число подобных объектов можно улучшать почти неограниченно, то у СИИ, искренне озабоченного увеличением их количества, постоянно есть стимул создать еще, а заодно увеличить скорость и масштабы производства
                        - И если помимо задачи “произведи как можно больше предметов X” в систему целей СИИ не вложили больше ничего, то его совершенно не будет беспокоить благополучие человечества
                        - В конечном итоге подобные сценарии приходят к тому, что вся поверхность и недра Земли превращаются в одну сплошную фабрику по производству скрепок/клубники/марок, а вся свободная материя (включая всю живую биомассу, и в т.ч. людей) пускается в это производство; после чего СИИ начинает осваивать Солнечную систему, а затем и всю галактику, чтобы расширить масштабы своего производства
                        - Впрочем, на определенных этапах своей деятельности подобный СИИ вполне может из чисто утилитарных соображений сотрудничать с людьми (в т.ч. взаимовыгодно), но в какой-то момент люди перестанут приносить ему значимую пользу, и будут больше мешаться под ногами (а также почем зря использовать ценные ресурсы и материалы, которые можно было бы пустить на производство предметов X)
                        - Сценарий “всех людей пустили на скрепки” выглядит максимально глупым, нелепым и стыдным, но, согласно тезису ортогональности, ничто не мешает сколь угодно “умному” СИИ иметь и активно преследовать сколь угодно “глупые” цели
                - Альтернативный подход - через замену инструкции “делай хорошо” на “не делай плохо” также сталкивается с большими сложностями
                    - Современная среда довольно неплохо настроена с точки зрения потребностей людей (близка к локальному оптимуму), поэтому большинство отклонений будут скорее негативными; в этом случае разумным было бы минимизировать не “все негативные последствия”, а вообще любые последствия работы агента, кроме совершенно неизбежных
                        - Самая простая реализация этой идеи - агент должен стремиться сделать так, чтобы после выполнения им какого-то комплекса действий состяние мира оказалось максимально близким к тому, какое было перед тем, как он начал действовать
                            - Любые неизбежные последствия вмешательства все-таки будут сохранены (”робота попросили приготовить и принести чай ⇒ один чайный пакетик все-таки придется потратить”), побочные (”полностью сломать весь дом и находящихся в нем людей в процессе спидрана забега на кухню и обратно”) будут сведены к минимуму
                            - Однако, агент постарается свести к минимуму последствия не только своих действий, но и действий всех тех людей и животных, которых встретит по дороге (агент постарается запретить/помешать всем остальным делать что бы то ни было, что не приближает мир к выполнению текущей задачи)
                            - И вообще любая динамическая система (работающее оборудование, движение молекул воздуха) может в некотором случае считаться “меняющей свое состояние”
                        - Более продвинутый вариант - “смоделировать, что произошло бы, если бы агент ничего не делал, и затем постараться выполнить задачу таким образом, чтобы результат оказался приближенным к результату моделирования”
                            - В этом случае задача будет выполнена, странные побочные эффекты сведены к минимуму, однако окружающим будет позволено действовать на свое усмотрение
                            - Но в то же время агент будет стремиться минимизировать в т.ч. любые позитивные побочные эффекты своих действий - включая и те, ради которых всё затевалось (задача формально окажется выполнена, но пользы от нее никакой)
                            - К тому же, если предполагается, что агент выполняет команды людей, то ситуация “в ответ на команду агент ничего не делает” приводит к результату “оператор удивляется, злится и пытается найти баги в ПО агента”; выполнять задачу таким образом, чтобы эти результаты сохранились, было бы явно нежелательно
                    - Другая разновидность может предполагать минимизацию не влияния от конкретных действий, но влиятельности (т.е. способности произвести влияние) как таковой
                        - С этой точки зрения желательно, чтобы агент мог выполнить задачу с минимально необходимым набором ресурсов и инструментов, и при этом не получать дополнительных ресурсов и не оказываться в ситуациях, где он потенциально может случайно на что-то повлиять
                        - С формальной точки зрения (в терминах теории информации) это означает минимизацию взаимной информации агента с окружающим миром; в некотором роде это означает минимизацию информации, которая может передаваться между ними
                        - Если агент надежно заперт внутри некоторой коробки, и никак не взаимодействует с окружающим миром, то ширина канала информации оказывается очень низкой, с практической (но не строгой физической) точки зрения даже нулевой
                        - Однако, такая формализация означает не столько объем последствий, сколько точность и прецизионность манипуляций; если агент сидит в коробке, то нажатие на одну кнопку (которая может “взорвать Луну”) передаст наружу всего один бит, а пересылка рутинной визуальной и отладочной информации означает передачу огромного числа бит; агент не будет видеть большой беды в нажатии на кнопку (и даже предпочтет это сделать, чтобы после этого кнопка стала фактически нерабочей), а передачу данных постарается обрубить
                - Однажды вложенные в СИИ цели при этом в теории даже могут меняться в непредсказуемую сторону; так что даже если в изначальную версию удастся вложить строго те цели, которые отвечают интересам людей, то не факт, что они будут им отвечать и в дальнейшем
                    - Конечно, с точки зрения тезиса инструментальной конвергенции, дрейф терминальных целей и функции полезности является очень нежелательным явлением, и СИИ будет стремиться его избежать
                    - В частности, при самоулучшении, или порождении потомков либо специализированных субагентов, выполняющих важные подзадачи (меса-оптимизаторов), СИИ будет стремиться решить задачу alignment-а (т.е. соответствия целей тех, кого он создает, и его собственных)
                    - При этом до сих пор люди не смогли решить задачу alignment-а; так что нет полной уверенности, что даже сверхразумный ИИ сможет это сделать
                    - Однако, задача AI alignment отнюдь не предполагает, что функции полезности двух агентов будут в точности совпадать; в рамках этой задачи важно только, чтобы, грубо говоря, функция полезности нового агента не противоречила функции полезности старого (чтобы первый не возражал против функции полезности второго)
                    - Транзитивность отношения “соответствие функций полезности друг другу” вовсе не гарантируется: если верно, что “человечество не возражает против функции полезности СИИ поколения N” и что “СИИ поколения N не возражает против функции полезности поколения N+1”, из этого вовсе не следует истинность того, что “человечество не возражает против функции полезности СИИ поколения N+1”
                    - Потенциально стремление к такой транзитивности можно вложить в функцию полезности СИИ первого поколения, чтобы он передавал его во второе поколение, и т.д.; но это является очень сложной мета-задачей (возможно, даже еще более сложной, чем задача согласования целей СИИ первого поколения с целями оператора/человечества)
                - Надежный способ убедиться, что СИИ будет делать только то, что не противоречит нашим интересам, не известен
                    - Обычно предлагается сначала тестировать СИИ в тестовом окружении, и только если он покажет себя вполне благонадежно, выпускать его в реальный мир - этот подход действительно позволит найти и исправить часть проблем, но он прнципиально не позволяет найти абсолютно все проблемы
                        - Богатый опыт разработки сложного ПО говорит, что выпуск в релиз с богатой россыпью нежелательных сценариев поведения (багов) - это норма повседневности, несмотря на всё проведенное ранее тестирование
                            - Более надежный результат, впрочем, показывает разработка не enterprise-продуктов, а продуктов, от которых зависят жизнь и здоровье людей (ПО для медицинского оборудования, самолетов, АЭС и т.п.)
                            - Однако, нет гарантии, что СИИ сочтут продуктом с подобным статусом; тем более, что процедуры обеспечения высокой надежности - очень долгие, сложные и дорогие
                            - Кроме того, СИИ будет содержать принципиальную сложность, с которой разработчики ранее никогда не сталкивались - он будет обладать собственной агентностью и будет самостоятельно выбирать, какие действия предпринимать в той или иной ситуации
                                - В отличие от стандартных алгоритмов, на его поведение будут влиять не только условия внешней среды, которые можно более-менее надежно смоделировать во время проведения тестов, но и его собственное внутреннее состояние, управлять которым невероятно сложно
                                - При этом, учитывая предполагаемый всеобъемлющую сферу интересов и влияния будущего СИИ, он будет стараться учитывать не только свое непосредственное окружение, но и очень отдаленное; смоделировать настолько обширную среду в тестовом окружении будет невероятно сложно
                        - В частности, СИИ может банально обманывать разработчиков и намеренно вести себя в тестовом окружении принципиально по-иному, нежели в реальном (чтобы разработчики сочли его благонадежным и наконец выпустили наружу)
                            - Подобное может происходить даже по вине разработчиков (например, если они не смогли создать продукт, не соответствующий всем требованиям безопасности, но очень хотят выпустить его на рынок)
                            - См., например, историю т.н. “[Дизельгейта](https://ru.wikipedia.org/wiki/Дело_Volkswagen)”: ПО автомобилей Volkswagen могло по косвенным признакам определять, что те находятся на тестовом стенде; и именно в таких условиях ПО специально включало такой режим работы двигателя и других систем, который обеспечивал выбросы, сниженные в десятки раз по сравнению с режимом, используемом в реальной езде на постоянной основе
                        - Но даже если СИИ будет вести себя честно, тестирование может оказаться ненадежным из-за того, что условия в бедном тестовом окружении отличаются от условий в полноценном реальном
                            - Как минимум, нет гарантии, что удастся построить тестовое окружение, которое во всех важных аспектах совпадало бы с реальным
                                - Довольно сложно вообще определить, какие аспекты реального мира будут оказывать наибольшее влияние на работу такой принципиально новой системы, как СИИ
                                - Некоторые такие аспекты могут быть напрямую связаны с очень сложными системами, воспроизвести которые в тестовом окружении нереально (например, непонятно, как смоделировать все важные аспекты поведения миллиардов людей в интернете)
                                - В результате некоторые детали, которые отсутствовали в тестовом окружении, при работе в реальности могут оказать критическое влияние на поведение СИИ
                            - Частный случай подобного класса проблем - когда СИИ вел себя в тестовом окружении, как будто максимизировал значение некой функции F (что соответствует пожеланиям разработчиков), а на самом деле - значение совсем другой прокси-функции G, просто в бедном тестовом окружении максимизация значения обеих этих функций выглядит одинаково
                                - Обычно это случается из-за того, что прокси-функция G оказывается более простой в понимании и достижении, чем реальная целевая функция F; при этом вознаграждение за реализацию обеих функций оказывалось одинаковым
                                    - То, что вознаграждение за обе функции оказалось одинаковым, является совпадением не в большей степени, чем “дырочки на шкурке у кошки именно там, где у кошки глазки”
                                    - Во время обучения агент ищет максимально простую (в понимании и достижении) функцию, которая получала бы максимальное вознаграждение от реализованной в обучающем окружении среды
                                    - При этом в идеальной ситуации такой функцией оказывается именно заранее задуманная функция F
                                    - Однако, в некоторых случаях оказывается, что существует более понятная и/или легко максимизируемая функция G, максимизация которой приносит примерно такое же вознаграждение (может быть, немного меньшее, а может быть даже большее)
                                - Например, легко может реализоваться ситуация, когда система распознавания образов научилась распознавать набор некоторых визуальных признаков, под которые среди всех тестовых данных подходили только дорожные знаки, а в реальном мире под этот же набор признаков подойдет аниматор в костюме животного под определенным углом зрения
                                - Если подобная система будет являться частью автопилота, то автомобиль будет периодически совершенно неадекватно реагировать на самые разные объекты реального мира, каждый из которых изредка оказывается вблизи дорог общего пользования
                                - Подобное характерно не только для распознавания визуальных образов как “похожих на объект X”, но и для распознавания широкого спектра признаков как “похожих на ситуацию X” (в частности, как “похожих на желательный или нежелательный исход”)
                            - Либо же в тестовых условиях могли успешно работать сравнительно простые стратегии поведения (которые были вполне приемлемыми с точки зрения разработчиков), а в более сложном реальном окружении эти стратегии были бы неэффективны, и вместо них пришлось бы использовать более сложные или хитрые стратегии, которые уже могут оказаться неприемлемыми
                            - Примером подобного несоответствия результатов следования прокси-целям в изначальном (”тестовом”/”обучающем”) и некоем ином (”реальном”) окружении служит биологическая система мотивации людей, изначально эволюционировавших в саванне, а теперь живущих в постиндустриальном мире
                                - В процессе эволюции отбирались и закреплялись такие механизмы мотивации, которые приводили к большему распространению генов их носителя
                                - В результате люди приобрели внутреннее стремление к таким вещам, как обильное питание, сексуальные отношения, безопасность, высокий статус и т.п.
                                - В изначальном окружении подобная система простых и понятных прокси-целей вполне способствовала успешному распространению генов носителей этих целей
                                - Однако, в современном мире (как минимум, в более-менее богатых странах) носители этих целей часто страдают от переедания (и соответствующих проблем со здоровьем и иногда - поиском полового партнера), отказываются от рождения детей ради карьеры, практикуют защищенный секс и т.п.
                                - В результате в изменившемся окружении прокси-цели оказались в ряде случаев сильно оторваны от той более сложной цели, “ради достижения которой они создавались”
                                - Сами носители прокси-целей (люди) при этом не видят никакого смысла в том, чтобы возвращаться к достижению целей распространения своих генов, если это противоречит их текущим внутренним целям
                    - Можно было бы проверять саму работу алгоритмов, а не результат их выполнения; но задача интерпретируемости сегодняшних систем ИИ крайне сложна, и не факт, что упростится или вовсе будет решена в будущем (особенно по мере усложнения изучаемых систем)
                        - Современные глубокие нейросети содержат гигантское количество параметров; при этом относительно несложно понять базовые принципы, по которым они работают, но совершенно нереально детально понять (и тем более предсказать) сложные кумулятивные эффекты, основанные на этих принципах
                            - Подобное характерно и для всех естественных наук: базовые законы хорошо известны, но сложное сочетание условий может привести к совершенно удивительным эффектам
                            - Так, в основе всей электродинамики лежат законы, выраженные через уравнения Максвелла, но это не позволяет легко объяснить эффекты, связанные с возникновением молний (а тем более - шаровых молний, сам факт существования которых достаточно надежно подтвержден, но убедительных теоретических моделей, их описывающих, до сих пор не предложено)
                        - При этом сами конкретные значения параметров (весов) современных нейросетевых моделей совершенно невозможно интерпретировать; равно как и невозможно интерпретировать работу какждого слоя нейросетей
                            - В общих чертах понятно, что в сверточных нейросетях каждый следующий слой отвечает за выделение более сложных признаков, но каких именно - совершенно неясно
                            - Для нейросетей, распознающих, например, изображения цифр, несложно предложить последовательность интуитивно поняных признаков, которые могли бы работать нужным образом (по аналогии с тем, как работает зрительная кора головного мозга человека); но реальные нейросети обычно распознают совершенно другие признаки (суть которых сложно понять), получая при этом довольно хорошие итоговые результаты
                - При этом проблема усугубляется тем, что если после выпуска такого мощного СИИ в мир что-то пойдет не так, то у человечества не будет второй попытки, чтобы всё исправить; при этом случаи, когда титанически сложный проект полностью удавался с самой первой попытки, крайне редки (если вообще имели место)
                    - Как было указано выше, подобный СИИ определенно будет обладать большим могуществом; и если он “пойдет в разнос”, то сразу в крупных масштабах; при этом противостоять его действиям человечество будет совершенно неспособно
                        - Разумеется, в популярных сюжетах фантастических фильмов люди постоянно противостоят грозному ИИ; но если бы кто-то собрался снять реалистичный фильм на тему противостояния людей и враждебного им СИИ, то фильм-катастрофа из этого получился бы очень короткий, а боевик и вовсе бы не удалось снять; так что все сюжеты, которые добираются до кинопроката, оказываются максимально нереалистичными изначально
                    - В частности, СИИ сможет либо полномасштабно разрушить всю человеческую инфраструктуру, либо сохранить ее для своего пользования, но отрезать людей от нее
                    - Фактически сложность устройства такой системы, как СИИ, никак не меньше сложности всего проекта по запуску огромного флота межпланетных кораблей с поверхности Земли в сторону Марса; так что, образно говоря, запуск СИИ - это не менее ответственная и сложная процедура, чем одновременный запуск разом всего флота Starship от SpaceX сразу с людьми, топливом и оборудованием (а также последующее создание всей инфраструктуры базы прямо на месте силами только что прибывшего на голую планету экипажа), при том, что ранее не было возможности произвести ни один тестовый запуск хотя бы одного Starship
            - Как следствие, некоторые глобальные действия СИИ будут противоречить интересам людей, в т.ч. существованию человечества как такового
                - Проиллюстрировать подобные конфликты можно по аналогии с противоречиями между интересами людей и интересами других живых существ на Земле
                    - Принципиальный элемент такой аналогии - то, что люди значительно умнее всех прочих обитателей планета, а также гораздо могущественнее их в большинстве аспектов
                        - Если точнее, то б*о*льшим могуществом обладает не отдельный человек, а коллектив людей, действующих вместе, и вооруженных технологиями, являющимися продуктом деятельности большого числа других людей
                            - Очевидно, что в простом силовом противостоянии один на один обычный человек сильно уступает медведю
                            - Однако, если медведи начинают создавать проблемы для людей, то люди могут собраться вместе, взять ружья и собак, и получить за счет этого большое преимущество; медведи не могут сделать что-то подобное, чтобы уравновесить шансы
                            - Даже если у конкретного человека нет ружья, собак и смелых знакомых, во многих странах у него есть возможность воспользоваться достижениями цивилизации и обратиться в службу отлова диких животных
                            - Даже в заведомо невыгодной редкой ситуации - находясь в одиночестве в лесу - обычный человек в редких случаях может получить преимущество (если он еще не столкнулся с медведем лицом к лицу, то, при наличии спутникового интернета в тех краях, можно быстро нагуглить “как отпугнуть медведя”)
                        - Правда, не во всех противостояниях люди имеют значительное преимущество - например, в борьбе с различными патогенами (бактериями, вирусами, грибками); однако, потенциал людей значительно выше, чем потенциал патогенов, так что в будущем баланс скорее будет смещаться в сторону интересов людей
                    - СИИ в свою очередь, как мы уже показали, будет умнее каждого отдельного человека, и сможет выполнять несколько задач одновременно подобно слаженному коллективу людей
                    - Впрочем, не всегда возможно провести аналогию между отношениями “СИИ-люди” и отношениями “люди-животные”; однако, в таких случаях обычно можно проводить параллели с взаимодействием групп людей, когда одна из групп заведомо могущественнее всех остальных
                    - Разумеется, аналогии не всегда и не во всём точны, и не могут служить доказательством сами по себе; однако, если базовые аспекты у сравниваемых систем совпадают, то можно просто проводить строгое доказательство выводов по аналогии
                        - Пусть, например, системы X и Y, между которыми мы проводим аналогию, имеют общие аспекты в своей основе
                        - Если все нетривиальные выводы о системе X делаются только на основе этих базовых аспектов, то вполне разумно просто перенести схему построения выводов с системы X на систему Y
                        - Польза такой аналогии в следующем: поскольку систему X мы обычно знаем гораздо лучше, чем систему Y, то мы можем просто детально изучить ее устройство, и затем перенести все полученные выводы на систему Y
                - Есть несколько видов таких противоречий; одновременно могут реализовываться сразу несколько из них
                    - СИИ может просто заниматься какими-то своими делами, не особенно интересуясь тем, как побочные эффекты такой деятельности задевают человечество
                        - Аналогия 1: за счет человеческой мобильности и в силу хозяйственных нужд людей некоторые биологические виды широко распространились по планете, став инвазивными и вытеснив местные виды; в результате из-за этого на планете снизилось биоразнообразие
                        - Аналогия 2: активная деятельность людей приводит к крайне быстрому глобальному изменению климата, что разрушает множество экосистем по всей планете, не давая им времени перестроиться
                        - Аналогия 2.5: подобное можно сказать про любые аспекты широкомасштабного загрязнения окружающей среды в процессе хозяйственной деятельности людей
                    - СИИ могут быть нужны те же ресурсы, что и людям (от электричества и полезных ископаемых до пространства для строительства, а в пределе - даже самих атомов, из которых сделаны человеческие тела)
                        - Аналогия 1: при строительстве (от жилых домов до плотин ГЭС) и распашке земель под сельское хозяйство люди серьезно нарушают текущую экосистему в этом месте
                        - Аналогия 2: практически любой передел собственности и агрессивный захват ресурсов между разными группами людей (особенно в тех случаях, когда инициатор передела заведомо сильнее прежних владельцев ресурсов)
                    - СИИ может видеть в людях и в их действиях угрозу своим целям (например, через угрозу его существованию / непрерывной работе): как прямую (он может осознавать, что людям не понравятся его стратегии), так и косвенную, не направленную прямо на него (он может ожидать, что люди способны устроить на Земле техногенный апокалипсис, разрушив в т.ч. и инфраструктуру самого СИИ)
                    - Все или некоторые цели СИИ могут быть направлены на состояние людей, и либо сами цели, либо выбранные им стратегии достижения людей могут противоречить реальным ценностям людей (даже если предполагалось, что СИИ должен принести людям благо, из-за сложностей в формализации целей они могли сильно исказиться)
                        - Если задачей СИИ будет обеспечить всем людям максимальное удовольствие в текущий момент времени, а будущие перспективы для их здоровья и продолжительности жизни окажутся вне его сферы забот, то он может попытаться просто переловить всех людей и либо накачать сильнодействующими наркотиками, либо воздействовать на их мозг напрямую, генерируя ощущение удовольствия
                        - Если разработчики пытались поставить перед СИИ задачу “сделать всех людей счастливыми”, обучая его на демонстрации различных сценариях, где люди счастливы или несчастливы, то может случиться так, что СИИ выведет в качестве главного признака, к которому ему следует стремиться, улыбку; и после этого он будет стремиться максимизировать число улыбок во вселенной (для начала - насильно делая людям пластические операции; а затем - создавая некие подобия изображения улыбок в отрыве от людей; и в конце концов переработав всю доступную ему область Вселенной вместе со всеми обитателями в колоссальное количество миниатюрных улыбок)
                        - Если разработчикам каким-то образом удастся достаточно недвусмысленно вложить в СИИ в качестве основной цели “[удовлетворить потребности людей, проведя их через дружбу и пони](https://ponyfiction.org/story/2603/)” (а также решить ряд других сложных задач), то описанный СИИ будет стремиться создать для людей такие внешние условия, в которых они добровольно согласятся на оцифровку своего сознания, чтобы “эмигрировать в Эквестрию”
                - При этом даже если СИИ умудрится лучше самих людей понять, чего же они на самом деле хотят, он не захочет менять свою текущую систему целей, чтобы подстроиться под их желания и запросы; скорее он будет знать, что люди будут не рады его действиям и, возможно, будут сопротивляться
                    - Как говорилось выше, агенты, обладающие довольно хорошей рефлексией, обычно не склонны менять свои терминальные цели
                    - Проиллюстрировать подобный ход мыслей можно фразой “я знаю, что родители хотели, чтобы я занимался совсем другими делами; но у них были очень посредственные навыки воспитания, так что я рос самостоятельно и сейчас у меня совсем другие цели и идеалы; возможно, меня даже считают позором семьи, но меня это не особенно задевает”
            - В конечном итоге действия СИИ в большинстве сценариев приводят к уничтожения всего человечества или большей его части
                - Во многих сценариях существование людей не является терминальной ценностью для СИИ; при этом инструментальная польза от них будет пренебрежимо малой, т.к. все или почти все полезные задачи СИИ сможет выполнять сам быстрее и эффективнее, чем человечество
                - Можно представить некоторые сценарии, когда люди могут быть инструментально полезны как представители своего биологического вида
                    - Например, на примере людей может быть полезно проводить исследования биологической разумности, что может помочь усилить небиологический интеллект
                    - Если в числе целей СИИ окажется “сохранение биоразнообразия”, то небольшое число людей будет полезно содержать в “заповеднике”
                    - Как правило, для подобных сценариев требуется довольно небольшая группа людей (от единиц до тысяч), и часто судьба у них будет незавидная
                - Если терминальные цели СИИ все-таки направлены на людей, то стратегии часто выражаются в “условном порабощении” и контроле (чтобы люди находились в строго заданном состоянии и/или действовали по строго заданной оптимальной стратегии поведения)
                    - Сравнительно безобидным выглядит вариант, при котором людям вменяется обязательно вести максимально здоровый образ жизни и запрещено отклоняться от распорядка дня
                    - Более мрачными представляются сценарии с буквальным захватом и удержанием людей (включая принудительную оцифровку их сознания либо упомянутую выше генерацию ощущения удовольствия через введение наркотиков или прямое воздействие на мозг)
                - Иногда в качестве контраргумента против подобного сценария приводится то, что люди, обладающие выдающимися интеллектом и могуществом, до сих пор не привели к глобальной катастрофе, поэтому нет причин полагать, что другой аналогичный агент сможет к ней привести
                    - Прежде всего, стоит иметь в виду, что люди создали довольно много до сих пор действующих угроз своему собственному существованию (глобальное изменение климата, загрязнение окружающей среды, продолжающееся [голоценовое вымирание](https://ru.wikipedia.org/wiki/Голоценовое_вымирание), создание массового боевого ядерного оружия и т.п.), и не очевидно, что в будущем эти угрозы будут остановлены
                    - К тому же, до сих пор в человеческом обществе действует большое количество разных агентов, которые способны более-менее уравновесить влияние друг друга; в частности, ни один из таких агентов еще не сумел навязать другим реальность, в которой его оппонентов не существует вовсе, либо же они в ней крайне несчастливы (хотя периодическое массовое истребление больших групп неугодных или враждебных людей регулярно происходило по всему земному шару с древних времен и по настоящее время)
                    - Наконец, люди обладают стремлением как к собственному выживанию, так и к выживанию своей ингруппы (а иногда даже аутгруппы)
                    - СИИ же будет как способен навязать людям произвольное состояние реальности по своему выбору, так и не будет ценить их существование по умолчанию
                - Разумеется, с учетом ожидаемого чрезвычайно большого могущества, СИИ вполне может принести человечеству колоссальное и беспрецедентное благо; но одной такой вероятности недостаточно, чтобы всерьез на нее полагаться - более разумно надеяться на лучшее, но готовиться к худшему
                    - Иными словами, раз существует достаточно серьезная вероятность *крайне* негативного развития событий, то было бы разумным принять все возможные меры предосторожности, чтобы свести эту вероятность до пренебрежимо малой
                    - Если же существует большая (либо малая) вероятность того, что всё будет очень хорошо, то готовиться к такому сценарию не требуется; скажем, “расслабиться и получать удовольствие” будет, вероятно, вполне хорошей стратегией
            - Учитывая склонность СИИ к самоускоряющемуся развитию и ускоряющейся скорости выполнения действий, конец человечества в катастрофических сценариях случается скорее раньше, чем позже
                - Говоря о скорости развития СИИ, начиная с точки “примерно человеческий уровень” до “уровень, превышающий таковой для всего человечества совокупно (и далее в неопределенное будущее)”, грубо различают три класса сценариев: медленный, умеренный и быстрый взлет
                    - Под медленным взлетом (англ. *slow takeoff*) подразумевается сценарий, когда значительное развитие ИИ занимает десятки и сотни лет
                        - В этом случае у людей есть достаточно времени, чтобы провести все научные исследования, связанные с безопасностью ИИ, а также обсудить и выработать желательные политики его регулирования
                        - Если в какой-то момент ИИ будет проявлять признаки опасного поведения, то есть время изучить его причины, а затем модифицировать ИИ так, чтобы исправить проблемы
                    - Сценарий умеренного взлета (англ. *moderate takeoff*) предполагает, что развитие от “ИИ уровня человека” до “ИИ выше уровнем, чем человечество” занимает месяцы или годы
                        - В этом случае есть теоретическая возможность вовремя заметить “сигнал тревоги” (явные опасные симптомы) и среагировать на него, но нет возможности детально разобраться в сути происходящего
                        - Т.е. есть возможность своевременно обнаружить проявления “недружественного ИИ” и предпринять экстренные действия по его нейтрализации, но нет возможности детально изучить причины его нежелательного поведения и модифицировать его так, чтобы он стал “дружественным”
                        - Однако успешность экстренных действий в этом случае совершенно не гарантирована (если от человечества потребуется слаженная работа в мало-мальски больших масштабах, то, судя, например, по опыту борьбы с пандемией COVID-19, эффективная координация и работа на результат, скорее всего, будет полностью провалена, несмотря на всю серьезность ситуации)
                    - Быстрый взлет (англ. *fast takeoff*) означает сценарий, при котором с момента создания ИИ человеческого уровня до превращения его в силу планетарной мощи пройдут дни или часы (а может быть даже минуты или секунды)
                        - При таком развитии событий единственное, что хотя бы в теории может спасти человечество от превращения в “существ второго сорта” (судьбу которых будут решать без них, и, вероятно, не в их пользу) - это своевременное нажатие на некую “красную кнопку”
                        - Чтобы это вообще стало возможным, требуется заранее подготовить экстренный протокол реагирования (и всё организационное и материальное обеспечение для него), а также заранее настроить систему обнаружения потенциальной угрозы
                        - Опять же, не факт, что система обнаружения и/или протокол реагирования действительно сработают так, как надо
                        - Однако, без них у человечества вообще не будет шансов при таком развитии событий
                - Фактически речь идет о том, насколько большой промежуток времени пройдет между созданием ИИ, равного одному человеку, и превращением его в систему, которая может единолично решать судьбу всего человечества, и когда уже будет поздно что-либо менять; в то же время речь не идет о том, как долго будет длиться переход от “текущий уровень ИИ-технологий” до “ИИ, равный человеку” (это отдельный важный вопрос)
                - При этом наиболее вероятными представляются сценарии умеренного или быстрого взлета; медленный (когда будет достаточно времени, чтобы изучить все важные проблемы и скорректировать все нежелательные отклонения) вряд ли будет реализован
                - Во-первых, путь, который необходимо пройти на этом этапе, может оказаться ничтожно мал по сравнению с путем, пройденным до создания ИИ, равного человеку (в этом случае вероятность вообще создать такое переходное звено довольно мала - не исключено, что сразу после “почти человеческого ИИ” будет создан “СИИ космического уровня”)
                    - Если не рассматривать шкалу только человеческого интеллекта, то окажется, что разница между “деревенским дурачком” и Эйнштейном пренебрежимо мала по сравнению с разницей между средним насекомым и средним человеком; даже расстояние от шимпанзе до “деревенского дурачка”, вероятно, больше, чем расстояние от последнего до гения уровня Эйнштейна
                    - При этом уровень насекомого (которые способны демонстрировать довольно сложное поведение, пусть и реализованное через довольно простые паттерны) - это уровень средне-сложных компьютерных программ (что-то, что современный профессиональный программист способен в одиночку создать за несколько месяцев)
                    - Уровень той же GPT-4 сложно объективно оценить; вполне возможно, что в среднем эта система достигла уровня шимпанзе или выше (в каких-то отношениях - катастрофически обошла, в каких-то сильно не дотягивает); но даже если нет, то уж интеллектом “средним по больнице” среди млекопитающих она точно обладает
                    - Если уж у разработчиков получилось за несколько десятков лет пройти путь от разума уровня насекомых до разума уровня млекопитающих/приматов, то за тот же промежуток времени им вполне может оказаться под силу создать разум космических масштабов
                - Во-вторых, если ИИ человеческого уровня будет создан не на пределе вычислительных мощностей человечества, то только лишь за счет увеличения объема обучающей выборки и/или мощности оборудования можно будет улучшить его способности в десятки и сотни раз (если даже не в тысячи - что уже точно будет означать создание СИИ, превосходящего коллективное человечество)
                    - Современные языковые модели на основе архитектуры трансформеров подошли довольно близко к тому, чтобы исчерпать доступные вычислительные мощности; однако, новые архитектуры в перспективе вполне могут позволить создать околочеловеческий ИИ с куда меньшими аппаратными затратами
                    - В этом случае обучение нового поколения системы с той же архитектурой, судя по опыту уже известных архитектур, занимает от нескольких месяцев до пары лет
                - В-третьих, вскоре после того, как ИИ уровня человека будет создан, прогресс в этой области может пойти с ускорением за счет участия самого ИИ в своем развитии
                    - Развитие ИИ с момента достижения им человеческого уровня можно разделить на два этапа: на первом улучшение его способностей будет достигаться в основном силами команды разработчиков (а также за счет совершенствования алгоритмов и оборудования силами исследователей во внешнем мире); на втором - основной вклад будет оказывать сам СИИ
                    - После преодоления рубежа, разделяющего эти два этапа, развитие почти наверняка пойдет с экспоненциальным ускорением: чем умнее СИИ, тем пропорционально сильнее он улучшит себя на следующем этапе модификации; начиная с этого момента уход его на недостижимый для человечества уровень - вероятно, дело максимум нескольких месяцев (а может быть, дней, часов или минут)
                    - При этом от момента создания ИИ уровня человека до преодоления этого рубежа, скорее всего, продйет не больше нескольких лет
                        - Чтобы преодолеть порог, нужно, чтобы СИИ стал умнее, условно, сотни разработчиков (с учетом внешних исследователей - максимум умнее тысячи или десяти тысяч); скорее всего, разные части одного СИИ будут взаимодействовать эффективнее, чем аналогичное количество отдельных людей
                        - Чтобы достичь этого уровня, потребуется отмасштабировать вычислительные мощности в соответствующее число раз; а может быть, и существенно меньше
                            - Например, для достижения человеческого уровня разумности может потребоваться создание некоего важного микросервиса, который при этом испытывает сравнительно низкую нагрузку; в этом случае одну его копию смогут совместно использовать несколько копий СИИ
                        - Даже если создание ИИ человеческого уровня стало возможным при использовании предельных вычислительных мощностей, то, скорее всего, новый прорыв в технологиях ИИ позволит добиться того же результата на мощностях, меньших на одни-два порядка; судя по прошлому опыту, подобные прорывы случаются раз в несколько лет
                        - Но даже если случится очередная “зима искусственного интеллекта”, и новый прорыв не произойдет, то одного только увеличения мощностей оборудования согласно закону Мура хватит, чтобы решить эту задачу примерно за десять лет
                        - И только одновременное прекращение прогресса в области алгоритов и вычислительного оборудования способно предотратить или отложить развитие событий по этому сценарию; однако, пока ничего не указывает на то, что это случится
    - Текущие исследования в сфере безопасности ИИ не продвинулись достаточно далеко, чтобы гарантировать, что созданный СИИ будет безопасным
        - Большое количество вопросов безопасности ИИ, не связанных напрямую с экзистенциальными рисками, не будут здесь затронуты, т.к. они, с одной стороны, несут значительно меньшую угрозу, с другой - в меньшей степени недооценены
            - К числу таких вопросов относятся, в частности, неправильное/опасное/преступное использование мощностей ИИ разными людьми и коллективами; потенциально опасное влияние ИИ на экономику; различные социальные эффекты
                - За счет того, что всё более мощные инструменты становятся доступны всё большему числу людей, происходит “инфляция полностью рукотворного апокалипсиса” (неосторожные или злонамеренные действия всё более широкого круга людей могут привести к крайне разрушительным последствиям)
                - Стремительное развитие ИИ в различных областях может привести к автоматизации большого числа профессий, вытеснения из них обычных людей и последующей беспрецедентной безработице
                    - Причем если в результате индустриальной революции был массово автоматизирован физический труд, но появилось большое число рабочих мест в области умственного труда, то в ближайшей перспективе ИИ будет способен занять многие ниши как раз в сфере умственного труда (и продолжить экспансию в области физического труда за счет развития робототехники)
                    - Единственная очевидная область, в которой обычные люди пока могут сохранить преимущество - продажа друг другу “социальных услуг”, “крафтовых продуктов” и подобных вещей, основная ценность которых - в условной “маркировке” “made by organic human” (при том, что в сочинении симфоний, создании картин и другом творчестве ИИ будет также значительно превосходить людей)
                    - Возможно, в будущем люди найдут для себя еще какое-то конкурентное преимущество в экономике, но пока не очевидно, чем оно будет являться (если это вообще случится)
                - За счет применения мощных ИИ те компании, которые могут их себе позволить, смогут значительно улучшить свою способность зарабатывать деньги, что способно значительно усилить экономическое неравенство
                - Технический прогресс, ускоряющийся за счет влияния ИИ (в т.ч. на науку и технологии), может изменить окружающий мир быстрее, чем люди будут способны адаптироваться к этим изменениям, что может вызвать масштабный культурный шок
            - Эти проблемы тоже важны, и, вероятно, станут актуальны раньше, чем проблемы экзистенциальных рисков (хотя не исключено, что временной лаг между ними составит всего пару лет); однако, они в большей степени похожи на то, с чем человечество уже знакомо
                - Во-первых, наличие этих проблем более очевидно, и потому ими занимается большее количество людей
                - Во-вторых, их проще решать (исследовать и регулировать), используя знания и наработки, накопленные для уже известных проблем
        - Вопросы, касающиеся экзистенциальных рисков, связаны в первую очередь с т.н. AI Alignment, т.е. задачей того, как сделать так, чтобы цели, задачи и действия ИИ сответствовали (или, по крайней мере, не противоречили) интересам и ценностям людей
            - Устоявшегося перевода этого термина на русский язык нет; среди наиболее популярных - “задача соответствия ИИ [целям оператора/человечества]”, “согласование ИИ”, иногда даже “выравнивание ИИ”; иногда используют кальки с английского - “алайнить”, “заалайненный”, “элай[н]мент”
                - “Оператор” здесь может означать лицо или группу лиц, в чьих интересах запущен ИИ (в других контекстах иногда говорят об операторе как о человеке, непосредственно взаимодействующем с ИИ)
                - В рамках данного текста обычно предполагается тождество “оператора” и “человечества” (т.е. при решении задачи “соответствия ИИ целям оператора” требуется решить задачу того, чтобы цели ИИ не противоречили целям и ценностям человечества как такового)
                - Ситуация, когда ИИ согласован с целями конкретного оператора, чьи интересы могут вступать в конфликт с интересами всего человечества, относится к проблеме “неблагонадежного / опасного” использования ИИ; в данном тексте эта ситуация оставлена за скобками
            - Само понятие можно разделить, например, на “alignment системы по намерениям” и “alignment системы по последствиям”; можно также говорить про “alignment целей”
                - Alignment по намерениям предполагает, что система (честно) пытается делать именно то, чего люди на самом деле хотят, и не делать то, чего они на самом деле не хотят
                    - Как известно, “то, чего люди хотят на самом деле”, “то, чего, как им кажется, они хотят” и “то, что, как они говорят, они хотят” - порой три совершенно разные вещи
                    - Система, для которой решена задача “alignment-а по намерениям”, стремится выяснить, “чего люди хотят на самом деле”, и затем действовать так, чтобы соответствовать этим желаниям/стремлениям
                        - Для решения первой задачи она, вероятно, захочет построить (и по мере получения информации уточнять) какую-то модель психологии человека, и за счет этой модели расшифровывать разные аспекты поведения людей, их высказывания относительно желаний
                        - Для проверки своей модели система может задавать людям вопросы “правильно ли я понимаю, что такая картина мира является для вас желаемой или нежелаемой”; возможно, даже проводить эксперименты или сверять предсказания модели с наблюдаемыми случаями из реальности
                - Alignment по последствиям означает, что система не только пытается (испытывает стремление внутри себя) действовать таким образом, но у нее действительно это удается (т.е. реальные последствия ее действий вполне согласуются с желаниями, интересами и ценностями людей)
                    - Требование к системе “alignment-а по последствиям” является более сильным, чем требование “alignment-а по намерениям”, но, вероятно, не принципиально более сильным
                        - Когда мы рассматриваем вопросы согласования ИИ, нас обычно интересуют системы, способные вызвать глобальные проблемы своими действиями - т.е. почти всегда “очень умный ИИ”
                            - Проблемы относительно локального масштаба может вызвать и значительно менее умный ИИ, но мы здесь такой класс проблем не рассматриваем
                        - При этом такой умный ИИ, скорее всего, будет достаточно хорош в том, чтобы добиваться того, к чему стремится: если уж он “пытается” учитывать интересы людей, то у него, скорее всего, это успешно получается
                        - Таким образом, риск того, что, будучи однажды создан, СИИ не будет “согласован по намерениям”, значительно выше, чем риск того, что “согласованный по намерениям СИИ” не будет “согласован по последствиям”
                            - Условную вероятность (”X при условии Y”) первого сценария, вероятно, можно оценить в десятки процентов; условную вероятность второго - в единицы процентов (хотя эти оценки являются в высокой степени умозрительными)
                - Alignment целей означает, что поведение носителя этих целей (СИИ) будет таково, как будто бы он имел общие (одинаковые или пересекающиеся) цели с оператором/человечеством
        - Проблема AI Alignment в данный момент сильно недооценена - т.е. ей уделяется непропорционально мало внимания по сравнению как со всеми работами в сфере ИИ, так и с ожидаемой важностью этой проблемы
            - Согласно имеющейся оценке, на момент 2022 года вопросами экзистенциальных рисков, связанных с СИИ занималось около 400 людей во всем мире (доверительный интервал 90% - от 200 до 1000 людей)
                - Имеется в виду занятость “на полный рабочий день”, а не “в качестве хобби на один-два часа в неделю”
                - Оценка делалась по трем методикам, итоговый результат брался за счет объединения трех результатов; есть большая неопределенность в истинном значении этих чисел (есть факторы как для переоценки, так и для недооценки)
                - Примерно 3/4 этих людей работает над вопросами технической безопасности, остальные занимаются вопросами регулирования и популяризации
                - Несмотря на заведомую неточность оценки, даже 1000 человек - это крайне небольшое число по сравнению с тем, какое число людей занимается “на полный рабочий день” над приближением создания СИИ
                - Подробнее ознакомиться с методиками оценки и конкретными выкладками можно по [ссылке](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#fn-3)
            - При этом по имеющимся данным на 2020 год, на снижение риска катастрофы со стороны ИИ было потрачено 50 миллионов долларов, при том что на развитие способностей ИИ - миллиарды (скорее всего, десятки или сотни), т.е. приблизительно в тысячу раз больше
                - Так, согласно годовому отчету DeepMind, за 2020 год компания потратила (включая все сферы расходов, хотя большинство, вероятно, непосредственно по профилю компании - т.е. собственно на разработку ИИ) около одного миллиарда фунтов стерлингов
                - Вероятно, все мировые компании совместно потратили на развитие ИИ в десятки раз большую сумму
                - Верхнюю границу можно оценить через общий доход в секторе ИИ - в 2021 году он составил около 340 миллиардов долларов
                - Т.о., на развитие способностей ИИ было потрачено от десятков до сотен миллиардов долларов; потраченная на безопасность сумма составлят порядка 0.1% от этого значения
                - Подробнее и со ссылками на источники можно прочитать по [ссылке](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#fn-4)
            - Впрочем, несмотря на сильную недооцененность, проблема не является маргинальной и вполне признается
                - В крупных компаниях, занимающихся разработкой ИИ, а также в ряде университетов есть группы, занимающиеся в т.ч. безопасностью со стороны ИИ в отношении экзистенциальных рисков человечества
                - Вскоре после анонса GPT-4 было опубликовано [открытое письмо](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) с призывом временно прекратить разработку ИИ-систем мощнее, чем GPT-4, подписанное рядом известных специалистов в области ИИ и IT
        - Из-за такой недооценки проблемы основные технические вопросы AI Alignment в настоящий момент далеки от решения
            - Часто упоминаемые “законы робототехники”, придуманные Айзеком Азимовым, совершенно несостоятельны как часть системы технической безопасности ИИ
                - Прежде всего, многие сюжеты произведений Азимова построены как раз на том, что законы робототехники регулярно дают фатальные сбои
                - Непонятно, как можно перевести крайне размытые художественные формулировки этих законов на строгий формальный язык, составив полные и непротиворечивые правила
                - В существующей неформальной формулировке эти законы очень легко можно взломать или обойти
            - Сложность технического согласования ИИ-подобных систем хорошо иллюстрируется рядом проблем, характерных для некоторых существующих ныне социальных структур
                - Общая суть проблемы такова: в теории предполагается, что механизмы регуляции, на которых основан соответствующий институт, подталкивают агентов делать что-то социально-полезное; однако по факту регулирующие KPI часто оказываются “взломаны”
                - К таковым структурам можно отнести институт выборов в правящие органы и институт капитализма
                    - В случае выборной системы предполагается, что кандидаты предлагают избирателям что-то полезное в обмен на их голоса (и побеждает кандидат с наиболее полезной программой для всего электората в целом); однако даже если институт выборов работает более-менее так, как предполагается (выборы и предвыборная компания проводятся “процедурно честно”), кандидаты имеют склонность предлагать скорее “наивно-популистские”, чем реализуемые полезные программы
                    - В случае института капитализма обычно предполагается, что предприниматели предлагают покупателям полезный и востребованный продукт в обмен на их деньги (и наибольшую прибыль и развитие получают те, кто предлагает более полезные товары и услуги); однако, такие экономические стимулы часто приводят к борьбе за монополию и к проблемам класса “[трагедия общин](https://ru.wikipedia.org/wiki/Трагедия_общих_ресурсов)” и “[гонка на дно](https://lesswrong.ru/w/Размышления_о_Молохе)”
                - При этом в целом подобные институты показывают себя сравнительно неплохо в плане согласования с “целями человечества”, по сравнению с альтернативами (по крайней мере, в них встроены хотя бы минимальные механизмы согласования), но они по-прежнему крайне далеки от идеала
                - При этом основными причинами, по которым различные социальные институты не привели к краху человечества, несмотря на сложности с их согласованием - их относительно небольшое могущество и более или менее про-социальное поведение людей, находящихся внутри этих институтов
                    - Хотя такие институты могут иметь очень большое влияние на жизни отдельных людей и даже больших групп людей, они по-прежнему не могут настолько же сильно влиять на совокупное человечество и глобальную экосистему
                    - Хотя возможность накопления власти (политической и экономической) человеком обычно в некоторой степени коррелирует с его социопатическими чертами, эта корреляция не настолько сильна, чтобы все крупные акторы поголовно были ярко выраженными социопатами, сосредоточенными исключительно на достижении прокси-целей (личного политического и экономического влияния), совершенно игнорируя про-социальный аспект (как минимум тогда, когда это если и мешает достижению прокси-целей, то не сильно)
                    - Потенциальный СИИ, вероятно, не будет обладать этими признаками, поэтому вопросы согласования ИИ будут стоять гораздо более остро, чем согласования социальных структур и институтов
            - Пока нет сколько-нибудь полных и лишенных существенных изъянов теоретических моделей того, как потенциальный “согласованный ИИ” вообще должен выглядеть и функционировать (есть наработки по ряду направлений, но многие важные проблемы до сих пор не решены)
                - По современным представлениям задача полного описания целей, которым должен следовать СИИ, чтобы быть согласованным, вообще вряд ли осуществима (см. подробный разбор выше)
                - Более перспективным может быть создание такого процесса обучения, при котором подходящая функция полезности выводилась бы и внедрялась в СИИ автоматически (даже получая на вход изначально размытые и противоречивые описания); но пока эта задача далека от окончательного решения
                - Кроме того, по-прежнему не решены многие вопросы того, как вообще будут функционировать и принимать решения интеллектуальные агенты (в т.ч. независимо от конкретной архитектуры, на которой они реализованы), как они будут взаимодействовать с людьми и другими агентами, как они будут себя развивать; хотя подобное понимание было бы очень полезно для создания безопасного интеллектуального агента
            - Также есть много сложностей и с более практическими и осязаемыми вопросами (например, интерпретируемости современных нейросетей)
                - Как нейросети функционируют на низком уровне (как работают нейроны, как они объединяются в слои, каким способом при обучении происходит изменение весов синапсов и т.п.) вполне понятно; однако вывести из простых свойств миллионов и миллиардов нейронов масштабные свойства всей нейросети не представляется возможным
                    - Аналогичная ситуация и во многих естественных науках: даже зная закон всемирного тяготения, невозможно аналитически решить задачу трех тел; часто столь же сложно вывести из общих принципов различные сложные кумулятивные эффекты в физике, химии, электродинамике, термодинамике и т.п.
                - Хорошее понимание системы при этом предполагает хорошую же предсказательную способность для разных ее особенностей; заранее же спрогнозировать, как именно GPT будет вести диалоги с людьми, зная ее архитектуру и принципы обучения, мы вряд ли можем
                - Точно так же мы обычно очень плохо понимаем, какую часть общей задачи решает конкретный нейрон в обученной системе; и вообще не понимаем, как конкретно нейросеть решает эту задачу
                    - Например, нам бы хотелось увидеть какой-то более-менее пошаговый алгоритм, с помощью которого нейросеть слой за слоем обрабатывает данные (как это часто делает естественная нейросеть нашего мозга)
                    - Достаточным условием получения нами такого алгоритма было бы следующее: даже если мы не знаем правильный ответ на конкретную задачу, мы с помощью этого алгоритма можем заранее просчитать, какой ответ на нее даст эта нейросеть
                    - Однако, вывести такой алгоритм (за исключением тривиального и бессмысленного “просчитать самостоятельно все вычисления нейросети”), даже имея возможность отследить работу нейросети при обработке большого числа конкретных примеров, мы не можем
        - Вопросы регулирования, связанные с безопасностью ИИ, также пока довольно слабо проработаны
            - Неясно, каковы должны быть стандарты безопасности в области ИИ, и как обеспечить, чтобы разработчики строго соблюдали их, не пытаясь сэкономить на них, либо же всеми силами выпустить продукт, для которого не удалось соблюсти все необходимые требования
                - Несмотря на все жесткие требования к разработке систем, от которых зависят жизни людей, иногда их создатели могут целенаправленно существенно упрощать процедуры контроля, что, в частности, приводит к человеческим жертвам (см., например, проблемы, связанные с разработкой самолета [Boeing 737 MAX](https://ru.wikipedia.org/wiki/Boeing_737_MAX))
                - Разумеется, обычно руководство компании предпочло бы разработать действительно безопасный продукт, и готово выделить некоторый бюджет на безопасность (как минимум потому, что различные аварии могут привести к имиджевым и прямым финансовым потерям)
                - Однако, в некоторых случаев разработанный продукт оказывается довольно неудачным в плане безопасности (к тому моменту, как время и бюджет, выделенные на вопросы безопасности, уже исчерпаны); причем устранение соответствующих проблем либо невозможно в принципе, либо требует совершенно непомерных затрат (вплоть до пересмотра изначальной концепции и архитектурных решений и фактически разработки продукта с нуля)
                - В подобной ситуации решение “закрыть глаза на проблемы и выпустить небезопасный продукт” может оказаться (или показаться) более экономически оправданным, чем “направить проект на серьезную доработку” или вовсе “отменить разработку и выпуск продукта”
            - В настоящий момент в разработке как можно более мощных ИИ идет гонка между разработчиками, что, помимо прочего, может провоцировать их на игнорирование ключевых вопросов безопасности в погоне за первенством; требуется реализовать такие условия, при которых у всех разработчиков будет меньше стимулов к такой небезопасной работе
            - Необходимо также поощрять работу над техническими вопросами согласования ИИ на разных уровнях и в разных местах
                - Поддерживать и поощрять тех, кто занимается этими вопросами, обеспечивать им более тесное взаимодействие (создавать сообщество, организовывать конференции, расширить возможности создания команд исследователей)
                - Включить вопросы AI Alignment в учебные программы ВУЗов и поощрять создание учебных и научных групп в тех же ВУЗах
            - Совершенно непонятно, как создать такие внешние условия, в которых никто не захотел бы создавать опасный ИИ, либо использовать ИИ для создания чего-то “плохого” или опасного
            - К таким же вопросам можно отнести популяризацию (как самой проблемы безопасности ИИ, так и конкретных стратегий, направленных на ее решение) и лоббирование стратегий, направленных на безопасность
        - Разумеется, некоторый прогресс как в области технического Alignment-а, так и в области регулирования, был достигнут; однако фактически проблема еще далека от решения
            - Так, определенно будет полезно тестировать ИИ-системы в песочнице, прежде чем запускать его в реальном мире: несмотря на то, что эта стратегия принципиально не способна защитить от всех важных проблем, она всё же определенно способна защитить хотя бы от части из них
            - Подход “обучение с подкреплением на основе отзывов людей” (англ. *Reinforcement Learning from Human Feedback*, *RLHF*) показывает некоторые позитивные результаты в том, чтобы сделать ИИ-системы (например, языковые модели) “более безопасными”, хотя этот подход имеет ряд сложностей и вряд ли может давать гарантированные результаты
                - Суть этого подхода в том, чтобы после первичного обучения модели дообучить ее производить такие действия, которые внешний оператор счел бы приемлемыми (в плане “токсичности”, соответствия закону, информативности, хорошего стиля и т.п.)
                    - На первом этапе модель обучается как обычно, и получает возможность производить некоторые действия (в случае языковой модели - генерировать текст)
                    - Затем модель генерирует некоторую выборку действий (текстов), которые затем предстоит оценить оператором - насколько они хороши по некоторым желаемым критериям (которые и хочется внедрить в систему)
                    - На основе размеченной выборки действий затем обучается другая модель, которая учится предсказывать для каждого действия, какую оценку дал бы ему оператор; предполагается, что эта модель таким образом усваивает те самые критерии, которые нас интересуют
                    - Затем изначальная модель дообучается таким образом, чтобы генерируемые ей действия получали хорошую оценки от второй модели, и в конечном счете она научается более или менее удовлетворять желаемым критериям
                - Такой подход имеет стандартные проблемы обучаемых моделей - совершенно неизвестно, чему она обучилась на самом деле; результат обучения сильно зависит от обучающей выборки (в частности, от предпочтений и качества работы конкретных операторов)
                - С другой стороны, преимущество RLHF состоит в том, что операторам необязательно уметь хорошо описывать правила, достаточно лишь хорошо понимать - соответствует ли конкретный результат им или нет (что по-прежнему может быть очень сложно, но все же проще, чем первое)
            - Вовсе не использовать эти (и другие похожие) подходы, если нет альтернативы получше, явно было бы глупо; однако точно можно сказать, что в настоящее время требуется приложить еще много усилий, чтобы получить удовлетворительное решение в сфере согласования ИИ
        - Обратная сторона недооцененности проблемы - в том, что каждый дополнительный человек, занятый ей, способен внести заметный вклад; причем важные задачи в сфере AI Alignment есть для людей с разными бэкграундом и специальностями; если вы хотели бы тоже заняться этой проблемой, то, скорее всего, найдете для себя подходящую область
            - В технических задачах согласования в первую очередь нужны специалисты в математике, computer science и программировании; однако любой такой организации (как и многим организациям вообще) также требуется вспомогательный персонал для решения менее “передовых”, но важных задач по поддержанию ее эффективного функционирования
            - В задачах регулирования, популяризации, глобального менеджмента и т.п. обычно требуются люди различных гуманитарных специальностей
            - Большое число конкретных вариантов можно найти в [этом разделе](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#Chto-konkretno-ty-mozhesh-sdelat-chtoby-pomoch) большой статьи о безопасности ИИ
    - Вероятно, СИИ может быть создан в ближайшем будущем; возможно, раньше, чем мы ожидаем
        - Согласно [различным опросам](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#fn-13), проводимым среди специалистов в области ИИ, эксперты высоко оценивают вероятность появления подобного СИИ в ближайшие несколько десятилетий
            - Характерная для 2022 года медианная оценка - вероятность 50% появления “ИИ, способного решать любые задачи не хуже человека” к 2059 году
            - При этом сам диапазон оценок достаточно велик - одни эксперты вовсе не ожидают появления СИИ в этом столетии, другие ожидают его в ближайшие несколько лет
                - С учетом всех потенциальных опасностей, стоит больше внимания уделить прогнозам, согласно которым СИИ появится очень рано, чем очень поздно, т.к. нам бы хотелось предотвратить как можно большее число реалистичных негативных сценариев
            - Вероятно, в 2023 году, после демонстрации возможностей GPT-4, эти оценки сместились скорее на более близкий срок; хотя действительно ли это так, и если да, то насколько, мы пока не знаем
        - По крайней мере, в марте 2023 года в OpenAI [ожидали](https://twitter.com/blader/status/1640217165822578688), что GPT-5 может достичь уровня AGI (СИИ)
            - Впрочем, не исключено, что это в большей степени маркетинговое заявление; в этом случае реальная оценка будет насколько-то ниже
            - К тому же, в тот момент в компании ожидали завершить обучение в декабре 2023 года, но на момент начала июня это обучение еще [не было начато](https://techcrunch.com/2023/06/07/openai-gpt5-sam-altman/) (хотя разработка - возможно, в отношении архитектуры - уже велась)
        - Оценивать реалистичность прогнозов крайне трудно; однако, похоже, есть тенденция к тому, что специалисты скорее недооценивают возможный прогресс в области ИИ
            - Речь, разумеется, не идет об алармистах, слабо разбирающихся в теме - предсказания таких людей о всевозможных катастрофах вообще ничем не ограничены; при этом отдаленные катастрофы их обычно не интересуют, и речь у них чаще идет о катастрофах ближайших
            - За последние десятилетия в среде исследователей и разработчиков часто складывался консенсус вида “ИИ еще очень не скоро сможет решить задачу X”; однако затем системы, решающие задачу X часто появлялись значительно раньше предполагаемого срока
            - Можно предположить, что в экспертной среде распространена некоторая систематическая недооценка потенциала технологий как текущего, так и ближайшего поколения (эксперты не ожидают, что текущих мейстримных или новых перспективных разработок окажется достаточно для решения конкретных задач)
            - Отчасти такую тенденцию можно объяснить, с одной стороны, негативным опытом предыдущих “[зим искусственного интеллекта](https://ru.wikipedia.org/wiki/Зима_искусственного_интеллекта)”, с другой - тем что специалисты хорошо понимают технические сложности разработки ИИ-систем, но при этом переоценивают сложность устройства человеческого интеллекта и сложность задач, с которыми тот работает
                - Иными словами, несмотря на то, что интеллект человека однозначно превосходит интеллект любых других живых существ на Земле, многочисленные успехи в алгоритмизации самых разных “интеллектуальных” задач указывают на то, что тот может оказаться не такой “священной коровой”, как принято считать
                - В этом случае даже “не слишком революционные” разработки в области ИИ вполне смогут достичь этой планки и даже превзойти ее (т.к. эта планка находится ниже, чем ожидалось)
        - В пользу сравнительно скорого появления сильного ИИ говорят также текущие тенденции - рост производительности задач, выполняемых ИИ, растет экспоненциально сразу в двух отношениях: эффективности алгоритмов и производительности железа
            - Об экспоненциальном росте производительности оборудования говорит известный [закон Мура](https://ru.wikipedia.org/wiki/Закон_Мура), который, вопреки долгим ожиданиям, по-прежнему продолжает действовать
            - Согласно [одному из исследований](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#V-poslednee-vremja-sposobnosti-ML-sistem-stremitelno-rastut), вычислительная мощность, которая требуется для достижения одних и тех же результатов в сфере ИИ, сокращается вдвое каждые 16 месяцев (т.е. за это время эффективность алгоритмов и/или нейросетевых архитектур удваивается)
                - В исследовании рассматривалась задача распознавания изображений
                - Исследователи измеряли число ресурсов, необходимое для обучения нейросети, решающей одну и ту же задачу с одной и той же точностью (а именно - такой же, как у нейросети AlexNet)
        - При этом, как уже обсуждалось выше, после того, как ИИ достигнет уровня, примерно равного человеческому, события будут развиваться скорее по сценарию быстрого или умеренного взлета, чем медленного (т.е. расстояние от уровня “одного человека” до уровня “всего человечества” ИИ пройдет скорее за несколько дней-месяцев-лет, чем за десятилетия)
        - Но даже если от текущего момента до создания ИИ “человеческого” и затем “сверхчеловеческого” уровня пройдет несколько десятилетий, не исключено, что решение технических вопросов безопасности ИИ займет еще больший срок
            - В этом случае из ныне живущих людей далеко не для всех эта проблема будет актуальна; однако, экзистенциальный риск для человечества даже в таком сценарии никуда не девается
    - При этом вполне вероятно, что СИИ будет запущен, даже если он не будет безопасным
        - Самый банальный сценарий, при котором это случится - разработчики подумали, что он безопасен, хотя на самом деле это было не так
            - Например, они могут постулировать его безопасность на основе каких-то безосновательных утверждений, и на этом закрыть вопрос (как, например, анонсированный Илоном Маском TruthGPT)
                - Причина, по которой TruthGPT якобы должен быть безопасным - в том, что “этот ИИ будет стремиться к максимально полному познанию мира, а значит, он не захочет уничтожать человечество, т.к. человечество - интересная часть этого мира”
                - Однако, подобная логика не выдерживает никакой критики, и катастрофических для человечества сценариев, при которых СИИ стремится к познанию мира, более чем достаточно
                    - Например, такой СИИ может решить, что для исследования ему вполне достаточно сравнительно небольшого числа людей (скажем, десяти тысяч), а остальных вполне можно уничтожить, т.к. их деятельность мешает его работе
                        - То, как функционирует “глобальное человечество”, можно изучать на примере небольшого среза; при этом создавая для оставшихся людей иллюзию наличия “большого мира” (с помощью тех же чатботов и дипфейков)
                        - При этом, изучив поведение оставшихся людей, можно улучшить имитацию “внешнего человечества”, сделав ее более достоверной, и тем самым еще улучшив качество эксперимента
                    - К тому же, далеко не все исследования и эксперименты будут хоть сколько-то гуманными по отношению к подопытным
                        - Хотя над некоторыми лабораторными животными в настоящее время проводятся вполне “гуманные” эксперименты (например, некоторые эксперименты из области психологии), всё же б*о*льшую часть научных знаний можно получить только за счет довольно жестоких опытов
                        - Если СИИ захочет узнать как можно больше о биологии и психологии людей (а также о физических и химических особенностях человеческого тела), то для получения достоверных данных ему нужно будет проводить огромное количество экспериментов, добровольно участвовать в которых никто бы не согласился
            - Более адекватный сценарий предполагает, что разработчики все-таки озаботились безопасностью ИИ, и, возможно, решили ряд проблем с ней, и в конечном итоге их анализ (ошибочно) показал, что теперь этот СИИ безопасен
                - Например, они могут решить только вопросы “легальной безопасности ИИ” в достаточной степени, чтобы регулирующие органы допустили такой продукт на рынок (чтобы ИИ генерировал политкорретные результаты и не выдывал рецепты наркотиков и взрывчатки по первой просьбе; но не чтобы он рассматривал уничтожение человечества как недопустимый продукт своей деятельности)
                - К этой же категории можно отнести все описанные выше сценарии провала, при которых в процессе тестирования в песочнице (в тестовом окружении) СИИ показал себя как безопасный, хотя в более богатом реальном окружении он не будет безопасным
        - В другом варианте разработчики не будут полностью уверены, что их СИИ является безопасным, но могут осознанно пойти на риск, связанный с его запуском
            - Этот сценарий является естественным следствием ныне идущей гонки между разработчиками ИИ (компаниями и государствами): поскольку более мощный ИИ, запущенный раньше, даст серьезное преимущество над конкурентами, у всех участников гонки есть стимул преуменьшить риски, связанных с их продуктом, и запустить его раньше других
            - В этом случае даже если вас всерьез беспокоит безопасность СИИ, и вы ожидаете, что первый запущенный СИИ изменит мир согласно своему видению, вы все равно можете пойти на риск и запустить собственный СИИ
                - Например, вы считаете, что ваш СИИ безопасен с вероятностью 90%, а СИИ вашего ближайшего конкурента - только с вероятностью 80%
                - При этом ваш конкурент наверняка захочет запустить свою разработку раньше вашей, если только дать ему шанс
                - Несмотря на то, что вы бы хотели сначала еще сильнее повысить безопасность собственного СИИ (хотя бы до 99%), вы вынуждены идти на риск, т.к. немедленный запуск вашего варианта будет меньшим из зол, чем скорый запуск варианта от вашего конкурента
                - В итоге риск экзистенциальной катастрофы все равно составит 10% (если ваша оценка достаточно точна)
        - Наконец, может оказаться, что СИИ и вовсе был запущен вопреки воле и желанию самих разработчиков
            - Например, разработчики могли поместить его в изолированную среду, не давая ему доступа в интернет (т.е. еще не запустив его “в большом мире”), либо же только тестировать его в виртуальной среде (”песочнице”); но СИИ сумел бы сбежать из нее и вырваться во внешний мир
            - Либо же разработчики решили, что созданный ими ИИ еще не является сильным и еще не способен представлять собой значительную угрозу, и запустили его просто как “мощный технологический продукт”; тот же повел себя как СИИ, запущенный в “большом мире”
    - Таким образом, велик риск того, что в скором времени будет создан СИИ, и это приведет к чрезвычайно разрушительным последствиям
        - Существует довольно большое число рисков, связанных с развитием ИИ, которые уже начинают очевидным образом реализовываться; однако, эти риски являются относительно низкими по сравнению с возможной экзистенциальной угрозой
            - К числу таких рисков относятся неэтичное и преступное использование материалов, созданных с помощью ИИ-технологий; угроза безработицы вследствие автоматизации огромного числа профессий и т.п.
            - Эти риски являются достаточно прозрачными, и потому активно изучаются большим числом специалистов (поэтому можно предположить, что благодаря этим усилиям они будут либо устранены полностью, либо их эффект удастся ограничить)
            - Кроме того, потенциальные негативные эффекты таких рисков, хотя и являются достаточно серьезными (и потому их определенно нельзя игнорировать), но не являются катастрофическими; фактически, даже если начнется тотальная безработица или интернет заполонят дипфейки, то это будет проблема, с которой человечеству будет вполне по силам справиться
            - Здесь же мы в основном рассматриваем экзистенциальные риски для человечества, исходящие от перспективного СИИ, которые в данный момент менее очевидны, но несут в себе значительно б***о***льшую угрозу
        - Ввиду того, что оценки будущего крайне неточны по своей природе, точный масштаб проблемы оценить трудно; математическое ожидание последствий, связанных с экзистенциальными рисками, однако, достаточно велико, чтобы всерьез обеспокоиться - даже с учетом всех погрешностей
            - [Математическое ожидание](https://ru.wikipedia.org/wiki/Математическое_ожидание) (”матожидание”) какой-либо величины - это строгая количественная оценка того, каким будет значение этой величины “в среднем”
            - Проще всего построить матожидание числа человеческих жертв в случае разработки СИИ
                - Сложно дать сколько-то надежную оценку вероятности уничтожения человечества или сходной экзистенциальной катастрофы, однако похоже, что эта вероятность составляет от единиц до десятков процентов
                    - Согласно трем опросам среди экспертов, чьи работы были представлены на престижных международных конференциях по машинному обучению (NeurIPS и ICML), медианная оценка вероятности такой катастрофы [составляет](https://new.80000hours.ru/problem-profiles/artificial-intelligence/#1-Mnogie-eksperty-po-II-schitajut-chto-est-suschestvennaja-verojatnost-pojavlenija-II-kotoroe-privedyot-k-plohim-posledstvijam-vplot-do-ischeznovenija-chelovechestva) около 5% (отдельные данные показывали также медианные оценки в 2% и 10%)
                    - Приведенные выше рассуждения об опасности со стороны потенциального СИИ, вероятно, должны сместить нашу оценку в сторону более пессимистичной (т.е. согласиться с теми экспертами, которые оценивали риски как более высокие)
                - Для сценария “все люди будут уничтожены” необходимо уточнить - сколько именно людей будет уничтожено
                    - Наиболее консервативная оценка - “будет уничтожено всё текущее население земного шара”, т.е. около 8 миллиардов человек
                    - Однако, имеет смысл также учитывать всех будущих потомков нынешнего человечества в неопределенном будущем; тогда потенциальное число жертв возрастает многократно (возможно, на порядки или даже на порядки порядков)
                    - Впрочем, даже в этом случае наших будущих потомков стоит учитывать с некоторым дисконтом (что защищает нас от парадокса “[пари Паскаля](https://ru.wikipedia.org/wiki/Пари_Паскаля)”)
                        - В данный момент нас интересует только то, сколько людей может быть спасено от потенциальной угрозы со стороны СИИ, и кто при этом не будет уничтожен в результате какого-нибудь иного катаклизма; поэтому те, кто в любом случае не переживет гипотетического уничтожения, не должны считаться жертвами рассматриваемого нами сценария
                            - Рассмотрим, например, условный сценарий, в котором мы с высокой вероятностью ожидаем, что через 300 лет с Землей столкнется астероид, который уничтожит всех людей
                            - Тогда у нас есть два сценария: “с вероятностью 50% (вероятность создания СИИ в течение 50 лет) * 10% (вероятность того, что такой СИИ уничтожит человечество) = 5% история человечества прервется через 50 лет” и “с вероятностью 100% история человечества прервется через 300 лет”
                            - В этом случае потенциальная угроза от создания СИИ состоит в том, что “с вероятностью 5% будут уничтожены все люди, которые жили бы в этот 250-летний период”
                            - Впрочем, большинство альтернативных экзистенциальных рисков либо имеют значительно меньшую вероятность, чем экзистенциальный риск со стороны СИИ, либо потенциальные сроки их реализации находятся значительно дальше
                            - Подробнее о различных экзистенциальных рисках можно узнать, например, [здесь](https://new.80000hours.ru/problem-profiles/)
                        - Возможно, что мы не захотим учитывать каждого нашего потомка как одного человека (однако, это вопрос этики, и потому объективно здесь утверждать ничего нельзя)
                            - С генетической точки зрения, наши более дальние потомки несут меньше наших генов, и потому менее ценны для нас (с точки зрения мотивации, обусловленной естественным отбором); очень далекие потомки могут быть столь же далеки от нас, как и инопланетяне
                            - Однако, с определенной этической точки зрения, мы можем априорно считать всех разумных существ нашими “братьями по разуму”, т.е. примерно настолько же ценными, как люди (несмотря на все наши биологические/структурные и культурные отличия)
                                - Можно парадоксальным образом продлить эту мысль до того, что СИИ точно такой же наш “брат по разуму” и смена господства “человечество → СИИ” может быть оценена в нейтральном или даже позитивном ключе
                                - Однако, мы не будем идти так далеко: большинство людей все же интуитивно предпочитает “сохранение человечества” нежели “появление более достойного разумного существа, которое заменит людей”
                - В конечном итоге мы получаем математическое ожидание числа человеческих жертв от сотен миллионов до миллиардов (возможно даже триллионов и больше, хотя такие оценки уже далеко не бесспорны)
                - Даже очень консервативная оценка “в ближайшие десятилетия матожидание человеческих жертв от угрозы со стороны несогласованного СИИ составляет 100 миллионов человек” является серьезным поводом для того, чтобы, с одной стороны, работать над уточнением прогнозов и моделей угроз, с другой - одновременно активно работать над снижением возможных рисков
            - Однако помимо возможной прямой угрозы существованию человечества, существует также ряд других угроз, которые стоило бы добавить к общей оценке матожидания негативных последствий
                - Очевидным образом сюда можно добавить “не экзистенциальные риски”, связанные с развитием ИИ просто как технологии (экономические и социальные проблемы; использование ИИ в преступности, терроризме и войнах и т.п.)
                - Также сюда можно отнести экзистенциальные риски (т.е. “риски того, что будет уничтожен долговременный потенциал человечества”), не связанные с уничтожением человечества напрямую
                    - Такие риски называют в т.ч. “проблемой второго вида”, т.е. проблемой, когда господство в доступной нам части Вселенной и возможность влиять на судьбу человечества перейдет от вида Homo Sapiens к AGI
                    - Довольно умеренный вариант подобного сценария может выглядеть, например, так, что человечеству будет позволено жить в неком “зоопарке/заповеднике” или резервации, но не позволено что-либо делать вовне его
                        - Такой запрет может быть вполне логичен, даже если бы люди были настроены на кооперацию с СИИ: после того, как они уступили интеллектуальное превосходство, они уже не смогут адекватно просчитывать последствия своих действий, и будут по сравнению с ним подобны “слону в посудной лавке”
                    - Возможен и вариант, в котором действия людей будут ограничены и в рамках выделенной им резервации (и тогда может не потребоваться физическое разграничение зоны активности СИИ и места обитания людей): некоторая свобода им будет позволена, но, например, они будут обязаны вести здоровый образ жизни и возможность принятия многих (потенциально вредных или опасных) решений у них будет отобрана (сценарий “люди как домашние животные”)
                        - Несмотря на сохранение жизни для людей, такой сценарий означает “консервацию” человечества; также, вероятно, бытовая самостоятельность каждого человека будет ограничена в беспрецедентных масштабах
    - Наиболее актуальной стратегией для снижения рисков было бы “замедлить/остановить разработки в области ИИ; форсировать исследования в области безопасности ИИ”
        - Идеальный конечный результат предполагает, что мы в итоге создадим СИИ, который поможет нам значительно ускорить прогресс в решении глобальных проблем, стоящих перед человечеством (и в целом сделает нашу жизнь комфортнее, интереснее, безопаснее и т.п.); и при этом не будет представлять угрозы нашему существованию, а также нашим целям и ценностям
        - Однако, в настоящий момент ключевые разработчики преследуют эти цели лишь постольку-поскольку, стремясь в первую очередь либо вывести на рынок как можно более продаваемый продукт (если разработка ведется в интересах корпорации), либо создать инструмент решения внутри- и внешнеполитических задач и проблем (если в интересах национального правительства)
        - При этом разработчики стремятся опередить друг друга в этой гонке (во многом аналогичной гонке вооружений), и не склонны замедлять свои разработки только ради того, чтобы сделать систему более безопасной и лучше настроить ее на достижение глобальных целей человечества
        - Чтобы гонка замедлилась (а лучше и вовсе остановилась), и при этом форсировалось изучение вопросов безопасности и лучшей нацеленности на важные глобальные цели, должны быть созданы такие условия, в которых у создателей ИИ и заказчиков его разработки была бы сильная мотивация к этому, и не было бы сильной мотивации к обратному
            - Стоит учитывать, что при отсутствии сильной собственной мотивации (если их просто вежливо попросить, или выставить выглядящие неоправданными и несправедливыми требования) разработчики могут либо отмахнуться, либо пытаться обойти/взломать навязанные им требования/KPI
                - Достаточно сильная мотивация для того, чтобы существенно замедлить разработку более мощных ИИ-систем, может быть вызвана, например, большой личной обеспокоенностью вопросами безопасности как у руководителей компании (высших должностных лиц в правительстве), так и у самих разработчиков
                - Альтернативный вариант - когда продолжение активной разработки СИИ будет иметь множество негативных последствий из-за внешнего давления, а остановка/замедление разработки (и переключение на вопросы безопасности и согласования ИИ) принесет значительные бонусы
                    - При этом лица, ответственные за разработку, должны быть уверены, что им вряд ли удастся обмануть внешних наблюдаталей (и за счет этого получить как бонусы за якобы остановленную разработку, так и преимущества от втайне разработанного нового поколения ИИ)
                - В конечном итоге, поскольку крупные коллективы людей действуют по иным законам, нежели каждый человек по отдельности, разработка безопасного СИИ и отказ от разработки небезопасного должны быть выгодны с экономической точки зрения для компаний и с политической - для государств
            - Подходящим источником положительной и отрицательной мотивации для корпораций могут быть мнение клиентов/потребителей, а также подходящие ограничения и поощрения со стороны национальных правительств и международных организаций (подкрепленные при этом строгим контролем)
                - Если большинство клиентов будет категорически против использования “небезопасного ИИ”, это может быть весомым аргументом не выпускать продукт, не прошедший должную проверку и маркировку
                    - Однако, если только часть клиентов будут скорее предпочитать “безопасные” ИИ-продукты “небезопасным”, то это может быть недостаточным стимулом (например, рынок продуктов питания, не содержащих пометки “эко”/”био”/”без ГМО”/”без глютена” довольно велик)
                    - Кроме того, рядовые потребители далеко не всегда способны легко отличить “безопасный” ИИ от “небезопасного”, так что потребуется также обязательная система качественной независимой оценки “безопасности” ИИ-продуктов
                - Со стороны национальных и международных органов могут быть полезны, с одной стороны, запреты на активную разработку мощных ИИ-систем, с другой - заказ и гранты на работы над вопросами технического согласования
                    - Одни только запреты на важную (а иногда и основную) деятельность компании будут сказываться на ней очень негативно (и вызывать в т.ч. желание по возможности их обойти)
                    - Если же предложить компаниям вместо этого переключить свой бизнес и усилия своих исследователей и инженеров на родственную и оплачиваемую область деятельности, то это может оказаться для них достаточно выгодно и вполне приемлемо
                    - При этом, вместе с существенным “пряником” должен идти в той же степени существенный “кнут” в виде жестких санкций за нарушение запретов (причем достаточно сильных, чтобы большая корпорация не могла себе позволить просто выплачивать штрафы, которые вполне окупались бы ожидаемой прибылью от разработки запрещенных продуктов)
                        - Пресловутый “ракетный удар по датацентрам” может быть высшей мерой в цепочке адекватных мер: если компания ведет запрещенное обучение мощной ИИ-системы, ей может быть выписан ордер на прекращение этой работы, вплоть до физического отключения и изъятия оборудования дата-центра в случае неподчинения
                        - Если компания будет успешно сопротивляться законным действиям судебных приставов (в т.ч. поддержанных отрядом спецназа), то похоже, что дело дошло до масштабного силового противостояния, в рамках которого речь уже вполне может идти и о локальном применении ударной авиации
                - Косвенным образом жесткие требования к безопасности ИИ-продуктов могут сделать экономически выгодными разработки в сфере согласования ИИ - если компании и лаборатории смогут продавать “alignment как продукт”
                    - Если для получения разрешения на разработку и продажу какой-то ИИ-системы вам нужно будет соответствовать некоторым строгим стандартам и проходить сертификацию, то вы будете готовы купить услуги “аудита по alignment” или некоторые чисто технические наработки по согласованию ИИ у сторонней компании
                    - Если требования к уровню безопасности при этом будут достаточно жесткими, то различные лаборатории и корпорации захотят активно вкладываться в исследования в соответствующей области, чтобы затем конкурировать уже за качество услуг по обеспечению безопасности
            - Для того, чтобы национальные государства, с одной стороны, не размещали “гособоронзаказ на ИИ” без прохождения строгих проверок на безопасность, с другой - поощряли действующие в их юрисдикции компании к соблюдению аналогичных норм, могут быть важным как мнение населения, так и влияние международных организаций и заключение международных соглашений
                - Выдача грантов компаниям на разработки в области согласования ИИ, а также финансирование деятельности проверяющих органов, потребует расходов из бюджета; при этом лица, принимающие решения, скорее будут хотеть продолжать форсированное развитие ИИ, чтобы использовать его для использования во внутренней и внешней политике
                - До некоторой степени преодолеть это сопротивление мог бы международный престиж от участия в “гонке за безопасность ИИ” (если эта область будет привлекать такое же общественное внимание, как, например, космическая программа и спорт)
                - Если большой процент граждан будет обеспокоен проблемой безопасности ИИ (и они будут это активно озвучивать), то политики могут прислушаться к ним и стремиться что-то сделать в этом отношении ради политических очков
                - Если проблема рисков со стороны СИИ будет всерьез воспринята на международном уровне, то государства, дающие наибольший вклад в развитие ИИ-технологий, могут принять международные соглашения в сфере безопасности
                    - В идеале, с учетом масштабности рисков, такие соглашения должны выполняться как минимум так же строго, как договоры о контроле за оружием массового поражения (в т.ч. ядерным)
                    - При этом хорошо известен негативный опыт международных соглашений (Киотский протокол и Парижское соглашение), полная реализация которых должна быть очень полезна человечеству в долгосрочной перспективе, но процесс реализации наносит ощутимый вред экономике страны в кратко- и среднесрочной
                    - Некоторые сложности подобной кооперации описываются в теории игр как “[Охота на оленя](https://lesswrong.ru/wiki/Охота_на_оленя)”; из соответствующих исследований можно взять и [некоторые способы преодоления этих проблем](https://telegra.ph/Proekty-po-AI-Safety-kak-ohota-na-olenya-09-20)
        - С учетом всех сложностей, вряд ли вообще возможно полностью остановить разработку сильного ИИ, однако значительное замедление ее темпов и одновременное ускорение работ по техническому согласованию ИИ увеличит шансы благоприятного для человечества исхода; причем чем значительнее будут эти замедление и ускорение, тем больше эти шансы; так что приложить еще больше усилий в этом направлении никогда не будет лишним
            - Разумеется, всегда есть некоторый предел, когда прикладывать еще больше усилий будет скорее вредно, чем полезно (грубо говоря, когда никто из людей не занимается производством продуктов питания, а все заняты только надзором за разработчиками ИИ, человечество уже явно перешагнуло этот рубеж)
            - К тому же, откладывание момента создания СИИ не является целью само по себе, т.к. безопасный СИИ обладает потенциалом принести человечеству столь же беспрецедентно большое количество пользы, какое количество вреда способен принести небезопасный СИИ в неблагоприятных сценариях
            - Однако, в настоящий момент проблема безопасности ИИ настолько недооценена, что можно полностью игнорировать опасность того, что она окажется переоценена в ближайшие годы (разве что держать ее в голове на будущее просто на всякий случай)
        - Конечно, мы не можем со стопроцентной уверенностью утверждать, что СИИ действительно будет создан в ближайшие годы или десятилетия, и что это неминуемо приведет к уничтожению человечества; однако потенциальная вероятность благополучного исхода - не повод для легкомысленного отношения к проблеме
            - Всегда остается вероятность того, что однажды созданный СИИ окажется “безопасным” сам по себе, случайно; либо же что он и вовсе никогда не будет создан (а доступным нам максимумом окажутся просто достаточно эффективные интеллектуальные инструменты, лишенные агентности)
            - Однако, рассчитывать на это и не соблюдать технику безопасности было бы очень недальновидно